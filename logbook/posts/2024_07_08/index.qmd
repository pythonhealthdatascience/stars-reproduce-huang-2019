---
title: "Day 4"
author: "Amy Heather"
date: "2024-07-08"
categories: [reproduce]
bibliography: ../../../quarto_site/references.bib
---

## 09.14-09.17, 09.22-09.24, 09.30-09.35: Continuing on in-text results 1 and 2

Re-ran twice more to see again how much variation we get between runs, and how likely that could attribute for the difference against the paper. We saw-

| Output | Result 1 (Day 3) | Result 2 (Day 3) | Result 3 (Today) | Result 4 (Today) | Paper |
| --- | --- | --- | --- | --- | --- |
| Baseline | 13.33 minutes | 13.65 minutes | 14.15 minutes | 14.09 minutes | - |
| Exclusive | 8.58 minutes (4.75 reduction) | 9.20 minutes (4.45 reduction) | 8.79 minutes (5.36 reduction) | 8.05 minutes (6.04 reduction) | 6 minute reduction from baseline |
| Two AngioINR | 14.86 minutes (1.53 increase) | 13.61 minutes (0.04 reduction) | 14.37 minutes (0.22 increase) | 14.04 minutes (0.05 reduction) | 4 minute reduction from baseline |

Based on this, it's reasonable to assume that a 6 minute reduction can be observed within the variation of model runs (in-text result 1), but that the two angioINR scenario is not matching up.

::: {.callout-tip}
## Reflections

Environment used does not match up to paper - paper use Simmer version 4.1.0, and otherwise, other versions of packages and of R being used are more recent than publication. It is unlikely that differences in results are due to this (although not impossible). Note trying to revert the environment to older versions as a possible troubleshooting strategy if issues persist, but not yet, due to major challenges found in trying to do so prior.
:::

## 09.50-10.49, 11.02-11.05, 11.13-11.14: Adding seeds

Based on [this tutorial](https://pythonhealthdatascience.github.io/stars-treat-simmer/02_model/03_r_sampling.html), add seeds to the model. This is because the result was only returned by certain runs of the model and not others, so want to add seeds now so can give a seed for which the result is reproduced. I installed simEd - `renv::install("simEd")` and add to `DESCRIPTION` and `renv::snapshot()` - and then made the following changes to the model:

* `library(simEd)`
* Input `seed` to function which becomes SEED, then `set.seed(SEED+i)` within model replications
* Sampling functions changed from `r` to `v` - i.e. `rpois()` to `vpois()`, with incremental stream numbers

I tried running baseline, but it took a long time - after 6 minutes, it was still running (which is normally how long the whole script takes). I interrupted it and it returned `Error : object 'shifts' not found`. However, no change has been made to shifts code. I ran a short section of code practicing sampling and this worked fine:

```
library(simEd)

ed_pt = 107000
year2min = 525600
I_ED  = round(year2min/ed_pt)

set.seed(5)
vpois(10, I_ED, stream=1)

set.seed(3)
vpois(10, I_ED, stream=1)

set.seed(5)
vpois(10, I_ED, stream=1)
```

I then tried running it with 3 replications instead of 30 (`baseline <- run_model(nsim=3, seed=100)`), and that ran fine, so it appears that introducing this library just slowed down the model alot, as 3 replications could complete in 40 seconds.

I looked into changing the `lapply()` in `model.R` to a parallel version:

* `parLapply` requires you to specify every variable to be included, plus additional lines of code to set up and close clusters
* `mcapply()` just requires you to change `lapply`

Hence, I tried `mcapply`, but it returned `Error: external pointer is not valid`, which was resolved based on [this post](https://groups.google.com/g/simmer-devel/c/4cQWtS571iQ?pli=1) by adding `wrap()`. However, learnt that mclapply wouldn't work on Windows. Moreover, it still took a fair while to run (testing with 30 replications, it's still going at 4 minutes).

As such, removed simEd from `model.R` and environment and returned to `rpois()`, and instead just set a simple seed without controlling streams. The time for this to run was as per usual, which was fab. I ran the baseline model twice with the same seed and compared the results, and it came out looking (by eye, at the processed results) identical.

I therefore ran baseline and exclusive with three different starter seeds, and the seed `200` came out closest to the paper -

* Baseline: 13.96 minutes
* Exclusive: 8.12 minutes
* Difference: 5.84 minutes

Hence, I feel we can mark in-text result 1 as reproduced at this time (11.14), with starter seed of 200.

```{python}
# Add timings for this result
```

### 11.15-: Figure 2

Figure 2 uses the results from the scenarios above but creates plots where:

* X axis is wait time in minutes (on a non-linear scale)
* Y axis is standardised density of patients in queue, from 0 to 1 (on a non-linear scale)
  * i.e. "Probability density of patients who are waiting standardised to patients who are not waiting"
  * i.e. "To facilitate graphical and descriptive comparison across models, we express waiting times as relative probabilities of waiting a given amount of time, compared to not waiting at all. Since most patients accessed services without waiting, wait time densities could be directly compared across simulations after this normalization."

It's not immediately clear to me what this means, but as a first guess/attempt, I'll try 
