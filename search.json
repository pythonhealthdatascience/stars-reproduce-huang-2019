[
  {
    "objectID": "original_study/desECR/description.html",
    "href": "original_study/desECR/description.html",
    "title": "Reproducing Huang et al. 2019",
    "section": "",
    "text": "# DES for ECR Authors: S Huang, J Maingard, H Kok, C Barras, V Thijs, RV Chandra, DM Brooks, H Asadi.\nThis is an interactive discrete event simulation of resource optimization of an endovascular clot retrieval service. Details of the simulation configuration can be visualized here. Built with R-simmer, Email questions/feedback to shiweihuang at outlook dot com."
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing",
    "section": "",
    "text": "üéâ Thank you for checking out our project! üéâ\nThis page contains guidelines on how to get in touch with us and potentially contribute towards this repository.\n\n\nYou can contact the researchers on this project using the provided email addresses in CITATION.cff.\n\n\n\nIf you spot an issue, you are welcome to raise this either by:\n\nUsing GitHub Issues.\nForking the repository, make your changes and submit a pull request for review."
  },
  {
    "objectID": "CONTRIBUTING.html#email",
    "href": "CONTRIBUTING.html#email",
    "title": "Contributing",
    "section": "",
    "text": "You can contact the researchers on this project using the provided email addresses in CITATION.cff."
  },
  {
    "objectID": "CONTRIBUTING.html#suggesting-changes",
    "href": "CONTRIBUTING.html#suggesting-changes",
    "title": "Contributing",
    "section": "",
    "text": "If you spot an issue, you are welcome to raise this either by:\n\nUsing GitHub Issues.\nForking the repository, make your changes and submit a pull request for review."
  },
  {
    "objectID": "logbook/posts/2024_07_12/index.html",
    "href": "logbook/posts/2024_07_12/index.html",
    "title": "Day 8",
    "section": "",
    "text": "Note\n\n\n\nSupplementary figure then evaluation against guidelines. Reproduction: 24h 10m (60.4%). Evaluation: 1h 31m."
  },
  {
    "objectID": "logbook/posts/2024_07_12/index.html#working-on-the-supplementary-figure",
    "href": "logbook/posts/2024_07_12/index.html#working-on-the-supplementary-figure",
    "title": "Day 8",
    "section": "09.00-09.07, 09.14-09.24, 09.36-09.40: Working on the supplementary figure",
    "text": "09.00-09.07, 09.14-09.24, 09.36-09.40: Working on the supplementary figure\nThe supplementary figure consists of doubling and tripling the number of ECR patients.\n\n‚ÄúSince acquiring data for this study, the demands for ECR at our Comprehensive Stroke Service has doubled between 2018 and 19 and is predicted to triple by the end of 2019. We simulated these increased demands on the resource. As expected, the patient wait times do become longer, but the patterns of resource utilization remained unchanged, suggesting that the same bottlenecks affect throughput‚Äù Huang et al. (2019)\n\nI‚Äôve assumed this to mean we double and triple the number of ED arrivals (ed_pt, e.g.ed_pt=107700*2), but the output plots looked vastly different to the paper!\n\n\n\nWrong supplementary figure\n\n\nI then realised I was mistaken, and that perhaps I should have interpreted this as being double and triple the ecr_pt (which is used when model decides whether patient needs ECR: PROB_ECR = ecr_pt / ais_pt). This looked right, and very similar to the paper, with the exception that my angioINR wait times curves peak much lower."
  },
  {
    "objectID": "logbook/posts/2024_07_12/index.html#untimed-consensus-on-reproduction-success",
    "href": "logbook/posts/2024_07_12/index.html#untimed-consensus-on-reproduction-success",
    "title": "Day 8",
    "section": "Untimed: Consensus on reproduction success",
    "text": "Untimed: Consensus on reproduction success\nShared with Tom and Alison. Tom and Alison both agreed with decisions (reproduced Figure 5 and in-text 1 and 2, and not other items)."
  },
  {
    "objectID": "logbook/posts/2024_07_12/index.html#tidy-and-email-author",
    "href": "logbook/posts/2024_07_12/index.html#tidy-and-email-author",
    "title": "Day 8",
    "section": "10.30-10.50: Tidy and email author",
    "text": "10.30-10.50: Tidy and email author\nDid a little bit of tidying (e.g.¬†removed some old scenarioes from reproduction.qmd), and then emailed the corresponding author, to let them know the progress with this work, and to ask for any advice re: discrepancies, and whether they might have and be happy to share code that produces these figures."
  },
  {
    "objectID": "logbook/posts/2024_07_12/index.html#timings-for-reproduction",
    "href": "logbook/posts/2024_07_12/index.html#timings-for-reproduction",
    "title": "Day 8",
    "section": "Timings for reproduction",
    "text": "Timings for reproduction\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 1409\n\n# Times from today\ntimes = [\n    ('09.00', '09.07'),\n    ('09.14', '09.24'),\n    ('09.36', '09.40'),\n    ('10.30', '10.50')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 41m, or 0h 41m\nTotal used to date: 1450m, or 24h 10m\nTime remaining: 950m, or 15h 50m\nUsed 60.4% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_07_12/index.html#badges",
    "href": "logbook/posts/2024_07_12/index.html#badges",
    "title": "Day 8",
    "section": "10.58-11.06: Badges",
    "text": "10.58-11.06: Badges\nAs mentioned in prior logbook entry, will now move on to evaluation stage.\nNotes on the unmet (and why)\n\narchive and id - only on GitHub\ncomplete - didn‚Äôt have the scenario and figure code\nstructure - not sufficient to facilitate reuse (i.e.¬†would want more commenting/docstrings/dcoumentation), and as structure was designed for application (but had to extract code to make the paper results)\ndocumentation_‚Ä¶ - no documentation provided\nregenerated - wasn‚Äôt able to reproduce all the items\nhour - took longer than an hour"
  },
  {
    "objectID": "logbook/posts/2024_07_12/index.html#stars-framework",
    "href": "logbook/posts/2024_07_12/index.html#stars-framework",
    "title": "Day 8",
    "section": "11.07-11.16: STARS Framework",
    "text": "11.07-11.16: STARS Framework"
  },
  {
    "objectID": "logbook/posts/2024_07_12/index.html#stress-des",
    "href": "logbook/posts/2024_07_12/index.html#stress-des",
    "title": "Day 8",
    "section": "11.26-11.51, 12.00-12.17, 13.04-13.18: STRESS-DES",
    "text": "11.26-11.51, 12.00-12.17, 13.04-13.18: STRESS-DES\n\nUncertain\n\n\n\n\n\n\n\nItem\nMy comments\n\n\n\n\nSTRESS-DES 1.2 Model outputs. Define all quantitative performance measures that are reported, using equations where necessary. Specify how and when they are calculated during the model run along with how any measures of error such as confidence intervals are calculated.\nIt does describe the measures, and how these are calculated, and so I have said it met these criteria, although I did find it hard to understand/calculate the relative probability of waiting, and would‚Äôve benefited from further detail/equations\n\n\nSTRESS-DES 1.3 Experimentation aims. If the model has been used for experimentation, state the objectives that it was used to investigate. (A) Scenario based analysis ‚Äì Provide a name and description for each scenario, providing a rationale for the choice of scenarios and ensure that item 2.3 (below) is completed.\nI feel the paper does describe the scenarios clearly - my only hesitation is that I have been unable to successfully implement the exclusive use scenario - but that feels like a coding issue rather than a description issue? As, on the face of it, the article describes everything I need to know.\n\n\nSTRESS-DES 3.2 Pre-processing. Provide details of any data manipulation that has taken place before its use in the simulation, e.g.¬†interpolation to account for missing data or the removal of outliers.\nNone provided, so presumed not applicable - but hard to say, as maybe there was just unmentioned. But as not possible to know either way, assumed not-applicable\n\n\nISPOR SDM 15 Is the model generalizability issue discussed?\nNot sure if it is partially or fully met?\n\n\n12 Is cross validation performed and reported\nIs it not met or not applicable? Introduction: ‚ÄúIn contrast to other healthcare fields, a resource-use optimization model has not been implemented\n\n\nfor comprehensive stroke services.‚Äù Huang et al. (2019)"
  },
  {
    "objectID": "logbook/posts/2024_07_12/index.html#des-checklist-derived-from-ispor-sdm",
    "href": "logbook/posts/2024_07_12/index.html#des-checklist-derived-from-ispor-sdm",
    "title": "Day 8",
    "section": "13.19-13.33, 13.39-13.43: DES checklist derived from ISPOR-SDM",
    "text": "13.19-13.33, 13.39-13.43: DES checklist derived from ISPOR-SDM"
  },
  {
    "objectID": "logbook/posts/2024_07_12/index.html#timings-for-evaluation",
    "href": "logbook/posts/2024_07_12/index.html#timings-for-evaluation",
    "title": "Day 8",
    "section": "Timings for evaluation",
    "text": "Timings for evaluation\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 0\n\n# Times from today\ntimes = [\n    ('10.58', '11.06'),\n    ('11.07', '11.16'),\n    ('11.26', '11.51'),\n    ('12.00', '12.17'),\n    ('13.04', '13.18'),\n    ('13.19', '13.33'),\n    ('13.39', '13.43')]\n\ncalculate_times(used_to_date, times, limit=False)\n\nTime spent today: 91m, or 1h 31m\nTotal used to date: 91m, or 1h 31m"
  },
  {
    "objectID": "logbook/posts/2024_07_12/index.html#untimed-working-on-research-compendium-stage",
    "href": "logbook/posts/2024_07_12/index.html#untimed-working-on-research-compendium-stage",
    "title": "Day 8",
    "section": "Untimed: Working on research compendium stage",
    "text": "Untimed: Working on research compendium stage\nSome tidying up and sorting for research compendium stage inc.\n\nWorking on README for reproduction\nLooking into options for unit testing in R‚Ä¶\n\nPackage testthat\nuse_testthat(). This initializes the unit testing machinery for your package. It adds Suggests: testthat to DESCRIPTION, creates the directory tests/testthat/, and adds the script tests/testthat.R.\nuse_test(\"functionname\") to create matching testfile.\nlibrary(testthat), load_all(), test() to run the test. Tests will also run whenever check() package\nrename_files(\"strsplit1\", \"str_split_one\") to rename the R file and will also rename testthat file (which we also edit with new function name), to fit with convention of filename matching function name\nCan use expect_equal() to compare two dataframes\nFile structure: tests/, testthat.R, testthat, helper_func1.R, helper_func2.R, test_name.R\nTest files prefixed test_, keep data in files prefixed helper_"
  },
  {
    "objectID": "logbook/posts/2024_07_05/index.html",
    "href": "logbook/posts/2024_07_05/index.html",
    "title": "Day 3",
    "section": "",
    "text": "Note\n\n\n\nSet-up environment and run model. Total time used: 7h 23m (18.5%)"
  },
  {
    "objectID": "logbook/posts/2024_07_05/index.html#set-python-interpreter",
    "href": "logbook/posts/2024_07_05/index.html#set-python-interpreter",
    "title": "Day 3",
    "section": "09.46-09.47: Set Python interpreter",
    "text": "09.46-09.47: Set Python interpreter\nSet Python interpreter (e.g.¬†to render these from RStudio) by clicking on the project in the top right of RStudio, then selecting Project Options &gt; Python, and selecting the quarto_huang_2019 virtual environment I‚Äôd set up."
  },
  {
    "objectID": "logbook/posts/2024_07_05/index.html#returning-to-troubleshooting-r-version",
    "href": "logbook/posts/2024_07_05/index.html#returning-to-troubleshooting-r-version",
    "title": "Day 3",
    "section": "09.48-10.21: Returning to troubleshooting R version",
    "text": "09.48-10.21: Returning to troubleshooting R version\nContinuing to look at the instructions for old R releases from yesterday:\n\n‚ÄúAs of July 2023, packages for R versions below 4.0 are no longer being updated. R 3.6 packages for Ubuntu on i386 and amd64 are available for most stable Desktop releases of Ubuntu until their official end of life date. However, only the latest Long Term Support (LTS) release is fully supported. As of November 18, 2018 the supported releases are Bionic Beaver (18.04;LTS), Xenial Xerus (16.04; LTS), and Trusty Tahr (14.04; LTS).‚Äù\n\nBy running lsb_release -a, I can see that my linux version is jammy (22.04.4 LTS). Looking at the instructions from this Stackoverflow post, I‚Äôm a bit unclear as to whether I can use any of these if they‚Äôre for older versions of linux.\nFrom this help post, I then stumbled across RStudio r-builds which has R builds that say they should install fast on Ubuntu from a .deb file and are designed to easily switch between multiple versions of R. These say they support Ubunutu 22.04. I ran:\nR_VERSION=3.6.0\ncurl -O https://cdn.posit.co/r/ubuntu-2204/pkgs/r-${R_VERSION}_1_amd64.deb\nsudo apt-get install gdebi-core\nsudo gdebi r-${R_VERSION}_1_amd64.deb\nI confirmed this was installed by running /opt/R/${R_VERSION}/bin/R --version.\nI then followed their instructions to add R to the system path:\nsudo ln -s /opt/R/${R_VERSION}/bin/R /usr/local/bin/R \nsudo ln -s /opt/R/${R_VERSION}/bin/Rscript /usr/local/bin/Rscript\nI restarted RStudio and found I was now in R 3.6.0. I delete the renv (which was built with 4.4.1) and remade it.\nrenv::deactivate(clean=TRUE)\ninstall.packages(\"renv\")\nrenv::init(bare=TRUE)\nrenv::snapshot()\nThe lock file now had R 3.6.0 (previously 4.4.1) and renv 1.0.7."
  },
  {
    "objectID": "logbook/posts/2024_07_05/index.html#installing-the-packages",
    "href": "logbook/posts/2024_07_05/index.html#installing-the-packages",
    "title": "Day 3",
    "section": "10.40-11.30, 11.35-11.41: Installing the packages",
    "text": "10.40-11.30, 11.35-11.41: Installing the packages\nI ran renv::install() but it failed with: Warning: failed to find source for 'simmer.plot 0.1.15' in package repositories. Error: failed to retrieve package 'simmer.plot@0.1.15'.\nI then tried with remotes:\ninstall.packages(\"remotes\")\nremotes::install_version(\"simmer\", \"4.2.2\")\nHowever, this failed like before. Instead, I decided a different tactic - to just download them without the specified versions. I removed the versions from DESCRIPTION and ran renv::install(). However, this stopped with an error: Error: package 'evaluate' is not available.\nI then tried working through each package one by one.\nrenv::install(\"simmer\") was successful.\nrenv::install(\"simmer.plot\") failed with the issue of ‚Äòevaluate‚Äô is not available. Based on this StackOverflow post, I tried installing ‚Äòstringi‚Äô - but that didn‚Äôt end up helping. I tried install evaluate before and after restarting the R session but still stated as not available.\nUncertain on what else might fix this, I decided to actually just start again from the latest version of R and try installing the packages there and see if I could get it to work without backdating the packages. I closed RStudio and ran the commands as above but changed R_VERSION to 4.4.1. I also couldn‚Äôt run the commands for symbolic link as it said the files already exist. I restarted R but still 3.6.0. Looking in /opt/R/, I can see I now have 3.6.0 and 4.4.1.\nBased on the prior tutorial I‚Äôd found, I tried:\nexport RSTUDIO_WHICH_R=/opt/R/4.4.1/bin/R\nrstudio\nThis worked, although default when open from application bar was still set to 3.6.0. I tried changing the .profile file (nano .profile) to add export RSTUDIO_WHICH_R=/opt/R/4.4.1/bin/R but made no difference.\nI tried forcing replacement of the symbolic links then reopening RStudio:\nR_VERSION=4.4.1\nsudo ln -snf /opt/R/${R_VERSION}/bin/R /usr/local/bin/R \nsudo ln -snf /opt/R/${R_VERSION}/bin/Rscript /usr/local/bin/Rscript\nThis worked! So, trying again (with DESCRIPTION file still containing no versions)‚Ä¶\nrenv::deactivate(clean=TRUE)\ninstall.packages(\"renv\")\nrenv::init(bare=TRUE)\nrenv::snapshot()\nrenv::install()\nrenv::snapshot()"
  },
  {
    "objectID": "logbook/posts/2024_07_05/index.html#try-running-the-code",
    "href": "logbook/posts/2024_07_05/index.html#try-running-the-code",
    "title": "Day 3",
    "section": "11.41-11.46, 11.52-12.00: Try running the code",
    "text": "11.41-11.46, 11.52-12.00: Try running the code\nI copied over server.R and, on opening, it said that plyr and shiny were required but not installed, so I add these to the environment as well.\nOn reflection, I realised that the settings to only store dependencies from DESCRIPTION in renv.lock probably wouldn‚Äôt be great, in case hidden things were also installed, so changed this setting to ‚Äúimplicit‚Äù (which is default).\nI ran the file and it did the command shiny, but said Error: object 'shiny' not found. I copied over all the files and tried again. It ran the script but nothing happened. Based on the Shiny documentation, I moved the files into a folder called app and run the following in R console:\nlibrary(shiny)\nrunApp(\"app\")\nThis opened up a shiny app, but got an error ‚Äúthere is no package called ‚Äòmarkdown‚Äô‚Äù. I add this to the environment and tried again.\nThis ran the app successfully.\nHowever, from having looked at the app online, I knew that the figures it produced are not what I need to reproduce the results presented in the paper."
  },
  {
    "objectID": "logbook/posts/2024_07_05/index.html#getting-the-raw-model-results",
    "href": "logbook/posts/2024_07_05/index.html#getting-the-raw-model-results",
    "title": "Day 3",
    "section": "12.07-12.21, 13.00-13.21, 13.28-13.41: Getting the raw model results",
    "text": "12.07-12.21, 13.00-13.21, 13.28-13.41: Getting the raw model results\nI copied the function simulate_nav from server.R. Looking through it, there was only one part still using shiny - the progress bar - and I removed those lines of code, then add a call for the function at the end of the script (simulate_nav()), and ran it. This ran for a while, which was a little odd given how quick the app was.\nI tried running it with a very short run time (simulate_nav(run_t=60)) and this returned results!\nI borrowed from the plot_nav() function in server.R to help process the results.\nI add the reproduction.Rmd to the Quarto site, but this had issues since the Quarto book renv is seperate to the analysis renv. Based on this forum post, there are two possible solutions:\n\nIntegrate the .html file produced from the .Rmd into the book, so it is pre-rendered, and set the .Rmd to Render on Save.\nAdd the packages needed for the book to the analysis renv.\n\nHowever, it appears you‚Äôd have to copy the .html code into the Quarto document. So, decided on the simpler solution of adding the required packages for the book to the analysis environment. I deleted the environment in the main folder."
  },
  {
    "objectID": "logbook/posts/2024_07_05/index.html#checking-model-parameters",
    "href": "logbook/posts/2024_07_05/index.html#checking-model-parameters",
    "title": "Day 3",
    "section": "13.42-14.13: Checking model parameters",
    "text": "13.42-14.13: Checking model parameters\n\nComparing parameters\nTable 1 provides the parameters for the model. I compared these against the function inputs (as the model have no comments/docstrings, it took a little while to make sure I was matching up the right things).\nPhysical and human resources:\n\n\n\n\n\n\n\n\nParameter\nPaper\nScript\n\n\n\n\nAngiography machine for INR and IR\n1\nangio_inr = 1\n\n\nAngiography machine for IR only\n1\nangio_ir = 1\n\n\nCT\n2\nct = 2\n\n\nInterventional neuroradiologist\n1 24h\ninr = 1 and inr_night = 1\n\n\nInterventional radiologist\n2 8am-5pm 1 5pm-8am\nir = 1 and ir_night = 1\n\n\nAngiography staff\n6 8am-5pm 3 5pm-8am\nangio_staff = 10 and angio_staff_night = 3\n\n\nED team\n10 24h\ned_staff = 10\n\n\nStroke team\n1 24h\nstroke_staff = 1\n\n\n\nFor the shifts parameter: shifts = c(8,17)\nPatients:\n\n\n\nParameter\nPaper N\nPaper IAT\nScript\n\n\n\n\nED\n107,700\n5\ned_pt = 107000\n\n\nSuspected stroke\n750\n701\nst_pt = 750\n\n\nAIS\n450\n1168\nais_pt = 450\n\n\nECR\n58\n9062\necr_pt = 58\n\n\nElective INR\n104\n5054\ninr_pt = 300\n\n\nEmergency IR\n468\n1123\neir_pt= 1000\n\n\nElective IR\n3805\n138\nir_pt = 4000\n\n\n\nI also compared some other parameters mentioned in the paper:\n\nSimulated each scenario 30 times - nsim = 1\nRuntime 365 days - run_t = 10000\n\n\n\nCorrecting differences\n\nInterventional neuroradiologist: inr_night = 0\nInterventional radiologist: ir = 2\nAngiography staff: angio_staff = 6\nED: ed_pt = 107700\nElective INR: inr_pt = 104\nEmergency INR: eir_pt= 468\nElective IR: ir_pt = 3805\nReplications: nsim=30\nRun time‚Ä¶\n\nIn the paper, run time is 365 days\nIn the script, run_t = 10000 and RUN_T = run_t * 40320\nDeduced that time unit is minutes\nThis is set up for the app, where user inputs the run time in months, and 40320 minutes = 28 days\nTo more easily reproduce paper (with run time 365 days), modified script so input is in days (which are then converted to minutes for RUN_T)"
  },
  {
    "objectID": "logbook/posts/2024_07_05/index.html#fixing-environment",
    "href": "logbook/posts/2024_07_05/index.html#fixing-environment",
    "title": "Day 3",
    "section": "14.22-14.27: Fixing environment",
    "text": "14.22-14.27: Fixing environment\nThe build of the book on GitHub failed:\nConfiguration failed because libcurl was not found. Try installing:\n * deb: libcurl4-openssl-dev (Debian, Ubuntu, etc)\n * rpm: libcurl-devel (Fedora, CentOS, RHEL)\nIf libcurl is already installed, check that 'pkg-config' is in your\nPATH and PKG_CONFIG_PATH contains a libcurl.pc file. If pkg-config\nis unavailable you can set INCLUDE_DIR and LIB_DIR manually via:\nR CMD INSTALL --configure-vars='INCLUDE_DIR=... LIB_DIR=...'\nBased on this GitHub issue, add installation of this to the action."
  },
  {
    "objectID": "logbook/posts/2024_07_05/index.html#in-text-results-1-and-2",
    "href": "logbook/posts/2024_07_05/index.html#in-text-results-1-and-2",
    "title": "Day 3",
    "section": "14.29-15.11, 15.32-16.23: In-text results 1 and 2",
    "text": "14.29-15.11, 15.32-16.23: In-text results 1 and 2\nThe provided processing scripts may be able to help guide us, but not provided will create the Figures in the paper, so we do need to write that from scratch.\nAlthough the article focuses on the AngioINR, the plot includes 6 resources. The resources are provided by simmer‚Äôs get_mon_resources().\nWe‚Äôll start with Figure 2 and its scenarios - with the related in-text results 1 and 2 probbaly being the easiest to initially check.\nThe plot the angio INR wait times, which can be obtained from the arrivals dataframe.\nI created a function that runs the model with the baseline parameters identified above, then looked to the Figure 2 model variants.\n\nExclusive use\nIn this scenario, AngioINR not available to elective IR patients. It is available to stroke, selective INR and emergency IR patients.\nLooking at the model code, use of the angioINR is controlled by a seize(\"angio_inr\", 1) statement in the trajectory() for each patient. Can see that it is seized in:\n\necr_traj (ECR)\nir_traj (Elective IR)\ninr_traj (Elective INR)\neir_traj (Emergency INR)\n\nHence, add an exclusive_use statement and conditional section to remove angio_inr as an option to choose from when the scenario is active for the ir_traj trajectory.\n\n\nTwo angioINRs scenario\nThis was super easy to change with the angio_inr parameter.\n\n\nCheck in-text results 1 and 2\nRan each of the scenarios and found the mean waiting time for each resource across all replications. Results for AngioINR were:\n\nBaseline: 86.99 minutes\nExclusive use: 63.33 minutes\nTwo AngioINR: 52.78 minutes\n\nThis is markedly more than in the paper (exclusive reduces by 6 min, and two angioINRs reduces by 4 min). The median was even more different.\nThis was looking at the waiting time for all patient types though. And the interest of the paper is in stroke (‚ÄúThe elective INR, elective IR and emergency IR pathways are modeled because they utilize resources shared with the stroke pathway.‚Äù Huang et al. (2019))\nThe results have 4 categories: ed, ir, eir, and inr. Hence, it appears ED should be stroke - and indeed, in paper, the stroke pathway begins with a new patient in the emergency department (ED). When filtered just to those patients‚Ä¶\nMean wait times are:\n\nBaseline 307.83 minutes\nExclusive: 292.18 minutes\nTwo AngioINR: 319.94 minutes\n\nMedian wait times for the AngioINR are:\n\nBaseline: 206.53 minutes\nExclusive: 199.03 minutes\nTwo AngioINR: 244.27 minutes\n\nBy the median times, we‚Äôre fairly close to the paper (comparing the averages, it 7 minutes quicker). However, two angioINR is very different, and I‚Äôm a little sceptical as to whether I‚Äôve got it quite right.\n\n\n\n\n\n\nReflections\n\n\n\nDisregarding my attempts to backdate R and the packages, the provided code was actually quite simple to get up and running as a shiny app.\nHowever, it was provided with the article more for that purpose, than to be producing the items in the article, as the base parameters of the model differ, and as there is no code to process and generate the figures and results.\nI‚Äôll keep working in latest R and packages, as current focus of this stage is just to try and reproduce the items. However, it would be good to try and figure out how to successfully backdate R and the packages, as that feels like an essential thing to be able to do, that I just hadn‚Äôt managed to get to the bottom of yet."
  },
  {
    "objectID": "logbook/posts/2024_07_05/index.html#continuing-to-troubleshoot-in-text-results-1-and-2",
    "href": "logbook/posts/2024_07_05/index.html#continuing-to-troubleshoot-in-text-results-1-and-2",
    "title": "Day 3",
    "section": "16.30-16.37, 16.43-16.48, 16.55-16.57: Continuing to troubleshoot in-text results 1 and 2",
    "text": "16.30-16.37, 16.43-16.48, 16.55-16.57: Continuing to troubleshoot in-text results 1 and 2\nI realised that perhaps my issue as in incorrectly assuming that I should set inr_night to 0. There should be one INR person 24h, but because all the staff get put on a schedule, by removing the ‚Äúnight‚Äù person I technically only have someone during day time hours.\nI changed this and re-ran (with timer - can see it took 6.3 minutes).\nThis was definitely a fix - the waiting times now look far closer to what I expected (around 10 minutes rather than 200!). The median results are very small, but looking at the mean results:\n\nBaseline: 13.33 minutes\nExclusive: 8.58 minutes\nTwo AngioINR: 14.86 minutes\n\nHowever, still not quite right‚Ä¶ 4.7 minute reduction for exclusive (should be 6 minutes) and 1.5 minute increase for two machines (should be 4 minute reduction). The exclusive is pretty close, although its not yet clear to me how much it varies between runs, and whether that could be reasonably attributed to be stochasticity or not. Given we‚Äôre comparing fairly small numbers, it is a bit on the fence for me, and wouldn‚Äôt yet say I feel confident in it being successfully reproduced.\nI tried re-running it all to see how much the results differed - and it was by a fair bit actually! Up to about a minute:\n\nBaseline: 13.65 minutes\nExclusive: 9.20 minutes\nTwo AngioINR: 13.61 minutes\n\nHowever, differences are still off the reported - 4.45 minute reduction and 0.01 minute reduction."
  },
  {
    "objectID": "logbook/posts/2024_07_05/index.html#timings",
    "href": "logbook/posts/2024_07_05/index.html#timings",
    "title": "Day 3",
    "section": "Timings",
    "text": "Timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 149\n\n# Times from today\ntimes = [\n    ('09.46', '09.47'),\n    ('09.48', '10.21'),\n    ('10.40', '11.30'),\n    ('11.35', '11.41'),\n    ('11.41', '11.46'),\n    ('11.52', '12.00'),\n    ('12.07', '12.21'),\n    ('13.00', '13.21'),\n    ('13.28', '13.41'),\n    ('13.42', '14.13'),\n    ('14.22', '14.27'),\n    ('14.29', '15.11'),\n    ('15.32', '16.23'),\n    ('16.30', '16.37'),\n    ('16.43', '16.48'),\n    ('16.55', '16.57')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 294m, or 4h 54m\nTotal used to date: 443m, or 7h 23m\nTime remaining: 1957m, or 32h 37m\nUsed 18.5% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_07_22/index.html",
    "href": "logbook/posts/2024_07_22/index.html",
    "title": "Day 13",
    "section": "",
    "text": "Note\n\n\n\nTom‚Äôs test run of the model."
  },
  {
    "objectID": "logbook/posts/2024_07_22/index.html#untimed-test-run-by-second-member-of-stars-team",
    "href": "logbook/posts/2024_07_22/index.html#untimed-test-run-by-second-member-of-stars-team",
    "title": "Day 13",
    "section": "Untimed: Test run by second member of STARS team",
    "text": "Untimed: Test run by second member of STARS team\nTom attempted to run the model from my reproduction/ folder.\nWas able to run the tests fine, and also ran some of the scripts.\nThe renv install okay - just needed to install some operating system dependencies via apt, but likely as that was the first time he‚Äôd used R on that machine.\nThe local build of the docker image worked fine first time. Launched RStudio server and ran testthat without an issue.\nNoted that in both cases RStudio was using 8GB RAM.\nWasn‚Äôt able to run it on his virtual machine (by default, that only allocates 4GB RAM, so that would be why)."
  },
  {
    "objectID": "logbook/posts/2024_07_03/index.html",
    "href": "logbook/posts/2024_07_03/index.html",
    "title": "Day 1",
    "section": "",
    "text": "Note\n\n\n\nSet-up repository and add article and code. Total time used: 0h 45m (1.9%)"
  },
  {
    "objectID": "logbook/posts/2024_07_03/index.html#set-up-and-update-repository",
    "href": "logbook/posts/2024_07_03/index.html#set-up-and-update-repository",
    "title": "Day 1",
    "section": "11.53-12.20, 12.27-12.33: Set-up and update repository",
    "text": "11.53-12.20, 12.27-12.33: Set-up and update repository\nI have previously (Friday 21st June 2024) sent an email to the corresponding author (Dr.¬†Shiwei Huang) to inform about the study, using the template email from our protocol.\nToday, used template repository to create this repository and updated it to be relevant to Huang et al.¬†2019 - updated..\n\nREADME\nHome page (index.qmd)\nLogbook\nCITATION.cff\n_quarto.yml\n\nFrom a quick look at their code repository, can see they use a GNU General Public License version 3. The requirements of this license are to:\n\nInclude a copy of the full license\nState all significant changes made to the software\nMake the original source code available when distributing binaries based on that work\nInclude a copy of the original copyright notice\n\nIt allows the code to be changed and distributed to others (as long as release under GPL v3 also). Hence, updated license (and references to it) to GNU GPL 3.0 accordingly.\nCreated environment for book."
  },
  {
    "objectID": "logbook/posts/2024_07_03/index.html#upload-model-code",
    "href": "logbook/posts/2024_07_03/index.html#upload-model-code",
    "title": "Day 1",
    "section": "12.34-12.36: Upload model code",
    "text": "12.34-12.36: Upload model code\nUploaded copy of https://github.com/shiweih/desECR to original_study/."
  },
  {
    "objectID": "logbook/posts/2024_07_03/index.html#check-journal-article-license-and-upload",
    "href": "logbook/posts/2024_07_03/index.html#check-journal-article-license-and-upload",
    "title": "Day 1",
    "section": "12.43-12.47, 14.53-14.59: Check journal article license and upload",
    "text": "12.43-12.47, 14.53-14.59: Check journal article license and upload\nThe journal article was published in Frontiers in Neurology and is available at https://doi.org/10.3389/fneur.2019.00653. It has the following copyright statement:\n\n‚Äú¬© 2019 Huang, Maingard, Kok, Barras, Thijs, Chandra, Brooks and Asadi. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.‚Äù\n\nHence, we are free to upload this article and images to the repository (ensuring we cite throughout whenever using them), as well as the supplementary material.\nI set this up to be displayed within the quarto site."
  },
  {
    "objectID": "logbook/posts/2024_07_03/index.html#timings",
    "href": "logbook/posts/2024_07_03/index.html#timings",
    "title": "Day 1",
    "section": "Timings",
    "text": "Timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 0\n\n# Times from today\ntimes = [\n    ('11.53', '12.20'),\n    ('12.27', '12.33'),\n    ('12.34', '12.36'),\n    ('12.43', '12.47'),\n    ('14.53', '14.59')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 45m, or 0h 45m\nTotal used to date: 45m, or 0h 45m\nTime remaining: 2355m, or 39h 15m\nUsed 1.9% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_07_10/index.html",
    "href": "logbook/posts/2024_07_10/index.html",
    "title": "Day 6",
    "section": "",
    "text": "Note\n\n\n\nReproduced in-text 2, working on Figures 2 + 3. Total time used: 20h 28m (51.2%)."
  },
  {
    "objectID": "logbook/posts/2024_07_10/index.html#going-back-to-the-app",
    "href": "logbook/posts/2024_07_10/index.html#going-back-to-the-app",
    "title": "Day 6",
    "section": "09.18-09.25: Going back to the app",
    "text": "09.18-09.25: Going back to the app\nAlthough the figures in the app don‚Äôt match up to the figures in the paper, I wanted to check to see if I could get any more similar results via the app.\nCould put in all the parameters, except number of simulations was limited to 10 (rather than 30) but crashes at that number, so run at their default. However, the outputs don‚Äôt really contain anything usable (e.g.¬†just know most had short wait time, and know median occupancy ratio was around 20%). However, it did make me think that‚Äôs it‚Äôs worth trying the models with the default parameters from the code (rather than the paper), just to see if that happens to look any more similar."
  },
  {
    "objectID": "logbook/posts/2024_07_10/index.html#running-the-model-with-default-parameters-from-the-code",
    "href": "logbook/posts/2024_07_10/index.html#running-the-model-with-default-parameters-from-the-code",
    "title": "Day 6",
    "section": "09.26-09.32, 09.38-9.40, 10.09-10.12: Running the model with default parameters from the code",
    "text": "09.26-09.32, 09.38-9.40, 10.09-10.12: Running the model with default parameters from the code\nRan baseline model with default parameters from the code (rather than fixing to meet paper).\nInteresting differences, for example, are that it is 1 simulation (nsim=1) but run time 10,000 days (run_t=10000) which works out to about 27 years (which is not far off running 30 simulations each of 1 year).\nHowever, can see this is absolutely wrong! Which is not surprising, but still good we checked.\n\n\n\nFigure 2A with parameters from code"
  },
  {
    "objectID": "logbook/posts/2024_07_10/index.html#in-middle-of-the-above-discussion-with-tom",
    "href": "logbook/posts/2024_07_10/index.html#in-middle-of-the-above-discussion-with-tom",
    "title": "Day 6",
    "section": "09.42-10.00 (in middle of the above): Discussion with Tom",
    "text": "09.42-10.00 (in middle of the above): Discussion with Tom\nShowed Tom the progress and he shared from additional suggestions of things to look into:\n\nCheck calculated inter-arrival times match paper\nCheck distributions are the same\nCheck length of resources (we realised not mentioned in paper - e.g.¬†timeout for appointment)\n\nAlso, reminded that the use of simEd and seed streams is not about getting the same results from the same model with the same parameters, but about controlling change when you change parameters (i.e.¬†so the only thing that changes is that parameter, and not the sampling). However, in this case, set.seed() is sufficient.\nMy additional reflections of things to try from this are to:\n\nVary length of resources\nTry not limiting to just ED patients\nDouble-check if INR procedures only have one room option (whilst IR have two rooms)\nLook at parameters used in the diagram on CLOUDES\n\nAgreed to explore these and anything else can think of, but if then still stuck, at that point to email the authors (once have tried the final figures - resource utilisation and supplementary).\nFelt could then move into evaluation against guidelines - in protocol, had mentioned waiting until after fully wrapped with the model, with rationale that it impacts on code timings, but on reflection, you could argue likewise for influence on timings of that evaluation if you waited before proceeding to it (e.g.¬†waiting for response) and had then had a gap from working on that model and were no longer as familiar."
  },
  {
    "objectID": "logbook/posts/2024_07_10/index.html#check-the-inter-arrival-times",
    "href": "logbook/posts/2024_07_10/index.html#check-the-inter-arrival-times",
    "title": "Day 6",
    "section": "10.31-10.36: Check the inter-arrival times",
    "text": "10.31-10.36: Check the inter-arrival times\n\n# Set in reproduction.qmd\ned_pt = 107700\ninr_pt = 104\neir_pt= 468\nir_pt = 3805\n\n# Set in model.R\nst_pt = 750\nais_pt = 450\necr_pt = 58\n\n# Calculate inter-arrival times (as from model.R)\nyear2min = 525600\nI_ED  = round(year2min/ed_pt)\nI_ST  = round(year2min/st_pt)\nI_AIS = round(year2min/ais_pt)\nI_ECR = round(year2min/ecr_pt)\nI_INR = round(year2min/inr_pt)\nI_EIR = round(year2min/eir_pt)\nI_IR  = round(year2min/ir_pt)\n\n# View calculated inter-arrival times\nprint(c(I_ED, I_ST, I_AIS, I_ECR, I_INR, I_EIR, I_IR))\n\n[1]    5  701 1168 9062 5054 1123  138\n\n\nThese match up with the times from the paper, as in the image below from Huang et al. (2019).\n\n\n\nTable 1"
  },
  {
    "objectID": "logbook/posts/2024_07_10/index.html#check-distributions-and-length-of-resources",
    "href": "logbook/posts/2024_07_10/index.html#check-distributions-and-length-of-resources",
    "title": "Day 6",
    "section": "10.51-12.02, 12.12-12.15: Check distributions and length of resources",
    "text": "10.51-12.02, 12.12-12.15: Check distributions and length of resources\nAs a reminder, this is the set-up of the model, with Figure 1 from Huang et al. (2019). There are several resources, including single plane (angioIR) and biplane (angioINR) angiography suites.\n\n\n\nFigure 1\n\n\nEmergency arrival (potential stroke) patients:\n\nStart as emergency arrival (new_patient_traj)\nBecome either a stroke patient (stroke_traj) or non-stroke patient (nonstroke_traj)\nThe stroke patients will then become either AIS (acute ischaemic stroke) (ais_traj) or non-AIS (timeout then leave)\nThe AIS patients will then become either ECR (endovascular clot retrieval) (ecr_traj) or TPA (tissue plasminogen activator) only (timeout then leave)\n\nOther patients (pathways included as they share resources with stroke pathway):\n\nInterventional radiology patients (ir_traj)\nEmergency interventional radiology patients (eir_traj)\nInterventional neuroradiology patients (inr_traj)\n\n\nEmergency arrival patient sampling / distributions / length\n\nModel\nEmergency arrivals (new_patient_traj):\n\nadd_generator(\"pt_ed\", new_patient_traj, function() rpois(1, I_ED) )\n\nWhere I_ED  = round(year2min/ed_pt) = 5\n\nTime with ed_staff: timeout(function() rnorm(1, 20,10)) (sample 1 from normal distribution with mean 20 and sd 10)\nProbability of stroke: sample(1:2, 1, prob = c(PROB_STROKE, (1-PROB_STROKE) )\n\nWhere PROB_STROKE = st_pt / ed_pt, which is 750/107700=0.006963788 (so probability 0.007, or 0.7%)\nInterestingly, the inter-arrival time calculated for stroke (I_ST  = round(year2min/st_pt)) is not used, and instead, the arrival of stroke patients is based on this probability sampling\n\n\nNon-stroke patients (nonstroke_traj):\n\nProbability of discharge vs ct review: sample(1:2, 1, prob = c(.9, .1)) so 0.9 or 90% leave, and then 10% get CT review before leave\nDischarge: timeout(1)\nCT review: timeout(20)\n\nStroke patients (stroke_traj):\n\nTime with stroke doctor: timeout(function() rnorm(1, 30, 10))\nCT time: timeout(function() rnorm(1, 20,10))\nProbability of AIS: sample(1:2, 1, prob = c(PROB_AIS, (1-PROB_AIS)))\n\nWhere PROB_AIS = ais_pt / st_pt = 450/750 = 0.6 (or 60%)\n\nNot ais: timeout(1)\n\nAIS patients:\n\nProbability of ECR: sample(1:2, 1, prob = c(PROB_ECR, (1-PROB_ECR))\n\nWhere PROB_ECR = ecr_pt / ais_pt = 58/450 = 0.1288889 (probability 0.13, or 13%)\n\nTPA only: timeout(1)\n\nECR patients:\n\nAngioINR time (uses angio_inr, inr, and 3 angio_staff): timeout(function() rnorm(1, 120,60))\n\n\n\nPaper\n‚ÄúThe stroke pathway begins with a new patient in the Emergency Department (ED) and ends with the patient ‚Äúseizing‚Äù an angioINR, an INR and angio staff which represents nurses and technologists. The patient must proceed through a sequence of events chronologically as follows: triage in ED, assessment by the stroke team, CT imaging, assessment for ECR eligibility and lastly, acquiring ECR resources (Figure 1). The decision to proceed to the next event is probabilistic and is acquired from logged data from a Comprehensive Stroke Service in Melbourne, Australia, between 2016 and 17 (Table 1).‚ÄùHuang et al. (2019)\nAs it stands, Table 1 just contains the number of resources and patients - but, from this paragraph, it appears it might previously have included some of these probabilities.\nI had a look online to see if I could find any pre-prints. I came across a poster abstract, but otherwise nothing that could help elucidate this. I also looked for the data from the Comprehensive Stroke Service (although I couldn‚Äôt easily come across anything with patient counts, and wasn‚Äôt certain this information would definitely be public, so limited search).\nI looked the the model on CLOUDES, and this had different parameters (although this might just be illustrative). But, for example:\n\nED arrivals - poisson with IAT 10 and 2 entities per arrival- similar to model (poisson with IAT 5 and 1 entity per arrival)\nED triage - normal mean 15 stdev 5 - differs from model (mean 20 sd 10)\nProbability stroke 0.7 (and 99.3 leave) - same as model\nTime with stroke doctor normal mean 30 sd 10 - same as model\nCT normal mean 20 sd 10 - same as model\nAIS probability 15 (and 85 leave) and then LVO probability 60 (and 40 leave) (which is described as probabilitiy true AIS) - differs from model (simply, from those who received the CT, 60% AIS and 40% exit)\nECR probabiltiy 15 (and 85 leave) - differs from model (13% ECR)\nAngioINR normal mean 120 sd 60 - same as model\n\nHowever, several of them are the same, so it seems it would be worth running the model with those parameters.\n\n\n\nOther patients\n\nModel\nInterventional radiology patients (ir_traj):\n\nadd_generator(\"pt_ir\", ir_traj, function() rpois(1, I_IR) )\n\nWhere I_IR  = round(year2min/ir_pt) = 138\n\nAngio staff time: timeout(function() rnorm(1, 20,10))\nAngioINR/IR time (uses angio_inr or angio_ir, plus ir and 3 angio_staff): timeout(function() rnorm(1, 60,30))\n\nEmergency interventional radiology patients (eir_traj):\n\nadd_generator(\"pt_eir\", eir_traj, priority = 1, function() rpois(1, I_EIR) )\n\nWhere I_EIR = round(year2min/eir_pt) = 1123\n\nAngio staff time: timeout(function() rnorm(1, 20,10))\nAngioINR/IR time (uses angio_inr or angio_ir, plus ir and 3 angio_staff): timeout(function() rnorm(1, 60,30))\n\nInterventional neuroradiology patients (inr_traj): * add_generator(\"pt_inr\", inr_traj, function() rpois(1, I_INR) ) * Where I_INR = round(year2min/inr_pt) = 5054 * Angio staff time: timeout(function() rnorm(1, 20,10)) * AngioINR time (uses angio_inr, inr and 3 angio_staff): timeout(function() rnorm(1, 60,30))\n\n\nCLOUDES\n\nNon-emergency IR arrivals - poisson IAT 120 1 entity - differs from model (138)\nEmergency IR arrivals - poisson IAT 1120 1 entity - differs from model (1123)\nNon-emergency INR arrivals - poisson IAT 5040 1 entity - differs from model (5040)\nTime with angio staff: normal mean 20 sd 10 - same as model\nRouting to rooms: non-emergency IR check for angio room for IR (which chooses between IR and INR based on shortest queue), non-emergency go into INR queue, doesn‚Äôt have route for emergency IR - differs from model but looks like this is due to limitation of software in only letting you choose one patient type or all patients\nAngio INR time - normal mean 120 sd 60 - differs from model (mean 60 sd 30) but this again might be limitation of software (only allowing one time length regardless of patient type)\nAngio IR time - normal mean 60 sd 30 - same as model\n\n\n\n\nReflections from this\nSome of these differences appear to be rounding/simplifying numbers, or limitations of the CLOUDES software. However, some are more different. My logic here is that the model code we have is for the app and some parameters differed to the paper - so I‚Äôm anticipating it‚Äôs possible that some of these other parameters may have differed too (but cannot confirm due to them not being reported in the paper). However, if there‚Äôs a chance that the CLOUDES model was based on the paper parameters (rather than app), there‚Äôs a chance it could help us match up? This seems unlikely though (given it accompanies the app).\nHowever, the only one of real interest I think (that is not simplification or limitation) is the difference in ED triage time."
  },
  {
    "objectID": "logbook/posts/2024_07_10/index.html#varying-ed-triage-length",
    "href": "logbook/posts/2024_07_10/index.html#varying-ed-triage-length",
    "title": "Day 6",
    "section": "13.00-13.15: Varying ED triage length",
    "text": "13.00-13.15: Varying ED triage length\nI modified the model.R so I could easily change the ED triage mean and SD, then ran a scenario where these were 15 and 5. However, that didn‚Äôt make much difference.\n\n\n\nFigure 2A with ED triage from CLOUDES"
  },
  {
    "objectID": "logbook/posts/2024_07_10/index.html#double-check-category-being-presented",
    "href": "logbook/posts/2024_07_10/index.html#double-check-category-being-presented",
    "title": "Day 6",
    "section": "13.16-13.28: Double check category being presented",
    "text": "13.16-13.28: Double check category being presented\nI‚Äôm pretty sure I‚Äôm presenting the right category (ED), but I looked at presenting wait times from patients in any category, or in each of the other categories.\nI temporarily removed the filtering from run_model() and then ran:\nbaseline &lt;- run_model(seed = SEED)\n\np1 &lt;- create_plot(baseline,\n                  group=\"resource\",\n                  title=\"All patients\")\np2 &lt;- create_plot(baseline %&gt;% filter(category == \"ed\"),\n                  group=\"resource\",\n                  title=\"ED\")\np3 &lt;- create_plot(baseline %&gt;% filter(category == \"ir\"),\n                  group=\"resource\",\n                  title=\"IR\")\np4 &lt;- create_plot(baseline %&gt;% filter(category == \"eir\"),\n                  group=\"resource\",\n                  title=\"EIR\")\np5 &lt;- create_plot(baseline %&gt;% filter(category == \"inr\"),\n                  group=\"resource\",\n                  title=\"INR\")\n\n# Arrange in a single figure\nggarrange(p1, p2, p3, p4, p5, nrow=1, ncol=5,\n          common.legend=TRUE, legend=\"bottom\")\nggsave(\"fig2a_categories.png\", width=18)\nThis supports that ED is the correct choice (the only other similar is EIR but logically, it does still make sense to be ED, and it doesn‚Äôt happen to be that EIR is a great match either, just similar).\n\n\n\nFigure 2A categories"
  },
  {
    "objectID": "logbook/posts/2024_07_10/index.html#double-check-inr-room-options",
    "href": "logbook/posts/2024_07_10/index.html#double-check-inr-room-options",
    "title": "Day 6",
    "section": "13.39-13.43: Double-check INR room options",
    "text": "13.39-13.43: Double-check INR room options\nLooks right compared with paper, can‚Äôt spot any issues"
  },
  {
    "objectID": "logbook/posts/2024_07_10/index.html#vary-length-of-resources-to-try-to-engineer-results",
    "href": "logbook/posts/2024_07_10/index.html#vary-length-of-resources-to-try-to-engineer-results",
    "title": "Day 6",
    "section": "13.44-14.10, 14.15-14.45: Vary length of resources to try to engineer results",
    "text": "13.44-14.10, 14.15-14.45: Vary length of resources to try to engineer results\nI can see what looks wrong in each of the figures and so, one option, is to see if I could easily attempt to engineer the results by varying the parameters slightly, to see what might make it look similar.\nLooking at Figure 2A as an example:\n\nI have lower AngioINR queue density and no visible angio staff queues (should be queues)\n\nCould try increasing the number of patients accessing the angioINR\n\nThere are INR queues (when should be none)\n\nCould try either having ED patients not use INR, or having more INR availability\n\nCT, ED staff and stroke doctor queues are similar\n\nI ran a few quick models (3 replications), just to see what comes out.\nrun_model(nsim=3, seed = SEED, ed_pt = 107700*2)\nDoubled the number of emergency department arrivals. This increased angio INR queue but moved CT and ED staff away from desired. Interestingly, no impact on angio staff.\n\n\n\nDouble ED\n\n\nrun_model(nsim=3, seed = SEED, angio_staff = 3)\nReduce number of angio staff to 3 during day, which had large impact on angioINR and INR queues, but still no visible angio staff queue.\n\n\n\nHalve angio daytime staff\n\n\nDouble ED AND less staff:\n\n\n\nDouble ED AND less staff\n\n\nLooking at model.R, these results are coming from the simpy resource itself, so this shouldn‚Äôt be due to any issues with the calculation of angio staff resource use.\nSome extra suggestions from quick chat with Tom:\n\nIncrease length of AngioINR appointment\nLook at the utilisation (e.g.¬†angio staff utilisation)\n\nHowever, if I just plot angio_staff (without the group by resource), I can see it! It just doesn‚Äôt appear in the other plot. I then realised that this is because the angio_staff and INR lines completely overlap. If we remove INR from the plot, it starts to look a bit more similar.\nHence, it seems that actually the main difference to the paper is just the angioINR queue."
  },
  {
    "objectID": "logbook/posts/2024_07_10/index.html#looking-into-figure-2c-and-3c-and-getting-in-text-result-2",
    "href": "logbook/posts/2024_07_10/index.html#looking-into-figure-2c-and-3c-and-getting-in-text-result-2",
    "title": "Day 6",
    "section": "16.03-16.19: Looking into Figure 2C and 3C, and getting in-text result 2!",
    "text": "16.03-16.19: Looking into Figure 2C and 3C, and getting in-text result 2!\nFigure 2C has double the machines but, in the paper, they have no change in angio staff levels, whilst I find that increases. That makes sense - with plenty of machines, the bottleneck is now on having the staff for those machines.\nI realised then, from reading back on the paper, that I should have replaced an angioIR machine with an angioINR machine (and not just add an extra angioINR machine).\n\n‚ÄúSecond, in the ‚Äútwo angioINRs‚Äù scenario, the angioIR is replaced with an angioINR, doubling angiography availability for ECR patients.‚Äù Huang et al. (2019)\n\nI changed this in reproduction.qmd (angio_inr=2, angio_ir=0) and re-ran the models for Figure 2 and 3. This fixed the (C) part of those figures to be more similar to the paper.\nThis then resolved in-text result 3, with a 4.3 minute reduction in the queue length (which is very similar to ‚Äú4 min less‚Äù). Hence, can consider that reproduced at this point!\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 975\n\n# Times from today\ntimes = [\n    ('09.18', '09.25'),\n    ('09.26', '09.32'),\n    ('09.38', '09.40'),\n    ('09.42', '10.00'),\n    ('10.09', '10.12'),\n    ('10.31', '10.36'),\n    ('10.51', '12.02'),\n    ('12.12', '12.15'),\n    ('13.00', '13.15'),\n    ('13.16', '13.28'),\n    ('13.39', '13.43'),\n    ('13.44', '14.10'),\n    ('14.15', '14.45'),\n    ('16.03', '16.19')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 218m, or 3h 38m\nTotal used to date: 1193m, or 19h 53m\nTime remaining: 1207m, or 20h 7m\nUsed 49.7% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_07_10/index.html#trying-to-raise-the-angioinr-queues",
    "href": "logbook/posts/2024_07_10/index.html#trying-to-raise-the-angioinr-queues",
    "title": "Day 6",
    "section": "16.25-17.00: Trying to raise the angioINR queues",
    "text": "16.25-17.00: Trying to raise the angioINR queues\nTried changing length of angio appointments for all non-ED patients to the same as ED patients - definitely not right!\n\n\n\nLonger angio\n\n\nShorterning the ED angio appointments to the non-ED length is also not helpful.\n\n\n\nShorter ED angio\n\n\nThen I ran through a bunch of different seeds, to see if that also could explain it. Some do come a little closer than others‚Ä¶ though this was only five replications. Should probably repeat this exercise, but with 30 replications!\n\n\n\nDifferent seeds\n\n\nplot_list &lt;- list()\ni &lt;- 0\nfor (s in seq(0, 800, 50)) {\n  i &lt;- i + 1\n  baseline &lt;- run_model(nsim=5, seed = s)\n  plot_list[[i]] &lt;- create_plot(baseline, group=\"resource\", title=\"\")\n}\nggarrange(plotlist=plot_list, common.legend=TRUE, legend=\"bottom\")\nggsave(\"../logbook/posts/2024_07_10/fig2a_5rep_diffseeds.png\", width=20, height=20)"
  },
  {
    "objectID": "logbook/posts/2024_07_10/index.html#timings",
    "href": "logbook/posts/2024_07_10/index.html#timings",
    "title": "Day 6",
    "section": "Timings",
    "text": "Timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 975\n\n# Times from today\ntimes = [\n    ('09.18', '09.25'),\n    ('09.26', '09.32'),\n    ('09.38', '09.40'),\n    ('09.42', '10.00'),\n    ('10.09', '10.12'),\n    ('10.31', '10.36'),\n    ('10.51', '12.02'),\n    ('12.12', '12.15'),\n    ('13.00', '13.15'),\n    ('13.16', '13.28'),\n    ('13.39', '13.43'),\n    ('13.44', '14.10'),\n    ('14.15', '14.45'),\n    ('16.03', '16.19'),\n    ('16.25', '17.00')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 253m, or 4h 13m\nTotal used to date: 1228m, or 20h 28m\nTime remaining: 1172m, or 19h 32m\nUsed 51.2% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_07_08/index.html",
    "href": "logbook/posts/2024_07_08/index.html",
    "title": "Day 4",
    "section": "",
    "text": "Note\n\n\n\nAdd seeds, got in-text result 1, working on Figure 2. Total time used: 13h 10m (32.9%)"
  },
  {
    "objectID": "logbook/posts/2024_07_08/index.html#continuing-on-in-text-results-1-and-2",
    "href": "logbook/posts/2024_07_08/index.html#continuing-on-in-text-results-1-and-2",
    "title": "Day 4",
    "section": "09.14-09.17, 09.22-09.24, 09.30-09.35: Continuing on in-text results 1 and 2",
    "text": "09.14-09.17, 09.22-09.24, 09.30-09.35: Continuing on in-text results 1 and 2\nRe-ran twice more to see again how much variation we get between runs, and how likely that could attribute for the difference against the paper. We saw-\n\n\n\n\n\n\n\n\n\n\n\nOutput\nResult 1 (Day 3)\nResult 2 (Day 3)\nResult 3 (Today)\nResult 4 (Today)\nPaper\n\n\n\n\nBaseline\n13.33 minutes\n13.65 minutes\n14.15 minutes\n14.09 minutes\n-\n\n\nExclusive\n8.58 minutes (4.75 reduction)\n9.20 minutes (4.45 reduction)\n8.79 minutes (5.36 reduction)\n8.05 minutes (6.04 reduction)\n6 minute reduction from baseline\n\n\nTwo AngioINR\n14.86 minutes (1.53 increase)\n13.61 minutes (0.04 reduction)\n14.37 minutes (0.22 increase)\n14.04 minutes (0.05 reduction)\n4 minute reduction from baseline\n\n\n\nBased on this, it‚Äôs reasonable to assume that a 6 minute reduction can be observed within the variation of model runs (in-text result 1), but that the two angioINR scenario is not matching up.\n\n\n\n\n\n\nReflections\n\n\n\nEnvironment used does not match up to paper - paper use Simmer version 4.1.0, and otherwise, other versions of packages and of R being used are more recent than publication. It is unlikely that differences in results are due to this (although not impossible). Note trying to revert the environment to older versions as a possible troubleshooting strategy if issues persist, but not yet, due to major challenges found in trying to do so prior."
  },
  {
    "objectID": "logbook/posts/2024_07_08/index.html#adding-seeds",
    "href": "logbook/posts/2024_07_08/index.html#adding-seeds",
    "title": "Day 4",
    "section": "09.50-10.49, 11.02-11.05, 11.13-11.14: Adding seeds",
    "text": "09.50-10.49, 11.02-11.05, 11.13-11.14: Adding seeds\nBased on this tutorial, add seeds to the model. This is because the result was only returned by certain runs of the model and not others, so want to add seeds now so can give a seed for which the result is reproduced. I installed simEd - renv::install(\"simEd\") and add to DESCRIPTION and renv::snapshot() - and then made the following changes to the model:\n\nlibrary(simEd)\nInput seed to function which becomes SEED, then set.seed(SEED+i) within model replications\nSampling functions changed from r to v - i.e.¬†rpois() to vpois(), with incremental stream numbers\n\nI tried running baseline, but it took a long time - after 6 minutes, it was still running (which is normally how long the whole script takes). I interrupted it and it returned Error : object 'shifts' not found. However, no change has been made to shifts code. I ran a short section of code practicing sampling and this worked fine:\nlibrary(simEd)\n\ned_pt = 107000\nyear2min = 525600\nI_ED  = round(year2min/ed_pt)\n\nset.seed(5)\nvpois(10, I_ED, stream=1)\n\nset.seed(3)\nvpois(10, I_ED, stream=1)\n\nset.seed(5)\nvpois(10, I_ED, stream=1)\nI then tried running it with 3 replications instead of 30 (baseline &lt;- run_model(nsim=3, seed=100)), and that ran fine, so it appears that introducing this library just slowed down the model alot, as 3 replications could complete in 40 seconds.\nI looked into changing the lapply() in model.R to a parallel version:\n\nparLapply requires you to specify every variable to be included, plus additional lines of code to set up and close clusters\nmcapply() just requires you to change lapply\n\nHence, I tried mcapply, but it returned Error: external pointer is not valid, which was resolved based on this post by adding wrap(). However, learnt that mclapply wouldn‚Äôt work on Windows. Moreover, it still took a fair while to run (testing with 30 replications, it‚Äôs still going at 4 minutes).\nAs such, removed simEd from model.R and environment and returned to rpois(), and instead just set a simple seed without controlling streams. The time for this to run was as per usual, which was fab. I ran the baseline model twice with the same seed and compared the results, and it came out looking (by eye, at the processed results) identical.\nI therefore ran baseline and exclusive with three different starter seeds, and the seed 200 came out closest to the paper -\n\nBaseline: 13.96 minutes\nExclusive: 8.12 minutes\nDifference: 5.84 minutes\n\nHence, I feel we can mark in-text result 1 as reproduced at this time (11.14), with starter seed of 200.\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 443\n\n# Times from today\ntimes = [\n    ('09.14', '09.17'),\n    ('09.22', '09.24'),\n    ('09.30', '09.35'),\n    ('09.50', '10.49'),\n    ('11.02', '11.05'),\n    ('11.13', '11.14')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 73m, or 1h 13m\nTotal used to date: 516m, or 8h 36m\nTime remaining: 1884m, or 31h 24m\nUsed 21.5% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_07_08/index.html#working-on-figure-2",
    "href": "logbook/posts/2024_07_08/index.html#working-on-figure-2",
    "title": "Day 4",
    "section": "11.15-12.30, 13:15-13.50, 13.55-14.55: Working on Figure 2",
    "text": "11.15-12.30, 13:15-13.50, 13.55-14.55: Working on Figure 2\nFigure 2 uses the results from the scenarios above but creates plots where:\n\nX axis is wait time in minutes (on a non-linear scale)\nY axis is standardised density of patients in queue, from 0 to 1 (on a non-linear scale)\n\ni.e.¬†‚ÄúProbability density of patients who are waiting standardised to patients who are not waiting‚Äù\ni.e.¬†‚ÄúTo facilitate graphical and descriptive comparison across models, we express waiting times as relative probabilities of waiting a given amount of time, compared to not waiting at all. Since most patients accessed services without waiting, wait time densities could be directly compared across simulations after this normalization.‚Äù\n\n\nIt‚Äôs not immediately clear exactly what this means, but I‚Äôll start with creating a density plot of waiting times for one of the resources. First though, I add some code to save the model results to CSV files so that we don‚Äôt have to re-run the model each time (since with seeds added, it should now come out the same each time anyway). I initially saved these with write.csv() but it was too slow, so then (based on this tutorial), I switched to data.table::fwrite() (‚Äúfast CSV writer‚Äù), which was much much better! Hence, used fread() to import (as should also be quicker, based on this tutorial).\nI then created a basic density plot with ggplot with ED AngioINR untransformed wait times.\nbase_angio &lt;- res_base %&gt;% filter(category == \"ed\") %&gt;% filter(resource == \"angio_inr\")\np &lt;- ggplot(base_angio, aes(x = wait_time)) +\n  geom_density()\nggsave(path_fig2a)\np\n\n\n\nFigure 2A raw wait times\n\n\n\nY axis\nI played around with various transformations, as it wasn‚Äôt immediately clear to me how they had stretched the y axis, including creating custom functions, transforming the data directly, and trying out default transform options. I eventually stumbled across scale_y_continuous(transform=\"sqrt\"), which matched up to the axis in the paper.\n\n\nStandardising the density\nI played around with a few different transformation as I tried to work out what they meant by standardised density of patients in queue. Whilst converting raw wait times to probabilities, I noticed a bunch of ever so slightly negative wait times, but given these are very small (i.e.¬†0.0000000‚Ä¶), I am not concerned.\nOne thing I tried was converting each wait time into a probability of that wait time (e.g.¬†rounding each to 2dp, then 0 wait time = probability 0.68).\n# Filter to just AngioINR for ED and round wait times to 2dp\nbase_angio &lt;- res_base %&gt;%\n  filter(category == \"ed\", resource == \"angio_inr\") %&gt;%\n  select(wait_time)\n\n# Round to 2dp\nbase_angio$wait_time &lt;- round(base_angio$wait_time, 2)\n\n# Convert raw wait times into probability of waiting that long given all\n# wait times observed\nprob_wait &lt;- base_angio %&gt;%\n  group_by(wait_time) %&gt;%\n  summarise(count = n()) %&gt;%\n  mutate(probability = count / sum(count)) %&gt;%\n  select(wait_time, probability)\n\nggplot(prob_wait, aes(x=wait_time, y=probability)) + geom_line() + geom_point()\nHowever, that really didn‚Äôt look quite right.\n\n\n\nFigure 2A wrong transformation\n\n\nLooking at the curve with the raw wait times, the shape of the curve is more similar to the paper, just with different y axis and stretched. Revisiting the paper description, it is the ‚Äúrelative probabilities of waiting a given amount of time, compared to not waiting at all‚Äù. So, it‚Äôs not just the relative probability of waiting a given amount of time, compared to any other time.\nI created a plot where the waiting times were normalised in such a way that the values range from 0 to 1, which starts to look a bit more similar to the paper -\n# Filter to just AngioINR for ED and round wait times to 2dp\nbase_angio &lt;- res_base %&gt;%\n  filter(category == \"ed\", resource == \"angio_inr\")\n\n# Set negative wait times to 0\nbase_angio$wait_time[base_angio$wait_time &lt; 0] &lt;- 0\n\n# Create the density data\ndensity_data &lt;- density(base_angio$wait_time)\n\n# Normalize the density values\nnormalized_density &lt;- density_data$y / max(density_data$y)\n\n# Create a data frame with the normalized density values\ndensity_df &lt;- data.frame(x = density_data$x, y = normalized_density)\n\n# Plot using ggplot2\nggplot(density_df, aes(x = x, y = y)) +\n  geom_line() +\n  scale_y_continuous(transform=\"sqrt\")\nggsave(path_fig2a)\n\n\n\nFigure 2A scaled to 0 to 1\n\n\nI then tried creating a dataframe of counts for each wait time, then calculated probability based on number of people with no wait time. However, many were tiny (as count e.g.¬†1 of wait time 0.00000000000002842171). Tried it with rounding first. However, it is still then the same, as most are just 0, and then e.g.¬†1 wait time 0.2, 3 wait time 0.5.\n# Filter to just AngioINR for ED and round wait times to 2dp\nbase_angio &lt;- res_base %&gt;%\n  filter(category == \"ed\", resource == \"angio_inr\")\n\n# Set negative wait times to 0\nbase_angio$wait_time[base_angio$wait_time &lt; 0] &lt;- 0\n\n# Round everything to 1dp\nbase_angio$wait_time &lt;- round(base_angio$wait_time, 1)\n\n# Get probability of no wait time\nn_zero = length(which(base_angio$wait_time == 0))\nprob_zero = n_zero / nrow(base_angio)\n\n# Convert dataframe to counts of each wait time\nwait_df = base_angio %&gt;%\n  group_by(wait_time) %&gt;%\n  summarise(count=n())\nI tried transforming by the density of 0 (density_data$y[which.min(abs(density_data$x - 0))]) but that worked out to just be the same as max(density_data$y), since 0 has the max density.\nI tried transforming the x axis, which also appears to be a sqrt transformation, although this has an issue of introducing Inf values and losing where x=0 and density=1. I explored a few different ways of doing this transformation to see if anything helps"
  },
  {
    "objectID": "logbook/posts/2024_07_08/index.html#research-into-transformations",
    "href": "logbook/posts/2024_07_08/index.html#research-into-transformations",
    "title": "Day 4",
    "section": "15.10-15.30: Research into transformations",
    "text": "15.10-15.30: Research into transformations\nAs I‚Äôm struggling with these transformations - to the x axis, and to the probability density function. As such, it seems a good idea to do a bit more research into these and what exactly they are doing, to see if that helps.\n\nSquare root axis transformation\nI read a few articles and looked at the documentation for the square root transformation, and understand that this simply applying the sqrt() function.\nYou get the same graph if you do this:\ndensity_df %&gt;%\n  mutate(x_sqrt = sqrt(x)) %&gt;%\n  ggplot(aes(x=x_sqrt, y=y)) + geom_line() + xlim(0, sqrt(200)) + scale_y_continuous(transform=\"sqrt\")\nThe only difference is the x axis labels - when we use the ggplot axis transformation, it keeps the old labels to maintain interpretation of the original data.\n\n\nDensity functions\nA probability density function is used to describe a continuous distribution. It can be used to find the likelihood of values of a continuous random variable.\nggplot::geom_density() is described as plotting a smoothed version of the histogram."
  },
  {
    "objectID": "logbook/posts/2024_07_08/index.html#returning-to-figure-2",
    "href": "logbook/posts/2024_07_08/index.html#returning-to-figure-2",
    "title": "Day 4",
    "section": "15.31-16.55: Returning to Figure 2",
    "text": "15.31-16.55: Returning to Figure 2\nI add the sqrt x axis transformation to the basic density plot, and suddenly got a result that looked alot like the article! The only differences are the range of each axis, and the min/max values for y (ranges from 0 to 0.2‚Ä¶)\n# Filter to just AngioINR for ED and round wait times to 2dp\nbase_angio &lt;- res_base %&gt;%\n  filter(category == \"ed\", resource == \"angio_inr\")\n\n# Set negative wait times to 0\nbase_angio$wait_time[base_angio$wait_time &lt; 0] &lt;- 0\n\nggplot(base_angio, aes(x = wait_time)) +\n  geom_density() +\n  scale_y_continuous(transform=\"sqrt\") +\n  scale_x_continuous(transform=\"sqrt\")\n.png\nI tried out using previous transforms but they didn‚Äôt look right. Then I came across this stack Overflow post which suggested you can scale the density estimate to a maximum of one by inputting ..scaled... This is the computed ..scaled.. value from geom_density() which provides the density estimate scaled to a maximum of 1. From the documentation, can see that ..scaled.. has been replaced with after_stat(scaled).\nThis is however assuming that scaling to 1 is the same as scaling by probability of 0 wait time (which is at least true in this case, as we saw above).\n# Filter to just AngioINR for ED and round wait times to 2dp\nbase_angio &lt;- res_base %&gt;%\n  filter(category == \"ed\", resource == \"angio_inr\")\n\n# Set negative wait times to 0\nbase_angio$wait_time[base_angio$wait_time &lt; 0] &lt;- 0\n\n# Create the plot, scaling the density estimate to a maximum of 1\nggplot(base_angio, aes(x=wait_time, y=after_stat(scaled))) +\n  geom_density() +\n  scale_y_continuous(transform=\"sqrt\") +\n  scale_x_continuous(transform=\"sqrt\")\n\n\n\nFigure 2A example 5\n\n\nI tried adding all the resources in to the plots, and converting it into a function so I can apply it to all three dataframes. To easily show the plots side-by-side with a shared legend, I installed the package ggpubr.\nInstallation of ggpubr failed with message ERROR: configuration failed for package ‚Äònloptr‚Äô. It suggested I install cmake so, as prompted, I ran sudo apt install cmake. This then installed fine.\nCreating the plots and making various tweaks to the plotting and appearance, we‚Äôre getting a bit closer to the paper.\ncreate_plot &lt;- function(df, title, xlim=c(0, 200)) {\n  #' Create sub-plots for Figure 2A\n  #' \n  #' @param df Dataframe with wait times across replications\n  #' @param xlim Tuple with limits for x axis\n\n  # Filter to just ED\n  base_angio &lt;- df %&gt;%\n    filter(category == \"ed\")\n  \n  # Set negative wait times to 0\n  base_angio$wait_time[base_angio$wait_time &lt; 0] &lt;- 0\n  \n  # Create the plot, scaling the density estimate to a maximum of 1\n  ggplot(base_angio, aes(x = wait_time,\n                         colour = resource,\n                         y = after_stat(scaled))) +\n    geom_density() +\n    # Apply square transformation to each axis, removing x points beyond limits\n    scale_y_continuous(transform = \"sqrt\") +\n    scale_x_continuous(transform = \"sqrt\",\n                       breaks = scales::breaks_width(50),\n                       limits = xlim,\n                       oob = scales::censor,\n                       guide = guide_axis(angle=45)) +\n    # Titles and styling\n    ggtitle(title) +\n    xlab(\"\") +\n    ylab(\"\") +\n    theme_bw(base_size=10)\n}\n\np1 &lt;- create_plot(res_base, title=\"Baseline\")\np2 &lt;- create_plot(res_exc, title=\"Exclusive-use\", xlim=c(0, 250))\np3 &lt;- create_plot(res_two, title=\"Double angio INRs\")\nggarrange(p1, p2, p3, nrow=1, common.legend=TRUE, legend=\"bottom\", labels=c(\"A\", \"B\", \"C\"))\nggsave(path_fig2a)\n\n\n\nFigure 2A example 6"
  },
  {
    "objectID": "logbook/posts/2024_07_08/index.html#timings",
    "href": "logbook/posts/2024_07_08/index.html#timings",
    "title": "Day 4",
    "section": "Timings",
    "text": "Timings\n\n# Minutes used prior to today\nused_to_date = 443\n\n# Times from today\ntimes = [\n    ('09.14', '09.17'),\n    ('09.22', '09.24'),\n    ('09.30', '09.35'),\n    ('09.50', '10.49'),\n    ('11.02', '11.05'),\n    ('11.13', '11.14'),\n    ('11.15', '12.30'),\n    ('13.15', '13.50'),\n    ('13.55', '14.55'),\n    ('15.10', '15.30'),\n    ('15.31', '16.55')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 347m, or 5h 47m\nTotal used to date: 790m, or 13h 10m\nTime remaining: 1610m, or 26h 50m\nUsed 32.9% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_07_16/index.html",
    "href": "logbook/posts/2024_07_16/index.html",
    "title": "Day 10",
    "section": "",
    "text": "Note\n\n\n\nWorking on research compendium stage."
  },
  {
    "objectID": "logbook/posts/2024_07_16/index.html#untimed-research-compendium",
    "href": "logbook/posts/2024_07_16/index.html#untimed-research-compendium",
    "title": "Day 10",
    "section": "Untimed: Research compendium",
    "text": "Untimed: Research compendium\n\nParallel processing\nTried adding parallel processing in model.R to speed it up\n\nAdd future.apply to the environment\nplan(multisession, workers=max(availableCores()-5, 1))\nfuture_lapply()\nHowever, it took longer than usual! So I removed it\n\n\n\nReorganising\n\nMoved scripts into a scripts/ folder\nMoved help functions from reproduction.Rmd into seperate R script (primarily so can reuse in tests more easily)\n\n\n\nFix image size\nSet ggsave() image width as realised it otherwise varied with window size when running\n\n\nTests\nCreate tests to check model results are consistent\n\nStarted with creating a basic test saving tempfile csv and loading it to compare to another dataframe\nThen made a test with two example models being run for 3 replications and comparing results\nThen, set up with two files, as testthat can run files in parallel, and configured parallel processing. This involved:\n\nAdding Config/testthat/parallel: true to DESCRIPTION\nCreate project-specific environment file with nano reproduction/.Renviron and setting TESTTHAT_CPUS=4\n\nRan testthat::test_dir(\"tests\"), although seemed to just run sequentially. Confirmed by checking testthat::isparallel() which returned FALSE.\nTried adding Config/testthat/start-first: shifts, model to DESCRIPTION and it ignored the order, so it appears the issue is it is not using info from the DESCRIPTION file\nChecked version and it is correct for running in parallel (testthat&gt;=3.0.0)\nTried instead running testthat::test_local(), and moving tests into a folder testthat/, and this returned an error Could not find a root 'DESCRIPTION' file that starts with '^Package' in /home/amy/Documents/stars/stars-reproduce-huang-2019/reproduction.\nChanged DESCRIPTION to add Package and re-run - but this had error that installation of renv failed. Same error occurs if run testthat::test_dir(). It says to Try removing ‚Äò/home/amy/.cache/R/renv/library/reproduction-0912b448/linux-ubuntu-jammy/R-4.4/x86_64-pc-linux-gnu/00LOCK-renv‚Äô. I deleted this file (navigated there than rm -r 00LOCK-renv) then re-ran. However, this kept getting the same error message with that same file being created.\nTried removing Package from DESCRIPTION and running testthat::test_dir(\"tests/testthat\", load_package=\"none\") - but that ignores the order in DESCRIPTION\nTried testthat::test_dir(\"tests/testthat\", load_package=\"source\") which had error that Field 'Version' not found. Once I had this and re-ran, it ran the tests in the specified order! From Config/testthat/start-first: shifts, model\nI then add in Config/testthat/parallel: true and Config/testthat/edition: 3 but it had the same renv error as before\nThen decided to just run without parallel for now, so removed those lines from DESCRIPTION, deleted the .Renviron file, and put tests in a single file\n\nPackage: huang2019\nVersion: 0.1\nConfig/testthat/start-first: shifts, model\nConfig/testthat/parallel: true\nConfig/testthat/edition: 3\n\nCreated function to simplify testing, then wrote tests fora selection of scenarios (not all scenarios, to minimise run time).\nTest was failing with error of Length mismatch: comparison on first 2 components. I tried changing from expect_equal to using all.equal() and then expect_true(is_true()) on result. But this returned the same error!\nI tried running everything manually in the console so I could inspect the dataframes myself.\n\nfile = \"tests/testthat/expected_results/fig2_baseline.csv.gz\"\nexp &lt;- as.data.frame(data.table::fread(file))\ninputs=list(seed=200)\nresult &lt;- do.call(run_model, inputs)\n\nI realised the issue was that the expected result included a column shift where value throughout was 5pm. This was likely due to changing it at some point but not having re-run the whole script since, so I did that (and timed it!). I removed some of the model variants that aren‚Äôt to produce results from the paper (E.g. varying seeds)\n\nIt takes a while to run and, midway through, the R session encountered a fatal error and aborted. Tried again, and it failed again on exclusive_f5 &lt;- run_model(exclusive_use = TRUE, seed = SEED, fig5=TRUE).\nI‚Äôm suspecting this might be due to the size of the dataframes produced? So tried removing them from the environment after saving and ran again - but it still crashed, this time on the next run_model() statement\nI considered trying again with parallelisation but, given I hadn‚Äôt had much luck with that before, and given that the issue here is with R crashing (and so parallelisation actually may not help), I decided to instead split up reproduction.rmd into a few smaller files.\nI re-ran each of these in full, recording the run times.\n\n\n\n\nDocker\nUsed the RStudio documentation and this tutorial to write a Dockerfile."
  },
  {
    "objectID": "logbook/logbook.html",
    "href": "logbook/logbook.html",
    "title": "Logbook",
    "section": "",
    "text": "These diary entries record daily progress in reproduction of the study, providing a transparent and detailed record of work.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay 15\n\n\n\n\n\n\nevaluation\n\n\n\n\n\n\n\n\n\nAug 15, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 14\n\n\n\n\n\n\nreproduction\n\n\n\n\n\n\n\n\n\nAug 12, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 13\n\n\n\n\n\n\ncompendium\n\n\n\n\n\n\n\n\n\nJul 22, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 12\n\n\n\n\n\n\ncompendium\n\n\n\n\n\n\n\n\n\nJul 19, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 11\n\n\n\n\n\n\ncompendium\n\n\n\n\n\n\n\n\n\nJul 18, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 10\n\n\n\n\n\n\ncompendium\n\n\n\n\n\n\n\n\n\nJul 16, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 9\n\n\n\n\n\n\nguidelines\n\n\ncompendium\n\n\n\n\n\n\n\n\n\nJul 15, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 8\n\n\n\n\n\n\nreproduce\n\n\nguidelines\n\n\ncompendium\n\n\n\n\n\n\n\n\n\nJul 12, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 7\n\n\n\n\n\n\nreproduce\n\n\n\n\n\n\n\n\n\nJul 11, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 6\n\n\n\n\n\n\nreproduce\n\n\n\n\n\n\n\n\n\nJul 10, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 5\n\n\n\n\n\n\nsetup\n\n\nreproduce\n\n\n\n\n\n\n\n\n\nJul 9, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 4\n\n\n\n\n\n\nreproduce\n\n\n\n\n\n\n\n\n\nJul 8, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 3\n\n\n\n\n\n\nreproduce\n\n\n\n\n\n\n\n\n\nJul 5, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 2\n\n\n\n\n\n\nsetup\n\n\nscope\n\n\nreproduce\n\n\n\n\n\n\n\n\n\nJul 4, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 1\n\n\n\n\n\n\nsetup\n\n\n\n\n\n\n\n\n\nJul 3, 2024\n\n\nAmy Heather\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "evaluation/reflections.html",
    "href": "evaluation/reflections.html",
    "title": "Reflections",
    "section": "",
    "text": "This page contains reflections on the facilitators and barriers to this reproduction, as well as a full list of the troubleshooting steps taken to reproduce this work."
  },
  {
    "objectID": "evaluation/reflections.html#what-would-have-helped-facilitate-this-reproduction",
    "href": "evaluation/reflections.html#what-would-have-helped-facilitate-this-reproduction",
    "title": "Reflections",
    "section": "What would have helped facilitate this reproduction?",
    "text": "What would have helped facilitate this reproduction?\nProvide environment\n\nList all packages required\n\nProvide code that produces results from the paper\n\nThe provided code could easily get up and running to produce the application, but the paper was not focused on that, and was instead focussed on some specific scenarios. It took alot of work modifying and writing code to change it from producing an app to producing the paper results (running scenarios, saving results, processing results, creating figures).\nOften made mistakes in my interpretation for the implementation of scenarios, which could be avoided if code for those scenarios was provided\nFor one of the figures, it would have been handy if informed that plot was produced by a simmer function (as didn‚Äôt initially realise this)\n\nProvide all model parameters in the paper\n\nIn this case, patient arrivals and resource numbers were listed in the paper, and there were several discprenancies between this and the provided code. However, for many of the model parameters like length of appointment, these were not mentioned in the paper, and so it was not possible to confirm whether or not those were correct.\n\nAdd comments/docstrings to code\n\nTook some time to decipher and ensure I have correctly understood code as uses lots of abbreviations\n\nExplain calculations (or provide the code)\n\nIt took a bit of time for me to work out how to transform the Figure axes as this was not mentioned in the paper (and no code was provided for these)\nIt was also unclear and a bit tricky to work out how to standardise the density in the figures (since it is only described in the text and no formula/calculations are provided there or in the code)\n\nUse seeds\n\nIt does not appear that the original authors used seeds (not mentioned in paper or provided in code). This would be an issue, as it means variation between scenarios could be just due to randomness (although its possible they might have used them and just not mentioned/included anymore)\nFor reproducibility, providing seeds would‚Äôve been beneficial, as then I could be sure that my results do not differ from the original simply due to randomness\n\nNote: Didn‚Äôt end up needing to have older/similar versions of R and packages for it to work, and ended up using latest versions, due to challenges in installing older versions."
  },
  {
    "objectID": "evaluation/reflections.html#what-did-help-facilitate-it",
    "href": "evaluation/reflections.html#what-did-help-facilitate-it",
    "title": "Reflections",
    "section": "What did help facilitate it?",
    "text": "What did help facilitate it?\nNot hard coding some parameters\n\nThe model was set up as a function with several of the parameters provided as inputs to that function, which made it really easy to implement some of the scenarios programmatically.\n\nParameters in paper being in the format as needed to input to the model\n\nThe calculations for inter-arrival times were provided in the code, and the inputs to the code were the number of arrivals, as reported in the paper, and so making it easy to compare those parameters and check if numbers were correct or not."
  },
  {
    "objectID": "evaluation/reflections.html#full-list-of-troubleshooting-steps",
    "href": "evaluation/reflections.html#full-list-of-troubleshooting-steps",
    "title": "Reflections",
    "section": "Full list of troubleshooting steps",
    "text": "Full list of troubleshooting steps\n\n\n\n\n\n\nView list\n\n\n\n\n\nTroubleshooting steps are grouped by theme, and the day these occurred is given in brackets at the end of each bullet.\nI want to note that, disregarding my attempts to backdate R and the packages, the provided code was actually quite simple to get up and running as a shiny app. However, as the article is not about the app and instead focuses on results from particular scenarios, there was still work to be done to alter the code to get those results (rather than to get the app).\n\nEnvironment\nPackages required:\n\nNo environment file (2)\nDependencies based on server.R (2)\nAdd some extra dependencies to environment (not listed as import but appear when try to run - plyr, shiny) (3)\nAdd packages for creating the figures (ggpubr (which required sudo apt install cmake)) (4)\n\nVersions required (tried to use same versions of R and packages as they might have used, but couldn‚Äôt get this to work, and ended up using most recent):\n\nMentions version of Simmer in the paper (4.1.0) (2)\nInitially tried with package versions on or prior to 27th May 2019 (2)\nAttempted to use renv to build an environment with those package versions. Had error installing older versions of packages (e.g.¬†‚ÄúERROR: compilation failed for packager ‚Äòsimmer‚Äô‚Äù)\nAfter some trial-and-error, manager to switch to the older version of R (2+3)\nThen attempting to install the specific package versions, I got more erors (e.g.¬†‚ÄúWarning: failed to find source for ‚Äòsimmer.plot 0.1.15‚Äô in package repositories.‚Äù) (3)\nI tried installing them with the older version of R with no specific versions. Simmer install fine but simmer.plot failed as ‚ÄúError: package ‚Äòevaluate‚Äô is not available‚Äù (3)\nDecided to just try switching to the latest version of R and installing the latest versions of all the packages (3)\nHad issues adding the model to the quarto site as they were using different renv, and decided just to merge the quarto site dependencies into the model renv (3)\nAlthough using the latest versions of packages and R, I don‚Äôt feel discrepancies are likely due to this, as I would expect issues from environment to be more along the lines of code not running or quite minor differences (5)\n\n\n\nGet model code\n\nModel set-up to run as a shiny app - so extracted the simulate_nav() and plot_nav() functions from the shiny app and removed a few lines of code that were still calling shiny, so that these could run in a simple .Rmd file. (3)\n\n\n\nGet model parameters\n\nSeveral parameters differed between provided code and paper, so identified correct parameters based on paper‚Äôs Table 1 (3)\nInitially made a mistake with the INR staffing as had assumed to set inr_night = 0 as that is one INR staff 24 hours, but then realised they were on schedule so needs inr=1 and inr_night=1 to make one 24 hour staff member (3)\n\n\n\nRun scenarios\n\nCreated .Rmd file to programmatically run model scenarios. A facilitator for this was that the model was already set up as a function with many of the required parameters already set as inputs to that function - e.g.¬†two angioINRs easy to change (3)\nNo code was provided for the ‚Äúexclusive use‚Äù scenario, so add some to the model based on my understanding from the paper of that scenario (3)\nInitially, made a mistake in implementation of two angioINRs (human error) as double the machines rather than replacing the angioIR (6)\nInitially, also misinterpreted the supplementary figure scenario, as increased ED arrivals, instead of just directly changing the ECR numbers (7)\nHad issues getting same results for scenarios, and tried out various things including -\n\nChanging how INR staff are in model (no impact) (5)\nUsing default parameters from the code (rather than parameters from paper) (6)\nConfirming calculated inter-arrival times match up with paper (6)\nWent carefully over each trajectory, identifying the distributions used and lengths of resources. Not possible to check many of them though, as the paper only mentions arrivals (and not e.g.¬†sampling for length of appointment) (6)\nSearching for pre-prints (6)\nUsing ED triage time from model on CLOUDES (6)\nChecking outcome from non-ED categories (6)\nVarying parameters to see how that alters results - e.g.¬†length of resources, number of arrivals, number of resources,, changing which patients can use machines, and running with lots of different seeds (6+7)\n\n\n\n\nCreating outputs\n\nAdd code to model to save results to CSV so don‚Äôt have to re-run each time (4)\nAdd code to get mean waiting times (3+)\n\nIdentify that should filter to ed results (3)\nIdentify that these are mean and not median times (3)\n\nAdd code to create figures (3+)\n\nTook a while to figure out what transformations had been done to the Figure axes as this isn‚Äôt mentioned anyway - eventually realised it was a square root transformation (4)\nInitially struggled with understanding how to standardise the density, as it is an unfamiliar calculation and just described in the article. After some trial and error, I managed to get a similar-ish plot by scaling to a maximum of 1 using the built in ..scaled.. values from geom_density(). (4)\nThen tried doing it manually again, diving density at each time by density from wait time 0, and this matched up with results from geom_density() scaled, and hence giving me reassurance that the calculation is likely correct. (5)\nFor a while, didn‚Äôt realise angio_staff line in plots was being hidden under inr (6)\nFor figure 5, realised it was being created with a simmer function plot.resources.utilization (7)\n\n\n\n\nSeeds\n\nResults could vary quite a lot between seeds. Original paper does not have any control of seeds, but when I re-ran several times, could see alot of change in mean waiting times (4+5) - but not much for other outputs like Figure 2 (5)\nAdd seeds (initially tried with simEd, but too slow, so switched to simpler option of just setting a single seed without controlling seeds) (4)"
  },
  {
    "objectID": "evaluation/badges.html",
    "href": "evaluation/badges.html",
    "title": "Journal badges",
    "section": "",
    "text": "This page evaluates the extent to which the author-published research artefacts meet the criteria of badges related to reproducibility from various organisations and journals.\nCaveat: Please note that these criteria are based on available information about each badge online, and that we have likely differences in our procedure (e.g.¬†allowed troubleshooting for execution and reproduction, not under tight time pressure to complete). Moreover, we focus only on reproduction of the discrete-event simulation, and not on other aspects of the article. We cannot guarantee that the badges below would have been awarded in practice by these journals."
  },
  {
    "objectID": "evaluation/badges.html#criteria",
    "href": "evaluation/badges.html#criteria",
    "title": "Journal badges",
    "section": "Criteria",
    "text": "Criteria\n\n\nCode\nfrom IPython.display import display, Markdown\nimport numpy as np\nimport pandas as pd\n\n# Criteria and their definitions\ncriteria = {\n    'archive': 'Stored in a permanent archive that is publicly and openly accessible',\n    'id': 'Has a persistent identifier',\n    'license': 'Includes an open license',\n    'relevant': '''Artefacts are relevant to and contribute to the article's results''',\n    'complete': 'Complete set of materials shared (as would be needed to fully reproduce article)',\n    'structure': 'Artefacts are well structured/organised (e.g. to the extent that reuse and repurposing is facilitated, adhering to norms and standards of research community)',\n    'documentation_sufficient': 'Artefacts are sufficiently documented (i.e. to understand how it works, to enable it to be run, including package versions)',\n    'documentation_careful': 'Artefacts are carefully documented (more than sufficient - i.e. to the extent that reuse and repurposing is facilitated - e.g. changing parameters, reusing for own purpose)',\n    # This criteria is kept seperate to documentation_careful, as it specifically requires a README file\n    'documentation_readme': 'Artefacts are clearly documented and accompanied by a README file with step-by-step instructions on how to reproduce results in the manuscript',\n    'execute': 'Scripts can be successfully executed',\n    'regenerated': 'Independent party regenerated results using the authors research artefacts',\n    'hour': 'Reproduced within approximately one hour (excluding compute time)',\n}\n\n# Evaluation for this study\neval = pd.Series({\n    'archive': 0,\n    'id': 0,\n    'license': 1,\n    'relevant': 1,\n    'complete': 0,\n    'structure': 0,\n    'documentation_sufficient': 0,\n    'documentation_careful': 0,\n    'documentation_readme': 0,\n    'execute': 1,\n    'regenerated': 0,\n    'hour': 0,\n})\n\n# Get list of criteria met (True/False) overall\neval_list = list(eval)\n\n# Define function for creating the markdown formatted list of criteria met\ndef create_criteria_list(criteria_dict):\n    '''\n    Creates a string which contains a Markdown formatted list with icons to\n    indicate whether each criteria was met\n\n    Parameters:\n    -----------\n    criteria_dict : dict\n        Dictionary where keys are the criteria (variable name) and values are\n        Boolean (True/False of whether this study met the criteria)\n\n    Returns:\n    --------\n    formatted_list : string\n        Markdown formatted list\n    '''\n    callout_icon = {True: '‚úÖ',\n                    False: '‚ùå'}\n    # Create list with...\n    formatted_list = ''.join([\n        '* ' +\n        callout_icon[eval[key]] + # Icon based on whether it met criteria\n        ' ' +\n        value + # Full text description of criteria\n        '\\n' for key, value in criteria_dict.items()])\n    return(formatted_list)\n\n# Define groups of criteria\ncriteria_share_how = ['archive', 'id', 'license']\ncriteria_share_what = ['relevant', 'complete']\ncriteria_doc_struc = ['structure', 'documentation_sufficient', 'documentation_careful', 'documentation_readme']\ncriteria_run = ['execute', 'regenerated', 'hour']\n\n# Create text section\ndisplay(Markdown(f'''\nTo assess whether the author's materials met the requirements of each badge, a list of criteria was produced. Between each badge (and between categories of badge), there is often alot of overlap in criteria.\n\nThis study met **{sum(eval_list)} of the {len(eval_list)}** unique criteria items. These were as follows:\n\nCriteria related to how artefacts are shared -\n\n{create_criteria_list({k: criteria[k] for k in criteria_share_how})}\n\nCriteria related to what artefacts are shared -\n\n{create_criteria_list({k: criteria[k] for k in criteria_share_what})}\n\nCriteria related to the structure and documentation of the artefacts -\n\n{create_criteria_list({k: criteria[k] for k in criteria_doc_struc})}\n\nCriteria related to running and reproducing results -\n\n{create_criteria_list({k: criteria[k] for k in criteria_run})}\n'''))\n\n\nTo assess whether the author‚Äôs materials met the requirements of each badge, a list of criteria was produced. Between each badge (and between categories of badge), there is often alot of overlap in criteria.\nThis study met 3 of the 12 unique criteria items. These were as follows:\nCriteria related to how artefacts are shared -\n\n‚ùå Stored in a permanent archive that is publicly and openly accessible\n‚ùå Has a persistent identifier\n‚úÖ Includes an open license\n\nCriteria related to what artefacts are shared -\n\n‚úÖ Artefacts are relevant to and contribute to the article‚Äôs results\n‚ùå Complete set of materials shared (as would be needed to fully reproduce article)\n\nCriteria related to the structure and documentation of the artefacts -\n\n‚ùå Artefacts are well structured/organised (e.g.¬†to the extent that reuse and repurposing is facilitated, adhering to norms and standards of research community)\n‚ùå Artefacts are sufficiently documented (i.e.¬†to understand how it works, to enable it to be run, including package versions)\n‚ùå Artefacts are carefully documented (more than sufficient - i.e.¬†to the extent that reuse and repurposing is facilitated - e.g.¬†changing parameters, reusing for own purpose)\n‚ùå Artefacts are clearly documented and accompanied by a README file with step-by-step instructions on how to reproduce results in the manuscript\n\nCriteria related to running and reproducing results -\n\n‚úÖ Scripts can be successfully executed\n‚ùå Independent party regenerated results using the authors research artefacts\n‚ùå Reproduced within approximately one hour (excluding compute time)"
  },
  {
    "objectID": "evaluation/badges.html#badges",
    "href": "evaluation/badges.html#badges",
    "title": "Journal badges",
    "section": "Badges",
    "text": "Badges\n\n\nCode\n# Full badge names\nbadge_names = {\n    # Open objects\n    'open_niso': 'NISO \"Open Research Objects (ORO)\"',\n    'open_niso_all': 'NISO \"Open Research Objects - All (ORO-A)\"',\n    'open_acm': 'ACM \"Artifacts Available\"',\n    'open_cos': 'COS \"Open Code\"',\n    'open_ieee': 'IEEE \"Code Available\"',\n    # Object review\n    'review_acm_functional': 'ACM \"Artifacts Evaluated - Functional\"',\n    'review_acm_reusable': 'ACM \"Artifacts Evaluated - Reusable\"',\n    'review_ieee': 'IEEE \"Code Reviewed\"',\n    # Results reproduced\n    'reproduce_niso': 'NISO \"Results Reproduced (ROR-R)\"',\n    'reproduce_acm': 'ACM \"Results Reproduced\"',\n    'reproduce_ieee': 'IEEE \"Code Reproducible\"',\n    'reproduce_psy': 'Psychological Science \"Computational Reproducibility\"'\n}\n\n# Criteria required by each badge\nbadges = {\n    # Open objects\n    'open_niso': ['archive', 'id', 'license'],\n    'open_niso_all': ['archive', 'id', 'license', 'complete'],\n    'open_acm': ['archive', 'id'],\n    'open_cos': ['archive', 'id', 'license', 'complete', 'documentation_sufficient'],\n    'open_ieee': ['complete'],\n    # Object review\n    'review_acm_functional': ['documentation_sufficient', 'relevant', 'complete', 'execute'],\n    'review_acm_reusable': ['documentation_sufficient', 'documentation_careful', 'relevant', 'complete', 'execute', 'structure'],\n    'review_ieee': ['complete', 'execute'],\n    # Results reproduced\n    'reproduce_niso': ['regenerated'],\n    'reproduce_acm': ['regenerated'],\n    'reproduce_ieee': ['regenerated'],\n    'reproduce_psy': ['regenerated', 'hour', 'structure', 'documentation_readme'],\n}\n\n# Identify which badges would be awarded based on criteria\n# Get list of badges met (True/False) overall\naward = {}\nfor badge in badges:\n    award[badge] = all([eval[key] == 1 for key in badges[badge]])\naward_list = list(award.values())\n\n# Write introduction\n# Get list of badges met (True/False) by category\naward_open = [v for k,v in award.items() if k.startswith('open_')]\naward_review = [v for k,v in award.items() if k.startswith('review_')]\naward_reproduce = [v for k,v in award.items() if k.startswith('reproduce_')]\n\n# Create and display text for introduction\ndisplay(Markdown(f'''\nIn total, the original study met the criteria for **{sum(award_list)} of the {len(award_list)} badges**. This included:\n\n* **{sum(award_open)} of the {len(award_open)}** ‚Äúopen objects‚Äù badges\n* **{sum(award_review)} of the {len(award_review)}** ‚Äúobject review‚Äù badges\n* **{sum(award_reproduce)} of the {len(award_reproduce)}** ‚Äúreproduced‚Äù badges\n'''))\n\n# Make function that creates collapsible callouts for each badge\ndef create_badge_callout(award_dict):\n    '''\n    Displays Markdown callouts created for each badge in the dictionary, showing\n    whether the criteria for that badge was met.\n\n    Parameters:\n    -----------\n    award_dict : dict\n        Dictionary where key is badge (as variable name), and value is Boolean\n        (whether badge is awarded)\n    '''\n    callout_appearance = {True: 'tip',\n                          False: 'warning'}\n    callout_icon = {True: '‚úÖ',\n                    False: '‚ùå'}\n    callout_text = {True: 'Meets all criteria:',\n                    False: 'Does not meet all criteria:'}\n\n    for key, value in award_dict.items():\n        # Create Markdown list with...\n        criteria_list = ''.join([\n            '* ' +\n            callout_icon[eval[k]] + # Icon based on whether it met criteria\n            ' ' +\n            criteria[k] + # Full text description of criteria\n            '\\n' for k in badges[key]])\n        # Create the callout and display it\n        display(Markdown(f'''\n::: {{.callout-{callout_appearance[value]} appearance=\"minimal\" collapse=true}}\n\n## {callout_icon[value]} {badge_names[key]}\n\n{callout_text[value]}\n\n{criteria_list}\n:::\n'''))\n\n# Create badge functions with introductions and callouts\ndisplay(Markdown('''\n### \"Open objects\" badges\n\nThese badges relate to research artefacts being made openly available.\n'''))\ncreate_badge_callout({k: v for (k, v) in award.items() if k.startswith('open_')})\n\ndisplay(Markdown('''\n### \"Object review\" badges\n\nThese badges relate to the research artefacts being reviewed against criteria of the badge issuer.\n'''))\ncreate_badge_callout({k: v for (k, v) in award.items() if k.startswith('review_')})\n\ndisplay(Markdown('''\n### \"Reproduced\" badges\n\nThese badges relate to an independent party regenerating the reuslts of the article using the author objects.\n'''))\ncreate_badge_callout({k: v for (k, v) in award.items() if k.startswith('reproduce_')})\n\n\nIn total, the original study met the criteria for 0 of the 12 badges. This included:\n\n0 of the 5 ‚Äúopen objects‚Äù badges\n0 of the 3 ‚Äúobject review‚Äù badges\n0 of the 4 ‚Äúreproduced‚Äù badges\n\n\n\n‚ÄúOpen objects‚Äù badges\nThese badges relate to research artefacts being made openly available.\n\n\n\n\n\n\n\n\n‚ùå NISO ‚ÄúOpen Research Objects (ORO)‚Äù\n\n\n\n\n\nDoes not meet all criteria:\n\n‚ùå Stored in a permanent archive that is publicly and openly accessible\n‚ùå Has a persistent identifier\n‚úÖ Includes an open license\n\n\n\n\n\n\n\n\n\n\n\n\n‚ùå NISO ‚ÄúOpen Research Objects - All (ORO-A)‚Äù\n\n\n\n\n\nDoes not meet all criteria:\n\n‚ùå Stored in a permanent archive that is publicly and openly accessible\n‚ùå Has a persistent identifier\n‚úÖ Includes an open license\n‚ùå Complete set of materials shared (as would be needed to fully reproduce article)\n\n\n\n\n\n\n\n\n\n\n\n\n‚ùå ACM ‚ÄúArtifacts Available‚Äù\n\n\n\n\n\nDoes not meet all criteria:\n\n‚ùå Stored in a permanent archive that is publicly and openly accessible\n‚ùå Has a persistent identifier\n\n\n\n\n\n\n\n\n\n\n\n\n‚ùå COS ‚ÄúOpen Code‚Äù\n\n\n\n\n\nDoes not meet all criteria:\n\n‚ùå Stored in a permanent archive that is publicly and openly accessible\n‚ùå Has a persistent identifier\n‚úÖ Includes an open license\n‚ùå Complete set of materials shared (as would be needed to fully reproduce article)\n‚ùå Artefacts are sufficiently documented (i.e.¬†to understand how it works, to enable it to be run, including package versions)\n\n\n\n\n\n\n\n\n\n\n\n\n‚ùå IEEE ‚ÄúCode Available‚Äù\n\n\n\n\n\nDoes not meet all criteria:\n\n‚ùå Complete set of materials shared (as would be needed to fully reproduce article)\n\n\n\n\n\n\n‚ÄúObject review‚Äù badges\nThese badges relate to the research artefacts being reviewed against criteria of the badge issuer.\n\n\n\n\n\n\n\n\n‚ùå ACM ‚ÄúArtifacts Evaluated - Functional‚Äù\n\n\n\n\n\nDoes not meet all criteria:\n\n‚ùå Artefacts are sufficiently documented (i.e.¬†to understand how it works, to enable it to be run, including package versions)\n‚úÖ Artefacts are relevant to and contribute to the article‚Äôs results\n‚ùå Complete set of materials shared (as would be needed to fully reproduce article)\n‚úÖ Scripts can be successfully executed\n\n\n\n\n\n\n\n\n\n\n\n\n‚ùå ACM ‚ÄúArtifacts Evaluated - Reusable‚Äù\n\n\n\n\n\nDoes not meet all criteria:\n\n‚ùå Artefacts are sufficiently documented (i.e.¬†to understand how it works, to enable it to be run, including package versions)\n‚ùå Artefacts are carefully documented (more than sufficient - i.e.¬†to the extent that reuse and repurposing is facilitated - e.g.¬†changing parameters, reusing for own purpose)\n‚úÖ Artefacts are relevant to and contribute to the article‚Äôs results\n‚ùå Complete set of materials shared (as would be needed to fully reproduce article)\n‚úÖ Scripts can be successfully executed\n‚ùå Artefacts are well structured/organised (e.g.¬†to the extent that reuse and repurposing is facilitated, adhering to norms and standards of research community)\n\n\n\n\n\n\n\n\n\n\n\n\n‚ùå IEEE ‚ÄúCode Reviewed‚Äù\n\n\n\n\n\nDoes not meet all criteria:\n\n‚ùå Complete set of materials shared (as would be needed to fully reproduce article)\n‚úÖ Scripts can be successfully executed\n\n\n\n\n\n\n‚ÄúReproduced‚Äù badges\nThese badges relate to an independent party regenerating the reuslts of the article using the author objects.\n\n\n\n\n\n\n\n\n‚ùå NISO ‚ÄúResults Reproduced (ROR-R)‚Äù\n\n\n\n\n\nDoes not meet all criteria:\n\n‚ùå Independent party regenerated results using the authors research artefacts\n\n\n\n\n\n\n\n\n\n\n\n\n‚ùå ACM ‚ÄúResults Reproduced‚Äù\n\n\n\n\n\nDoes not meet all criteria:\n\n‚ùå Independent party regenerated results using the authors research artefacts\n\n\n\n\n\n\n\n\n\n\n\n\n‚ùå IEEE ‚ÄúCode Reproducible‚Äù\n\n\n\n\n\nDoes not meet all criteria:\n\n‚ùå Independent party regenerated results using the authors research artefacts\n\n\n\n\n\n\n\n\n\n\n\n\n‚ùå Psychological Science ‚ÄúComputational Reproducibility‚Äù\n\n\n\n\n\nDoes not meet all criteria:\n\n‚ùå Independent party regenerated results using the authors research artefacts\n‚ùå Reproduced within approximately one hour (excluding compute time)\n‚ùå Artefacts are well structured/organised (e.g.¬†to the extent that reuse and repurposing is facilitated, adhering to norms and standards of research community)\n‚ùå Artefacts are clearly documented and accompanied by a README file with step-by-step instructions on how to reproduce results in the manuscript"
  },
  {
    "objectID": "evaluation/badges.html#sources",
    "href": "evaluation/badges.html#sources",
    "title": "Journal badges",
    "section": "Sources",
    "text": "Sources\nNational Information Standards Organisation (NISO) (NISO Reproducibility Badging and Definitions Working Group (2021))\n\n‚ÄúOpen Research Objects (ORO)‚Äù\n‚ÄúOpen Research Objects - All (ORO-A)‚Äù\n‚ÄúResults Reproduced (ROR-R)‚Äù\n\nAssociation for Computing Machinery (ACM) (Association for Computing Machinery (ACM) (2020))\n\n‚ÄúArtifacts Available‚Äù\n‚ÄúArtifacts Evaluated - Functional‚Äù\n‚ÄúArtifacts Evaluated - Resuable‚Äù\n‚ÄúResults Reproduced‚Äù\n\nCenter for Open Science (COS) (Blohowiak et al. (2023))\n\n‚ÄúOpen Code‚Äù\n\nInstitute of Electrical and Electronics Engineers (IEEE) (Institute of Electrical and Electronics Engineers (IEEE) (n.d.))\n\n‚ÄúCode Available‚Äù\n‚ÄúCode Reviewed‚Äù\n‚ÄúCode Reproducible‚Äù\n\nPsychological Science (Hardwicke and Vazire (2023) and Association for Psychological Science (APS) (2023))\n\n‚ÄúComputational Reproducibility‚Äù"
  },
  {
    "objectID": "evaluation/reporting.html",
    "href": "evaluation/reporting.html",
    "title": "Reporting guidelines",
    "section": "",
    "text": "This page evaluates the extent to which the journal article meets the criteria from two discrete-event simulation study reporting guidelines:"
  },
  {
    "objectID": "evaluation/reporting.html#stress-des",
    "href": "evaluation/reporting.html#stress-des",
    "title": "Reporting guidelines",
    "section": "STRESS-DES",
    "text": "STRESS-DES\nOf the 24 items in the checklist:\n\n14 were met fully (‚úÖ)\n6 were partially met (üü°)\n3 were not met (‚ùå)\n1 was not applicable (N/A)\n\n\n\n\n\n\n\n\n\n\nItem\nRecommendation\nMet by study?\nEvidence\n\n\n\n\nObjectives\n\n\n\n\n\n1.1 Purpose of the model\nExplain the background and objectives for the model\n‚úÖ Fully\nIntroduction: ‚ÄúEndovascular clot retrieval (ECR) is the first-line treatment for acute ischemic stroke (AIS) due to arterial large vessel occlusion (LVO) with several trials demonstrating its efficacy in reducing mortality and morbidity (1‚Äì3). However, ECR is considerably more costly than traditional care (4), with estimated procedure costs ranging between 9,000 and 14,000 US dollars per patient (4, 5). Major expenditure is required for capital equipment such as angiography equipment purchase and maintenance. Staffing must be adequate to deliver a 24/7 rapid response service. Government funding agencies seek to optimize return on investment, such as that on resources allocated to acute stroke services. In contrast to other healthcare fields, a resource-use optimization model has not been implemented for comprehensive stroke services.‚ÄùHuang et al. (2019)\n\n\n1.2 Model outputs\nDefine all quantitative performance measures that are reported, using equations where necessary. Specify how and when they are calculated during the model run along with how any measures of error such as confidence intervals are calculated.\n‚úÖ Fully\nOutcome Measures: ‚ÄúWe examined two outcome measures in this model: the patient wait time and resource utilization rate. ‚ÄúPatient wait time‚Äù is the time spent queuing for a resource. ‚ÄúResource utilization rate‚Äù represents the median occupancy rate.‚ÄùStatistics and software: ‚ÄúTo facilitate graphical and descriptive comparison across models, we express waiting times as relative probabilities of waiting a given amount of time, compared to not waiting at all.‚ÄùHuang et al. (2019)\n\n\n1.3 Experimentation aims\nIf the model has been used for experimentation, state the objectives that it was used to investigate.(A) Scenario based analysis ‚Äì Provide a name and description for each scenario, providing a rationale for the choice of scenarios and ensure that item 2.3 (below) is completed.(B) Design of experiments ‚Äì Provide details of the overall design of the experiments with reference to performance measures and their parameters (provide further details in data below).(C) Simulation Optimisation ‚Äì (if appropriate) Provide full details of what is to be optimised, the parameters that were included and the algorithm(s) that was be used. Where possible provide a citation of the algorithm(s).\n‚úÖ Fully\nAll scenarios are described and justified.Results: ‚ÄúTo investigate why a bottleneck exists at angioINR, we tested three scenarios with varying degrees of patient accessibility to angioINR. First, in the ‚Äúexclusive-use‚Äù scenario, angioINR is not available for elective IR patients. Its use is restricted to stroke, elective INR and emergency IR patients. Second, in the ‚Äútwo angioINRs‚Äù scenario, the angioIR is replaced with an angioINR, doubling angiography availability for ECR patients. Lastly, in the ‚Äúextended schedule‚Äù scenario, day time working hours of all human resources are extended by up to 2 h, extending resource access to all patients.‚ÄùResults: Using DES to Predict Future Resource Usage: ‚ÄúSince acquiring data for this study, the demands for ECR at our Comprehensive Stroke Service has doubled between 2018 and 19 and is predicted to triple by the end of 2019. We simulated these increased demands on the resource.‚ÄùHuang et al. (2019)\n\n\nLogic\n\n\n\n\n\n2.1 Base model overview diagram\nDescribe the base model using appropriate diagrams and description. This could include one or more process flow, activity cycle or equivalent diagrams sufficient to describe the model to readers. Avoid complicated diagrams in the main text. The goal is to describe the breadth and depth of the model with respect to the system being studied.\n‚úÖ Fully\nFigure 1:Huang et al. (2019)\n\n\n2.2 Base model logic\nGive details of the base model logic. Give additional model logic details sufficient to communicate to the reader how the model works.\n‚úÖ Fully\nDetailed in Methods: Model Algorithm\n\n\n2.3 Scenario logic\nGive details of the logical difference between the base case model and scenarios (if any). This could be incorporated as text or where differences are substantial could be incorporated in the same manner as 2.2.\n‚úÖ Fully\nAs in 1.3.\n\n\n2.4 Algorithms\nProvide further detail on any algorithms in the model that (for example) mimic complex or manual processes in the real world (i.e.¬†scheduling of arrivals/ appointments/ operations/ maintenance, operation of a conveyor system, machine breakdowns, etc.). Sufficient detail should be included (or referred to in other published work) for the algorithms to be reproducible. Pseudo-code may be used to describe an algorithm.\nüü° Partially\nMethods: Model Properties: Patients: ‚ÄúPatients are generated by a Poissone process with an inter-arrival time as specified in Table 1.‚ÄùHuang et al. (2019) Doesn‚Äôt describe some of the other processes from the code (e.g.¬†sampling appointment length, or intricacies of how the suspected stroke / AIS / ECR are not directly inter-arrival time but instead probability based).\n\n\n2.5.1 Components - entities\nGive details of all entities within the simulation including a description of their role in the model and a description of all their attributes.\n‚úÖ Fully\nDescribes all four patient types in Methods: Model Algorithm - ‚Äú(1) a stroke pathway, (2) an elective non-stroke interventional neuroradiology (elective INR) pathway, (3) an emergency interventional radiology (emergency IR) pathway and (4) an elective interventional radiology (elective IR) pathway.‚ÄùHuang et al. (2019)\n\n\n2.5.2 Components - activities\nDescribe the activities that entities engage in within the model. Provide details of entity routing into and out of the activity.\n‚úÖ Fully\nDescribed in Methods: Model Algorithm and visualised in Figure 1.Huang et al. (2019)\n\n\n2.5.3 Components - resources\nList all the resources included within the model and which activities make use of them.\n‚úÖ Fully\nMethods: ‚Äúresources represent human and physical resources such as interventional radiologist (IR), interventional neuroradiologist (INR), stroke physician, nurse, radiology technologist, CT scanner, single plane (angioIR), and biplane (angioINR) angiography suites.‚ÄùUsed described in Methods: Model Algorithm and visualised in Figure 1.Huang et al. (2019)\n\n\n2.5.4 Components - queues\nGive details of the assumed queuing discipline used in the model (e.g.¬†First in First Out, Last in First Out, prioritisation, etc.). Where one or more queues have a different discipline from the rest, provide a list of queues, indicating the queuing discipline used for each. If reneging, balking or jockeying occur, etc., provide details of the rules. Detail any delays or capacity constraints on the queues.\n‚úÖ Fully\nMethods: Model Properties: Queueing: ‚ÄúIn the real world, resources are preferentially given to emergency patients over elective or non-emergency patients. In our model, emergency IR and stroke patients have higher priority than elective patients for resources. Specifically, angioINRs are capable of both INR and IR procedures, although all patient types can utilize this resource, stroke patients have priority compared to other patient types. Emergency IR patients are next in line, followed by elective patients. For example, if a stroke patient and an emergency IR patient enter a queue with 10 elective patients for angioINR, the stroke patient will automatically be placed in front of the queue followed by the emergency IR patient. For an angiography machine for IR procedures only (angioIR), emergency IR patients have priority over elective IR patients. When no resources are available, but multiple resource choices are present, a patient automatically enters the resource queue with the least number of entities (i.e., the shortest queue).‚ÄùHuang et al. (2019)\n\n\n2.5.5 Components - entry/exit points\nGive details of the model boundaries i.e.¬†all arrival and exit points of entities. Detail the arrival mechanism (e.g.¬†‚Äòthinning‚Äô to mimic a non-homogenous Poisson process or balking)\n‚úÖ Fully\nEasily understood from Figure 1.Huang et al. (2019)\n\n\nData\n\n\n\n\n\n3.1 Data sources\nList and detail all data sources. Sources may include:‚Ä¢ Interviews with stakeholders,‚Ä¢ Samples of routinely collected data,‚Ä¢ Prospectively collected samples for the purpose of the simulation study,‚Ä¢ Public domain data published in either academic or organisational literature. Provide, where possible, the link and DOI to the data or reference to published literature.All data source descriptions should include details of the sample size, sample date ranges and use within the study.\n‚úÖ Fully\nMethods: Model Algorithm: ‚ÄúThe decision to proceed to the next event is probabilistic and is acquired from logged data from a Comprehensive Stroke Service in Melbourne, Australia, between 2016 and 17‚ÄùModel Properties: Patients: ‚ÄúInter-arrival times are calculated from patient statistics which were obtained from logged data from a Comprehensive Stroke Service in Melbourne, Australia between 2016 and 17.‚ÄùHuang et al. (2019)\n\n\n3.2 Pre-processing\nProvide details of any data manipulation that has taken place before its use in the simulation, e.g.¬†interpolation to account for missing data or the removal of outliers.\nN/A\nNone provided, so presumed not applicable.\n\n\n3.3 Input parameters\nList all input variables in the model. Provide a description of their use and include parameter values. For stochastic inputs provide details of any continuous, discrete or empirical distributions used along with all associated parameters. Give details of all time dependent parameters and correlation.Clearly state:‚Ä¢ Base case data‚Ä¢ Data use in experimentation, where different from the base case.‚Ä¢ Where optimisation or design of experiments has been used, state the range of values that parameters can take.‚Ä¢ Where theoretical distributions are used, state how these were selected and prioritised above other candidate distributions.\nüü° Partially\nMany are provided in Table 1, although some parameters are not described (e.g.¬†length of time with resources)Huang et al. (2019)\n\n\n3.4 Assumptions\nWhere data or knowledge of the real system is unavailable what assumptions are included in the model? This might include parameter values, distributions or routing logic within the model.\n‚ùå Not met\nCannot identify in paper.\n\n\nExperimentation\n\n\n\n\n\n4.1 Initialisation\nReport if the system modelled is terminating or non-terminating. State if a warm-up period has been used, its length and the analysis method used to select it. For terminating systems state the stopping condition.State what if any initial model conditions have been included, e.g., pre-loaded queues and activities. Report whether initialisation of these variables is deterministic or stochastic.\n‚ùå Not met\nNot described.\n\n\n4.2 Run length\nDetail the run length of the simulation model and time units.\n‚úÖ Fully\nMethods: Statistics and Software: ‚ÄúEach scenario has a runtime of 365 days‚ÄùHuang et al. (2019)\n\n\n4.3 Estimation approach\nState the method used to account for the stochasticity: For example, two common methods are multiple replications or batch means. Where multiple replications have been used, state the number of replications and for batch means, indicate the batch length and whether the batch means procedure is standard, spaced or overlapping. For both procedures provide a justification for the methods used and the number of replications/size of batches.\nüü° Partially\nNumber of replications stated but not justified.Methods: Statistics and Software: ‚ÄúEach scenario‚Ä¶ was simulated 30 times‚ÄùHuang et al. (2019)\n\n\nImplementation\n\n\n\n\n\n5.1 Software or programming language\nState the operating system and version and build number.State the name, version and build number of commercial or open source DES software that the model is implemented in.State the name and version of general-purpose programming languages used (e.g.¬†Python 3.5).Where frameworks and libraries have been used provide all details including version numbers.\nüü° Partially\nSome details provided - Methods: Statistics and Software: ‚ÄúThe DES model was built with Simmer (version 4.1.0), a DES package for R. The interactive web application was built with R-Shiny‚ÄùHuang et al. (2019)\n\n\n5.2 Random sampling\nState the algorithm used to generate random samples in the software/programming language used e.g.¬†Mersenne Twister.If common random numbers are used, state how seeds (or random number streams) are distributed among sampling processes.\nüü° Partially\nSampling described for arrivals but not for length of time with resources. Doesn‚Äôt mention whether seeds are used.Methods: Model Properties: Patients: ‚ÄúPatients are generated by a Poissone process with an inter-arrival time as specified in Table 1.‚ÄùHuang et al. (2019)\n\n\n5.3 Model execution\nState the event processing mechanism used e.g.¬†three phase, event, activity, process interaction.Note that in some commercial software the event processing mechanism may not be published. In these cases authors should adhere to item 5.1 software recommendations.State all priority rules included if entities/activities compete for resources.If the model is parallel, distributed and/or use grid or cloud computing, etc., state and preferably reference the technology used. For parallel and distributed simulations the time management algorithms used. If the HLA is used then state the version of the standard, which run-time infrastructure (and version), and any supporting documents (FOMs, etc.)\nüü° Partially\nDoes not state event processing mechanism. Does describe priority rules - Methods: Model Properties: Queueing - e.g.¬†‚Äún our model, emergency IR and stroke patients have higher priority than elective patients for resources. Specifically, angioINRs are capable of both INR and IR procedures, although all patient types‚Ä¶\n\n\n5.4 System specification\nState the model run time and specification of hardware used. This is particularly important for large scale models that require substantial computing power. For parallel, distributed and/or use grid or cloud computing, etc. state the details of all systems used in the implementation (processors, network, etc.)\n‚ùå Not met\n-\n\n\nCode access\n\n\n\n\n\n6.1 Computer model sharing statement\nDescribe how someone could obtain the model described in the paper, the simulation software and any other associated software (or hardware) needed to reproduce the results. Provide, where possible, the link and DOIs to these.\n‚úÖ Fully\nMethods: ‚ÄúThe source code for the model is available at https://github.com/shiweih/desECR under a GNU General Public License.‚ÄùMethods: Statistics and Software: ‚ÄúDES model was built with Simmer (version 4.1.0), a DES package for R. The interactive web application was built with R-Shiny‚ÄùDiscussion: ‚ÄúThe model is currently available online at https://rebrand.ly/desECR11‚Äù Huang et al. (2019)"
  },
  {
    "objectID": "evaluation/reporting.html#des-checklist-derived-from-ispor-sdm",
    "href": "evaluation/reporting.html#des-checklist-derived-from-ispor-sdm",
    "title": "Reporting guidelines",
    "section": "DES checklist derived from ISPOR-SDM",
    "text": "DES checklist derived from ISPOR-SDM\nOf the 18 items in the checklist:\n\n7 were met fully (‚úÖ)\n2 were partially met (üü°)\n7 were not met (‚ùå)\n2 were not applicable (N/A)\n\n\n\n\n\n\n\n\n\n\nItem\nAssessed if‚Ä¶\nMet by study?\nEvidence/location\n\n\n\n\nModel conceptualisation\n\n\n\n\n\n1 Is the focused health-related decision problem clarified?\n‚Ä¶the decision problem under investigation was defined. DES studies included different types of decision problems, eg, those listed in previously developed taxonomies.\n‚úÖ Fully\nECR resource utilisation, as in Introduction.\n\n\n2 Is the modeled healthcare setting/health condition clarified?\n‚Ä¶the physical context/scope (eg, a certain healthcare unit or a broader system) or disease spectrum simulated was described.\n‚úÖ Fully\nImplicit that it is a single hospital, and the relevant pathways for different patient types are described in the Methods: Model Algorithm.\n\n\n3 Is the model structure described?\n‚Ä¶the model‚Äôs conceptual structure was described in the form of either graphical or text presentation.\n‚úÖ Fully\nDescribed in Methods: Model Algorithm and visualised in Figure 1:Huang et al. (2019)\n\n\n4 Is the time horizon given?\n‚Ä¶the time period covered by the simulation was reported.\n‚úÖ Fully\nMethods: Statistics and Software: ‚ÄúEach scenario has a runtime of 365 days‚ÄùHuang et al. (2019)\n\n\n5 Are all simulated strategies/scenarios specified?\n‚Ä¶the comparators under test were described in terms of their components, corresponding variations, etc\n‚úÖ Fully\nAll scenarios are specified.Results: ‚ÄúTo investigate why a bottleneck exists at angioINR, we tested three scenarios with varying degrees of patient accessibility to angioINR. First, in the ‚Äúexclusive-use‚Äù scenario, angioINR is not available for elective IR patients. Its use is restricted to stroke, elective INR and emergency IR patients. Second, in the ‚Äútwo angioINRs‚Äù scenario, the angioIR is replaced with an angioINR, doubling angiography availability for ECR patients. Lastly, in the ‚Äúextended schedule‚Äù scenario, day time working hours of all human resources are extended by up to 2 h, extending resource access to all patients.‚ÄùResults: Using DES to Predict Future Resource Usage: ‚ÄúSince acquiring data for this study, the demands for ECR at our Comprehensive Stroke Service has doubled between 2018 and 19 and is predicted to triple by the end of 2019. We simulated these increased demands on the resource.‚ÄùHuang et al. (2019)\n\n\n6 Is the target population described?\n‚Ä¶the entities simulated and their main attributes were characterized.\n‚ùå Not met\n-\n\n\nParamaterisation and uncertainty assessment\n\n\n\n\n\n7 Are data sources informing parameter estimations provided?\n‚Ä¶the sources of all data used to inform model inputs were reported.\n‚úÖ Fully\nMethods: Model Algorithm: ‚ÄúThe decision to proceed to the next event is probabilistic and is acquired from logged data from a Comprehensive Stroke Service in Melbourne, Australia, between 2016 and 17‚ÄùModel Properties: Patients: ‚ÄúInter-arrival times are calculated from patient statistics which were obtained from logged data from a Comprehensive Stroke Service in Melbourne, Australia between 2016 and 17.‚ÄùHuang et al. (2019)\n\n\n8 Are the parameters used to populate model frameworks specified?\n‚Ä¶all relevant parameters fed into model frameworks were disclosed.\nüü° Partially\nMany are provided in Table 1, although some parameters are not described (e.g.¬†length of time with resources)Huang et al. (2019)\n\n\n9 Are model uncertainties discussed?\n‚Ä¶the uncertainty surrounding parameter estimations and adopted statistical methods (eg, 95% confidence intervals or possibility distributions) were reported.\n‚ùå Not met\n-\n\n\n10 Are sensitivity analyses performed and reported?\n‚Ä¶the robustness of model outputs to input uncertainties was examined, for example via deterministic (based on parameters‚Äô plausible ranges) or probabilistic (based on a priori-defined probability distributions) sensitivity analyses, or both.\n‚ùå Not met\nDoes mention in the Discussion that ‚ÄúThe quality of the ECR service appears to be robust to important parameters, such as the number of radiologists‚Äù, but no sensitivity analysis is reported\n\n\nValidation\n\n\n\n\n\n11 Is face validity evaluated and reported?\n‚Ä¶it was reported that the model was subjected to the examination on how well model designs correspond to the reality and intuitions. It was assumed that this type of validation should be conducted by external evaluators with no stake in the study.\n‚ùå Not met\n-\n\n\n12 Is cross validation performed and reported\n‚Ä¶comparison across similar modeling studies which deal with the same decision problem was undertaken.\n‚ùå Not met\n-\n\n\n13 Is external validation performed and reported?\n‚Ä¶the modeler(s) examined how well the model‚Äôs results match the empirical data of an actual event modeled.\nN/A\nDiscussion: ‚ÄúIn general, a limitation of the current implementation is that few measurements exist to parameterize or validate many aspects of the simulation, because such records are not routinely kept. However, explicitly modeling the workflow can allow administrators to keep track of key parameters and performance, improving the model over time.‚ÄùHuang et al. (2019)\n\n\n14 Is predictive validation performed or attempted?\n‚Ä¶the modeler(s) examined the consistency of a model‚Äôs predictions of a future event and the actual outcomes in the future. If this was not undertaken, it was assessed whether the reasons were discussed.\nN/A\nThis is only relevant to forecasting models\n\n\nGeneralisability and stakeholder involvement\n\n\n\n\n\n15 Is the model generalizability issue discussed?\n‚Ä¶the modeler(s) discussed the potential of the resulting model for being applicable to other settings/populations (single/multiple application).\n‚úÖ Fully\nDiscussion: ‚ÄúThe quality of the ECR service appears to be robust to important parameters, such as the number of radiologists. The simulation findings apply to ECR services that can be represented by the model in this study. As such, utilization of this model to its maximum capacity requires tailoring the model to local needs, as institutional bottlenecks differ between providers. We specifically developed this model using an open source programming language so that the source code can serve as a basis for future model refinement and modification.‚ÄùHuang et al. (2019)\n\n\n16 Are decision makers or other stakeholders involved in modeling?\n‚Ä¶the modeler(s) reported in which part throughout the modeling process decision makers and other stakeholders (eg, subject experts) were engaged.\n‚ùå Not met\n-\n\n\n17 Is the source of funding stated?\n‚Ä¶the sponsorship of the study was indicated.\n‚ùå Not met\n-\n\n\n18 Are model limitations discussed?\n‚Ä¶limitations of the assessed model, especially limitations of interest to decision makers, were discussed.\nüü° Partially\nDoes mention a general limitation, but I don‚Äôt feel limitations were explored in as much detail as they could be.Discussion: ‚ÄúIn general, a limitation of the current implementation is that few measurements exist to parameterize or validate many aspects of the simulation, because such records are not routinely kept. However, explicitly modeling the workflow can allow administrators to keep track of key parameters and performance, improving the model over time.‚ÄùHuang et al. (2019)"
  },
  {
    "objectID": "reproduction/scripts/reproduction.html",
    "href": "reproduction/scripts/reproduction.html",
    "title": "Reproduce Figures 2-4 and in-text results 1-3",
    "section": "",
    "text": "The majority of the items in the model scope are reproduced in this file, but Figure 5 and the supplementary figure are created in seperate .qmd files.\nThis decision was primarily due to issues with RStudio crashing when running all scenarios from a single .Rmd file.\nRun time: 18.024 minutes (will vary between machines)"
  },
  {
    "objectID": "reproduction/scripts/reproduction.html#set-up",
    "href": "reproduction/scripts/reproduction.html#set-up",
    "title": "Reproduce Figures 2-4 and in-text results 1-3",
    "section": "Set up",
    "text": "Set up\n\n# Clear environment\nrm(list=ls())\n\n# Start timer\nstart.time &lt;- Sys.time()\n\n# Disable scientific notation\noptions(scipen=999)\n\n# Import required libraries (if not otherwise import in R scripts below)\nlibrary(ggpubr)\n\nLoading required package: ggplot2\n\nlibrary(tidyr, include.only = c(\"pivot_wider\"))\n\n# Get the model and helper functions (but hide loading warnings for each package)\nsuppressMessages(source(\"model.R\"))\nsuppressMessages(source(\"helpers.R\"))\n\n\n# Set the seed and default dimensions for figures\nSEED = 200\nDEFAULT_WIDTH = 7\nDEFAULT_HEIGHT = 4\n\n# Set file paths to save results\n\nfolder = \"../outputs\"\n\npath_baseline_f2 &lt;- file.path(folder, \"fig2_baseline.csv.gz\")\npath_exclusive_f2 &lt;- file.path(folder, \"fig2_exclusive.csv.gz\")\npath_twoangio_f2 &lt;- file.path(folder, \"fig2_twoangio.csv.gz\")\n\npath_baseline_f3 &lt;- file.path(folder, \"fig3_baseline.csv.gz\")\npath_exclusive_f3 &lt;- file.path(folder, \"fig3_exclusive.csv.gz\")\npath_twoangio_f3 &lt;- file.path(folder, \"fig3_twoangio.csv.gz\")\n\npath_txt2 &lt;- file.path(folder, \"txt2.csv\") # Used for results 1 and 2\npath_txt3 &lt;- file.path(folder, \"txt3.csv\")\npath_fig2 &lt;- file.path(folder, \"fig2.png\")\npath_fig3 &lt;- file.path(folder, \"fig3.png\")\npath_fig4 &lt;- file.path(folder, \"fig4.png\")"
  },
  {
    "objectID": "reproduction/scripts/reproduction.html#run-models",
    "href": "reproduction/scripts/reproduction.html#run-models",
    "title": "Reproduce Figures 2-4 and in-text results 1-3",
    "section": "Run models",
    "text": "Run models\nSet to true or false, depending on whether you want to run everything.\n\nrun &lt;- FALSE\n\nRun model scenarios.\n\nif (isTRUE(run)) {\n  # Run model\n  baseline &lt;- run_model(seed = SEED)\n  baseline_6pm &lt;- run_model(shifts = c(8,18), seed = SEED)\n  baseline_7pm &lt;- run_model(shifts = c(8,19), seed = SEED)\n\n  exclusive &lt;- run_model(exclusive_use = TRUE, seed = SEED)\n  exclusive_6pm &lt;- run_model(shifts = c(8,18), exclusive_use = TRUE, seed = SEED)\n  exclusive_7pm &lt;- run_model(shifts = c(8,19), exclusive_use = TRUE, seed = SEED)\n\n  twoangio &lt;- run_model(angio_inr = 2, angio_ir=0, seed = SEED)\n  twoangio_6pm &lt;- run_model(shifts = c(8,18), angio_inr = 2, angio_ir=0, seed = SEED)\n  twoangio_7pm &lt;- run_model(shifts = c(8,19), angio_inr = 2, angio_ir=0, seed = SEED)\n}\n\n\n# (in seperate cell to above as otherwise seemed to crash)\nif (isTRUE(run)) {\n  # Save results for Figure 2\n  data.table::fwrite(baseline, path_baseline_f2)\n  data.table::fwrite(exclusive, path_exclusive_f2)\n  data.table::fwrite(twoangio, path_twoangio_f2)\n\n  # Process and save results for Figure 3\n  process_f3_data(baseline, baseline_6pm, baseline_7pm, path_baseline_f3)\n  process_f3_data(exclusive, exclusive_6pm, exclusive_7pm, path_exclusive_f3)\n  process_f3_data(twoangio, twoangio_6pm, twoangio_7pm, path_twoangio_f3)\n\n  # Remove the dataframes from environment\n  rm(baseline, baseline_6pm, baseline_7pm,\n     exclusive, exclusive_6pm, exclusive_7pm,\n     twoangio, twoangio_6pm, twoangio_7pm)\n}"
  },
  {
    "objectID": "reproduction/scripts/reproduction.html#import-results",
    "href": "reproduction/scripts/reproduction.html#import-results",
    "title": "Reproduce Figures 2-4 and in-text results 1-3",
    "section": "Import results",
    "text": "Import results\nImport the results, adding a column to each to indicate the scenario.\n\nbase_f2 &lt;- import_results(path_baseline_f2,\n                          \"Baseline\")\nexc_f2 &lt;- import_results(path_exclusive_f2,\n                         \"Exclusive use\")\ntwo_f2 &lt;- import_results(path_twoangio_f2,\n                         \"Two AngioINRs\")\n\nbase_f3 &lt;- import_results(path_baseline_f3,\n                          \"Baseline\")\nexc_f3 &lt;- import_results(path_exclusive_f3,\n                         \"Exclusive use\")\ntwo_f3 &lt;- import_results(path_twoangio_f3,\n                         \"Two AngioINRs\")"
  },
  {
    "objectID": "reproduction/scripts/reproduction.html#in-text-results",
    "href": "reproduction/scripts/reproduction.html#in-text-results",
    "title": "Reproduce Figures 2-4 and in-text results 1-3",
    "section": "In-text results",
    "text": "In-text results\nIn-text results 1 and 2\n\ntxt2 &lt;- dplyr::bind_rows(base_f2, exc_f2, two_f2) %&gt;%\n  filter(resource==\"angio_inr\") %&gt;%\n  group_by(scenario) %&gt;%\n  summarize(mean = mean(wait_time)) %&gt;%\n  mutate(diff_from_baseline = round(mean - mean[1], 2))\n\n# Save and display result\ndata.table::fwrite(txt2, path_txt2)\ntxt2\n\n# A tibble: 3 √ó 3\n  scenario       mean diff_from_baseline\n  &lt;chr&gt;         &lt;dbl&gt;              &lt;dbl&gt;\n1 Baseline      14.0                0   \n2 Exclusive use  8.12              -5.84\n3 Two AngioINRs  9.62              -4.34\n\n\nIn-text result 3\n\ntxt3 &lt;- dplyr::bind_rows(base_f3, exc_f3, two_f3) %&gt;%\n  filter(resource==\"angio_inr\") %&gt;%\n  group_by(scenario, shift) %&gt;%\n  summarize(mean = mean(wait_time)) %&gt;%\n  mutate(diff_from_5pm = round(mean - mean[1], 2))\n\n`summarise()` has grouped output by 'scenario'. You can override using the\n`.groups` argument.\n\n# Save and display result\ndata.table::fwrite(txt3, path_txt3)\ntxt3\n\n# A tibble: 9 √ó 4\n# Groups:   scenario [3]\n  scenario      shift  mean diff_from_5pm\n  &lt;chr&gt;         &lt;chr&gt; &lt;dbl&gt;         &lt;dbl&gt;\n1 Baseline      5pm   14.0           0   \n2 Baseline      6pm   12.5          -1.47\n3 Baseline      7pm   12.5          -1.47\n4 Exclusive use 5pm    8.12          0   \n5 Exclusive use 6pm    7.80         -0.31\n6 Exclusive use 7pm    6.43         -1.69\n7 Two AngioINRs 5pm    9.62          0   \n8 Two AngioINRs 6pm    9.22         -0.4 \n9 Two AngioINRs 7pm    8.70         -0.92"
  },
  {
    "objectID": "reproduction/scripts/reproduction.html#figure-2",
    "href": "reproduction/scripts/reproduction.html#figure-2",
    "title": "Reproduce Figures 2-4 and in-text results 1-3",
    "section": "Figure 2",
    "text": "Figure 2\n\n# Create sub-plots\np1 &lt;- create_plot(base_f2,\n                  group=\"resource\",\n                  title=\"Baseline\",\n                  ylab=\"Standardised density of patient in queue\")\np2 &lt;- create_plot(exc_f2,\n                  group=\"resource\",\n                  title=\"Exclusive-use\",\n                  xlab=\"Patient wait time (min)\",\n                  xlim=c(0, 250))\np3 &lt;- create_plot(two_f2,\n                  group=\"resource\",\n                  title=\"Double angio INRs\")\n\n# Arrange in a single figure\nggarrange(p1, p2, p3, nrow=1,\n          common.legend=TRUE, legend=\"bottom\",\n          labels=c(\"A\", \"B\", \"C\"))\n\nWarning: Removed 1 row containing non-finite outside the scale range (`stat_density()`).\nRemoved 1 row containing non-finite outside the scale range (`stat_density()`).\nRemoved 1 row containing non-finite outside the scale range (`stat_density()`).\nRemoved 1 row containing non-finite outside the scale range (`stat_density()`).\nRemoved 1 row containing non-finite outside the scale range (`stat_density()`).\nRemoved 1 row containing non-finite outside the scale range (`stat_density()`).\nRemoved 1 row containing non-finite outside the scale range (`stat_density()`).\nRemoved 1 row containing non-finite outside the scale range (`stat_density()`).\n\n\n\n\n\n\n\n\nggsave(path_fig2, width=DEFAULT_WIDTH, height=DEFAULT_HEIGHT)\n\n\nDemonstrate that geom_density scaled is scaling against density of 0 wait time\n\n# Create figure as usual\np &lt;- create_plot(base_f2,\n                 group=\"resource\",\n                 title=\"Baseline\",\n                 ylab=\"Standardised density of patient in queue\")\n\n# Get data from the plot\nplot_data &lt;- ggplot_build(p)$data[[1]]\n\nWarning: Removed 1 row containing non-finite outside the scale range (`stat_density()`).\nRemoved 1 row containing non-finite outside the scale range (`stat_density()`).\n\n# Create dataframe with the densities for when the waitimes are 0\nno_wait &lt;- plot_data %&gt;% filter(x==0) %&gt;% select(colour, density, scaled)\n\n# Loop through each of the colours (which reflect the resource groups)\nfor (c in no_wait$colour) {\n  # Filter the plot data to that resource group, then divide the densities by\n  # the density from wait time 0\n  d &lt;- plot_data %&gt;%\n    filter(colour == c) %&gt;%\n    mutate(scaled2 = density / no_wait[no_wait$colour==c, \"density\"]) %&gt;%\n    ungroup() %&gt;%\n    select(scaled, scaled2)\n\n  # Find the number of rows where these values match the scaled values\n  n_match &lt;- sum(apply(d, 1, function(x) length(unique(x)) == 1))\n  n_total &lt;- nrow(d)\n  print(sprintf(\"%s out of %s results match\", n_match, n_total))\n}\n\n[1] \"512 out of 512 results match\"\n[1] \"512 out of 512 results match\"\n[1] \"512 out of 512 results match\"\n[1] \"512 out of 512 results match\"\n[1] \"512 out of 512 results match\""
  },
  {
    "objectID": "reproduction/scripts/reproduction.html#figure-3",
    "href": "reproduction/scripts/reproduction.html#figure-3",
    "title": "Reproduce Figures 2-4 and in-text results 1-3",
    "section": "Figure 3",
    "text": "Figure 3\n\n# Create sub-plots\np1 &lt;- create_plot(base_f3,\n                  group=\"shift\",\n                  title=\"Baseline\",\n                  ylab=\"Standardised density of patient in queue\")\np2 &lt;- create_plot(exc_f3,\n                  group=\"shift\",\n                  title=\"Exclusive-use\",\n                  xlab=\"Patient wait time (min)\",\n                  xlim=c(0, 300),\n                  breaks_width=100)\np3 &lt;- create_plot(two_f3,\n                  group=\"shift\",\n                  title=\"Double angio INRs\",\n                  xlim=c(0, 250))\n\n# Arrange in a single figure\nggarrange(p1, p2, p3, nrow=1,\n          common.legend=TRUE, legend=\"bottom\",\n          labels=c(\"A\", \"B\", \"C\"))\n\nWarning: Removed 5 rows containing non-finite outside the scale range\n(`stat_density()`).\nRemoved 5 rows containing non-finite outside the scale range\n(`stat_density()`).\nRemoved 5 rows containing non-finite outside the scale range\n(`stat_density()`).\nRemoved 5 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\nWarning: Removed 1 row containing non-finite outside the scale range (`stat_density()`).\nRemoved 1 row containing non-finite outside the scale range (`stat_density()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\nRemoved 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\nggsave(path_fig3, width=DEFAULT_WIDTH, height=DEFAULT_HEIGHT)"
  },
  {
    "objectID": "reproduction/scripts/reproduction.html#figure-4",
    "href": "reproduction/scripts/reproduction.html#figure-4",
    "title": "Reproduce Figures 2-4 and in-text results 1-3",
    "section": "Figure 4",
    "text": "Figure 4\n\n# Get the relevant results from in-text results 1, 2 and 3\n# Then calculate difference from baseline\nfig4 &lt;- dplyr::bind_rows(txt2 %&gt;% select(scenario, mean),\n                         txt3 %&gt;%\n                          filter(scenario==\"Exclusive use\", shift==\"6pm\") %&gt;%\n                          mutate(scenario=\"Exclusive use (+1h)\") %&gt;%\n                          select(scenario, mean)) %&gt;%\n  mutate(diff = mean - mean[1]) %&gt;%\n  filter(scenario!=\"Baseline\") %&gt;%\n  mutate(dis_free_gain = abs(diff)*4.2)\n\n# Set order of the bars, and give full labels\nfig4_col &lt;- c(\"Exclusive use\", \"Two AngioINRs\", \"Exclusive use (+1h)\")\nfig4_col_l &lt;- c(\"Exclusive-use\", \"Two angio INRs\", \"Exclusive-use and +1hr work\")\nfig4$scenario &lt;- factor(fig4$scenario, levels=fig4_col)\nfig4$scenario_lab &lt;- plyr::mapvalues(fig4$scenario, from=fig4_col, to=fig4_col_l)\n\nggplot(fig4, aes(x=scenario_lab, y=dis_free_gain)) +\n  geom_bar(stat=\"identity\") +\n  ylim(0, 32) +\n  xlab(\"Scenarios\") +\n  ylab(\"Mean disability-free life added (days)\")\n\n\n\n\n\n\n\nggsave(path_fig4, width=5, height=3)"
  },
  {
    "objectID": "reproduction/scripts/reproduction.html#time-elapsed",
    "href": "reproduction/scripts/reproduction.html#time-elapsed",
    "title": "Reproduce Figures 2-4 and in-text results 1-3",
    "section": "Time elapsed",
    "text": "Time elapsed\n\nif (isTRUE(run)) {\n  end.time &lt;- Sys.time()\n  elapsed.time &lt;- round((end.time - start.time), 3)\n  elapsed.time\n}"
  },
  {
    "objectID": "reproduction/scripts/reproduction_supp.html",
    "href": "reproduction/scripts/reproduction_supp.html",
    "title": "Reproduce supplementary figure",
    "section": "",
    "text": "This is run in a separate script from the other figures due to issues with RStudio crashing when all scenarios were run from a single script.\nIf run is TRUE, it will run scenarios with double and triple the number of ECR patients.\nTo create the figure, it will use those files, as well as the baseline file created within reproduction.qmd.\nRun time: 4.975 minutes (will vary between machines)"
  },
  {
    "objectID": "reproduction/scripts/reproduction_supp.html#set-up",
    "href": "reproduction/scripts/reproduction_supp.html#set-up",
    "title": "Reproduce supplementary figure",
    "section": "Set up",
    "text": "Set up\n\n# Clear environment\nrm(list=ls())\n\n# Start timer\nstart.time &lt;- Sys.time()\n\n# Disable scientific notation\noptions(scipen=999)\n\n# Get the model and helper functions (but hide loading warnings for each package)\nsuppressMessages(source(\"model.R\"))\nsuppressMessages(source(\"helpers.R\"))\n\n# Import other required libraries (if not otherwise import in R scripts below)\nlibrary(ggpubr)\nlibrary(tidyr, include.only = c(\"pivot_wider\"))\n\n\n# Set the seed and default dimensions for figures\nSEED = 200\nDEFAULT_WIDTH = 7\nDEFAULT_HEIGHT = 4\n\n# Set file paths to save results\nfolder = \"../outputs\"\n\npath_baseline_f2 &lt;- file.path(folder, \"fig2_baseline.csv.gz\")\npath_double_sup &lt;- file.path(folder, \"sup_baseline_double.csv.gz\")\npath_triple_sup &lt;- file.path(folder, \"sup_baseline_triple.csv.gz\")\n\npath_supfig &lt;- file.path(folder, \"supplementary_figure.png\")"
  },
  {
    "objectID": "reproduction/scripts/reproduction_supp.html#run-models",
    "href": "reproduction/scripts/reproduction_supp.html#run-models",
    "title": "Reproduce supplementary figure",
    "section": "Run models",
    "text": "Run models\nSet to true or false, depending on whether you want to run everything.\n\nrun &lt;- FALSE\n\nRun baseline with double and triple the number of ECR patients, for the supplementary figure.\n\nif (isTRUE(run)) {\n  baseline_sup2 &lt;- run_model(seed = SEED, ecr_pt = 58*2)\n  baseline_sup3 &lt;- run_model(seed = SEED, ecr_pt = 58*3)\n}\n\n\nif (isTRUE(run)) {\n  # Save results\n  data.table::fwrite(baseline_sup2, path_double_sup)\n  data.table::fwrite(baseline_sup3, path_triple_sup)\n\n  # Remove the dataframes from environment\n  rm(baseline_sup2, baseline_sup3)\n}"
  },
  {
    "objectID": "reproduction/scripts/reproduction_supp.html#import-results",
    "href": "reproduction/scripts/reproduction_supp.html#import-results",
    "title": "Reproduce supplementary figure",
    "section": "Import results",
    "text": "Import results\nImport the results, adding a column to each to indicate the scenario.\n\nbase_f2 &lt;- import_results(path_baseline_f2, \"Baseline\")\nbase_sup_double &lt;- import_results(path_double_sup, \"Baseline (double)\")\nbase_sup_triple &lt;- import_results(path_triple_sup, \"Baseline (triple)\")"
  },
  {
    "objectID": "reproduction/scripts/reproduction_supp.html#supplementary-figure",
    "href": "reproduction/scripts/reproduction_supp.html#supplementary-figure",
    "title": "Reproduce supplementary figure",
    "section": "Supplementary figure",
    "text": "Supplementary figure\n\n# Create sub-plots\np1 &lt;- create_plot(base_f2,\n                  group=\"resource\",\n                  title=\"Baseline\",\n                  ylab=\"Standardised density of patient in queue\")\np2 &lt;- create_plot(base_sup_double,\n                  group=\"resource\",\n                  title=\"Doubling ECR patients\",\n                  xlab=\"Patient wait time (min)\",\n                  xlim=c(0, 300),\n                  breaks_width=100)\np3 &lt;- create_plot(base_sup_triple,\n                  group=\"resource\",\n                  title=\"Tripling ECR patients\",\n                  xlim=c(0, 300))\n\n# Arrange in a single figure\nggarrange(p1, p2, p3, nrow=1,\n          common.legend=TRUE, legend=\"bottom\",\n          labels=c(\"A\", \"B\", \"C\"))\n\nWarning: Removed 1 row containing non-finite outside the scale range (`stat_density()`).\nRemoved 1 row containing non-finite outside the scale range (`stat_density()`).\nRemoved 1 row containing non-finite outside the scale range (`stat_density()`).\nRemoved 1 row containing non-finite outside the scale range (`stat_density()`).\n\n\nWarning: Removed 4 rows containing non-finite outside the scale range\n(`stat_density()`).\nRemoved 4 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_density()`).\nRemoved 3 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\nggsave(path_supfig, width=DEFAULT_WIDTH, height=DEFAULT_HEIGHT)"
  },
  {
    "objectID": "reproduction/scripts/reproduction_supp.html#time-elapsed",
    "href": "reproduction/scripts/reproduction_supp.html#time-elapsed",
    "title": "Reproduce supplementary figure",
    "section": "Time elapsed",
    "text": "Time elapsed\n\nif (isTRUE(run)) {\n  end.time &lt;- Sys.time()\n  elapsed.time &lt;- round((end.time - start.time), 3)\n  elapsed.time\n}"
  },
  {
    "objectID": "quarto_site/license.html",
    "href": "quarto_site/license.html",
    "title": "Open Source License",
    "section": "",
    "text": "This repository is licensed under a GPL-3.0 license.\n\n\n\n\n\n\nView license\n\n\n\n\n\nGNU GENERAL PUBLIC LICENSE Version 3, 29 June 2007 Copyright ¬© 2007 Free Software Foundation, Inc.¬†http://fsf.org/\nEveryone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\nPreamble\nThe GNU General Public License is a free, copyleft license for software and other kinds of works.\nThe licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change all versions of a program‚Äìto make sure it remains free software for all its users. We, the Free Software Foundation, use the GNU General Public License for most of our software; it applies also to any other work released this way by its authors. You can apply it to your programs, too.\nWhen we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.\nTo protect your rights, we need to prevent others from denying you these rights or asking you to surrender the rights. Therefore, you have certain responsibilities if you distribute copies of the software, or if you modify it: responsibilities to respect the freedom of others.\nFor example, if you distribute copies of such a program, whether gratis or for a fee, you must pass on to the recipients the same freedoms that you received. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights.\nDevelopers that use the GNU GPL protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License giving you legal permission to copy, distribute and/or modify it.\nFor the developers‚Äô and authors‚Äô protection, the GPL clearly explains that there is no warranty for this free software. For both users‚Äô and authors‚Äô sake, the GPL requires that modified versions be marked as changed, so that their problems will not be attributed erroneously to authors of previous versions.\nSome devices are designed to deny users access to install or run modified versions of the software inside them, although the manufacturer can do so. This is fundamentally incompatible with the aim of protecting users‚Äô freedom to change the software. The systematic pattern of such abuse occurs in the area of products for individuals to use, which is precisely where it is most unacceptable. Therefore, we have designed this version of the GPL to prohibit the practice for those products. If such problems arise substantially in other domains, we stand ready to extend this provision to those domains in future versions of the GPL, as needed to protect the freedom of users.\nFinally, every program is threatened constantly by software patents. States should not allow patents to restrict development and use of software on general-purpose computers, but in those that do, we wish to avoid the special danger that patents applied to a free program could make it effectively proprietary. To prevent this, the GPL assures that patents cannot be used to render the program non-free.\nThe precise terms and conditions for copying, distribution and modification follow.\nTERMS AND CONDITIONS\n\nDefinitions.\n\n‚ÄúThis License‚Äù refers to version 3 of the GNU General Public License.\n‚ÄúCopyright‚Äù also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.\n‚ÄúThe Program‚Äù refers to any copyrightable work licensed under this License. Each licensee is addressed as ‚Äúyou‚Äù. ‚ÄúLicensees‚Äù and ‚Äúrecipients‚Äù may be individuals or organizations.\nTo ‚Äúmodify‚Äù a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a ‚Äúmodified version‚Äù of the earlier work or a work ‚Äúbased on‚Äù the earlier work.\nA ‚Äúcovered work‚Äù means either the unmodified Program or a work based on the Program.\nTo ‚Äúpropagate‚Äù a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.\nTo ‚Äúconvey‚Äù a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.\nAn interactive user interface displays ‚ÄúAppropriate Legal Notices‚Äù to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.\n\nSource Code. The ‚Äúsource code‚Äù for a work means the preferred form of the work for making modifications to it. ‚ÄúObject code‚Äù means any non-source form of a work.\n\nA ‚ÄúStandard Interface‚Äù means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.\nThe ‚ÄúSystem Libraries‚Äù of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A ‚ÄúMajor Component‚Äù, in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.\nThe ‚ÄúCorresponding Source‚Äù for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work‚Äôs System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.\nThe Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.\nThe Corresponding Source for a work in source code form is that same work.\n\nBasic Permissions. All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.\n\nYou may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.\nConveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.\n\nProtecting Users‚Äô Legal Rights From Anti-Circumvention Law. No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.\n\nWhen you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work‚Äôs users, your or third parties‚Äô legal rights to forbid circumvention of technological measures.\n\nConveying Verbatim Copies. You may convey verbatim copies of the Program‚Äôs source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.\n\nYou may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.\n\nConveying Modified Source Versions. You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:\n\n\nThe work must carry prominent notices stating that you modified it, and giving a relevant date.\nThe work must carry prominent notices stating that it is released under this License and any conditions added under section 7. This requirement modifies the requirement in section 4 to ‚Äúkeep intact all notices‚Äù.\nYou must license the entire work, as a whole, under this License to anyone who comes into possession of a copy. This License will therefore apply, along with any applicable section 7 additional terms, to the whole of the work, and all its parts, regardless of how they are packaged. This License gives no permission to license the work in any other way, but it does not invalidate such permission if you have separately received it.\nIf the work has interactive user interfaces, each must display Appropriate Legal Notices; however, if the Program has interactive interfaces that do not display Appropriate Legal Notices, your work need not make them do so.\n\nA compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an ‚Äúaggregate‚Äù if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation‚Äôs users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.\n\nConveying Non-Source Forms. You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:\n\n\nConvey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by the Corresponding Source fixed on a durable physical medium customarily used for software interchange.\nConvey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by a written offer, valid for at least three years and valid for as long as you offer spare parts or customer support for that product model, to give anyone who possesses the object code either (1) a copy of the Corresponding Source for all the software in the product that is covered by this License, on a durable physical medium customarily used for software interchange, for a price no more than your reasonable cost of physically performing this conveying of source, or (2) access to copy the Corresponding Source from a network server at no charge.\nConvey individual copies of the object code with a copy of the written offer to provide the Corresponding Source. This alternative is allowed only occasionally and noncommercially, and only if you received the object code with such an offer, in accord with subsection 6b.\nConvey the object code by offering access from a designated place (gratis or for a charge), and offer equivalent access to the Corresponding Source in the same way through the same place at no further charge. You need not require recipients to copy the Corresponding Source along with the object code. If the place to copy the object code is a network server, the Corresponding Source may be on a different server (operated by you or a third party) that supports equivalent copying facilities, provided you maintain clear directions next to the object code saying where to find the Corresponding Source. Regardless of what server hosts the Corresponding Source, you remain obligated to ensure that it is available for as long as needed to satisfy these requirements.\nConvey the object code using peer-to-peer transmission, provided you inform other peers where the object code and Corresponding Source of the work are being offered to the general public at no charge under subsection 6d.\n\nA separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.\nA ‚ÄúUser Product‚Äù is either (1) a ‚Äúconsumer product‚Äù, which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, ‚Äúnormally used‚Äù refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.\n‚ÄúInstallation Information‚Äù for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.\nIf you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).\nThe requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.\nCorresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.\n\nAdditional Terms. ‚ÄúAdditional permissions‚Äù are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.\n\nWhen you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.\nNotwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:\n\nDisclaiming warranty or limiting liability differently from the terms of sections 15 and 16 of this License; or\nRequiring preservation of specified reasonable legal notices or author attributions in that material or in the Appropriate Legal Notices displayed by works containing it; or\nProhibiting misrepresentation of the origin of that material, or requiring that modified versions of such material be marked in reasonable ways as different from the original version; or\nLimiting the use for publicity purposes of names of licensors or authors of the material; or\nDeclining to grant rights under trademark law for use of some trade names, trademarks, or service marks; or\nRequiring indemnification of licensors and authors of that material by anyone who conveys the material (or modified versions of it) with contractual assumptions of liability to the recipient, for any liability that these contractual assumptions directly impose on those licensors and authors.\n\nAll other non-permissive additional terms are considered ‚Äúfurther restrictions‚Äù within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.\nIf you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.\nAdditional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.\n\nTermination. You may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).\n\nHowever, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.\nMoreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.\nTermination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.\n\nAcceptance Not Required for Having Copies. You are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.\nAutomatic Licensing of Downstream Recipients. Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.\n\nAn ‚Äúentity transaction‚Äù is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party‚Äôs predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.\nYou may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.\n\nPatents. A ‚Äúcontributor‚Äù is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor‚Äôs ‚Äúcontributor version‚Äù.\n\nA contributor‚Äôs ‚Äúessential patent claims‚Äù are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, ‚Äúcontrol‚Äù includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.\nEach contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor‚Äôs essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.\nIn the following three paragraphs, a ‚Äúpatent license‚Äù is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To ‚Äúgrant‚Äù such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.\nIf you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. ‚ÄúKnowingly relying‚Äù means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient‚Äôs use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.\nIf, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.\nA patent license is ‚Äúdiscriminatory‚Äù if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.\nNothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.\n\nNo Surrender of Others‚Äô Freedom. If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.\nUse with the GNU Affero General Public License. Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU Affero General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the special requirements of the GNU Affero General Public License, section 13, concerning interaction through a network will apply to the combination as such.\nRevised Versions of this License. The Free Software Foundation may publish revised and/or new versions of the GNU General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.\n\nEach version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU General Public License ‚Äúor any later version‚Äù applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU General Public License, you may choose any version ever published by the Free Software Foundation.\nIf the Program specifies that a proxy can decide which future versions of the GNU General Public License can be used, that proxy‚Äôs public statement of acceptance of a version permanently authorizes you to choose that version for the Program.\nLater license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.\n\nDisclaimer of Warranty. THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM ‚ÄúAS IS‚Äù WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.\nLimitation of Liability. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\nInterpretation of Sections 15 and 16. If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.\n\nEND OF TERMS AND CONDITIONS\nHow to Apply These Terms to Your New Programs\nIf you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.\nTo do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the ‚Äúcopyright‚Äù line and a pointer to where the full notice is found.\n&lt;one line to give the program‚Äôs name and a brief idea of what it does.&gt; Copyright (C)  \nThis program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\nYou should have received a copy of the GNU General Public License along with this program. If not, see http://www.gnu.org/licenses/.\nAlso add information on how to contact you by electronic and paper mail.\nIf the program does terminal interaction, make it output a short notice like this when it starts in an interactive mode:\n Copyright (C)   This program comes with ABSOLUTELY NO WARRANTY; for details type show w'.  This is free software, and you are welcome to redistribute it under certain conditions; typeshow c‚Äô for details.\nThe hypothetical commands show w' andshow c‚Äô should show the appropriate parts of the General Public License. Of course, your program‚Äôs commands might be different; for a GUI interface, you would use an ‚Äúabout box‚Äù.\nYou should also get your employer (if you work as a programmer) or school, if any, to sign a ‚Äúcopyright disclaimer‚Äù for the program, if necessary. For more information on this, and how to apply and follow the GNU GPL, see http://www.gnu.org/licenses/.\nThe GNU General Public License does not permit incorporating your program into proprietary programs. If your program is a subroutine library, you may consider it more useful to permit linking proprietary applications with the library. If this is what you want to do, use the GNU Lesser General Public License instead of this License. But first, please read http://www.gnu.org/philosophy/why-not-lgpl.html.\n\n\n\nThis is aligned with the original study, who shared their code under a GPL-3.0 license.\n\n\n\n\n\n\nView license\n\n\n\n\n\n                GNU GENERAL PUBLIC LICENSE\n                   Version 3, 29 June 2007\nCopyright (C) 2007 Free Software Foundation, Inc.¬†https://fsf.org/ Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\n                        Preamble\nThe GNU General Public License is a free, copyleft license for software and other kinds of works.\nThe licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change all versions of a program‚Äìto make sure it remains free software for all its users. We, the Free Software Foundation, use the GNU General Public License for most of our software; it applies also to any other work released this way by its authors. You can apply it to your programs, too.\nWhen we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.\nTo protect your rights, we need to prevent others from denying you these rights or asking you to surrender the rights. Therefore, you have certain responsibilities if you distribute copies of the software, or if you modify it: responsibilities to respect the freedom of others.\nFor example, if you distribute copies of such a program, whether gratis or for a fee, you must pass on to the recipients the same freedoms that you received. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights.\nDevelopers that use the GNU GPL protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License giving you legal permission to copy, distribute and/or modify it.\nFor the developers‚Äô and authors‚Äô protection, the GPL clearly explains that there is no warranty for this free software. For both users‚Äô and authors‚Äô sake, the GPL requires that modified versions be marked as changed, so that their problems will not be attributed erroneously to authors of previous versions.\nSome devices are designed to deny users access to install or run modified versions of the software inside them, although the manufacturer can do so. This is fundamentally incompatible with the aim of protecting users‚Äô freedom to change the software. The systematic pattern of such abuse occurs in the area of products for individuals to use, which is precisely where it is most unacceptable. Therefore, we have designed this version of the GPL to prohibit the practice for those products. If such problems arise substantially in other domains, we stand ready to extend this provision to those domains in future versions of the GPL, as needed to protect the freedom of users.\nFinally, every program is threatened constantly by software patents. States should not allow patents to restrict development and use of software on general-purpose computers, but in those that do, we wish to avoid the special danger that patents applied to a free program could make it effectively proprietary. To prevent this, the GPL assures that patents cannot be used to render the program non-free.\nThe precise terms and conditions for copying, distribution and modification follow.\n                   TERMS AND CONDITIONS\n\nDefinitions.\n\n‚ÄúThis License‚Äù refers to version 3 of the GNU General Public License.\n‚ÄúCopyright‚Äù also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.\n‚ÄúThe Program‚Äù refers to any copyrightable work licensed under this License. Each licensee is addressed as ‚Äúyou‚Äù. ‚ÄúLicensees‚Äù and ‚Äúrecipients‚Äù may be individuals or organizations.\nTo ‚Äúmodify‚Äù a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a ‚Äúmodified version‚Äù of the earlier work or a work ‚Äúbased on‚Äù the earlier work.\nA ‚Äúcovered work‚Äù means either the unmodified Program or a work based on the Program.\nTo ‚Äúpropagate‚Äù a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.\nTo ‚Äúconvey‚Äù a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.\nAn interactive user interface displays ‚ÄúAppropriate Legal Notices‚Äù to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.\n\nSource Code.\n\nThe ‚Äúsource code‚Äù for a work means the preferred form of the work for making modifications to it. ‚ÄúObject code‚Äù means any non-source form of a work.\nA ‚ÄúStandard Interface‚Äù means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.\nThe ‚ÄúSystem Libraries‚Äù of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A ‚ÄúMajor Component‚Äù, in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.\nThe ‚ÄúCorresponding Source‚Äù for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work‚Äôs System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.\nThe Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.\nThe Corresponding Source for a work in source code form is that same work.\n\nBasic Permissions.\n\nAll rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.\nYou may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.\nConveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.\n\nProtecting Users‚Äô Legal Rights From Anti-Circumvention Law.\n\nNo covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.\nWhen you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work‚Äôs users, your or third parties‚Äô legal rights to forbid circumvention of technological measures.\n\nConveying Verbatim Copies.\n\nYou may convey verbatim copies of the Program‚Äôs source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.\nYou may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.\n\nConveying Modified Source Versions.\n\nYou may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:\na) The work must carry prominent notices stating that you modified\nit, and giving a relevant date.\n\nb) The work must carry prominent notices stating that it is\nreleased under this License and any conditions added under section\n7.  This requirement modifies the requirement in section 4 to\n\"keep intact all notices\".\n\nc) You must license the entire work, as a whole, under this\nLicense to anyone who comes into possession of a copy.  This\nLicense will therefore apply, along with any applicable section 7\nadditional terms, to the whole of the work, and all its parts,\nregardless of how they are packaged.  This License gives no\npermission to license the work in any other way, but it does not\ninvalidate such permission if you have separately received it.\n\nd) If the work has interactive user interfaces, each must display\nAppropriate Legal Notices; however, if the Program has interactive\ninterfaces that do not display Appropriate Legal Notices, your\nwork need not make them do so.\nA compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an ‚Äúaggregate‚Äù if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation‚Äôs users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.\n\nConveying Non-Source Forms.\n\nYou may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:\na) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by the\nCorresponding Source fixed on a durable physical medium\ncustomarily used for software interchange.\n\nb) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by a\nwritten offer, valid for at least three years and valid for as\nlong as you offer spare parts or customer support for that product\nmodel, to give anyone who possesses the object code either (1) a\ncopy of the Corresponding Source for all the software in the\nproduct that is covered by this License, on a durable physical\nmedium customarily used for software interchange, for a price no\nmore than your reasonable cost of physically performing this\nconveying of source, or (2) access to copy the\nCorresponding Source from a network server at no charge.\n\nc) Convey individual copies of the object code with a copy of the\nwritten offer to provide the Corresponding Source.  This\nalternative is allowed only occasionally and noncommercially, and\nonly if you received the object code with such an offer, in accord\nwith subsection 6b.\n\nd) Convey the object code by offering access from a designated\nplace (gratis or for a charge), and offer equivalent access to the\nCorresponding Source in the same way through the same place at no\nfurther charge.  You need not require recipients to copy the\nCorresponding Source along with the object code.  If the place to\ncopy the object code is a network server, the Corresponding Source\nmay be on a different server (operated by you or a third party)\nthat supports equivalent copying facilities, provided you maintain\nclear directions next to the object code saying where to find the\nCorresponding Source.  Regardless of what server hosts the\nCorresponding Source, you remain obligated to ensure that it is\navailable for as long as needed to satisfy these requirements.\n\ne) Convey the object code using peer-to-peer transmission, provided\nyou inform other peers where the object code and Corresponding\nSource of the work are being offered to the general public at no\ncharge under subsection 6d.\nA separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.\nA ‚ÄúUser Product‚Äù is either (1) a ‚Äúconsumer product‚Äù, which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, ‚Äúnormally used‚Äù refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.\n‚ÄúInstallation Information‚Äù for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.\nIf you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).\nThe requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.\nCorresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.\n\nAdditional Terms.\n\n‚ÄúAdditional permissions‚Äù are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.\nWhen you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.\nNotwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:\na) Disclaiming warranty or limiting liability differently from the\nterms of sections 15 and 16 of this License; or\n\nb) Requiring preservation of specified reasonable legal notices or\nauthor attributions in that material or in the Appropriate Legal\nNotices displayed by works containing it; or\n\nc) Prohibiting misrepresentation of the origin of that material, or\nrequiring that modified versions of such material be marked in\nreasonable ways as different from the original version; or\n\nd) Limiting the use for publicity purposes of names of licensors or\nauthors of the material; or\n\ne) Declining to grant rights under trademark law for use of some\ntrade names, trademarks, or service marks; or\n\nf) Requiring indemnification of licensors and authors of that\nmaterial by anyone who conveys the material (or modified versions of\nit) with contractual assumptions of liability to the recipient, for\nany liability that these contractual assumptions directly impose on\nthose licensors and authors.\nAll other non-permissive additional terms are considered ‚Äúfurther restrictions‚Äù within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.\nIf you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.\nAdditional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.\n\nTermination.\n\nYou may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).\nHowever, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.\nMoreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.\nTermination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.\n\nAcceptance Not Required for Having Copies.\n\nYou are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.\n\nAutomatic Licensing of Downstream Recipients.\n\nEach time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.\nAn ‚Äúentity transaction‚Äù is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party‚Äôs predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.\nYou may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.\n\nPatents.\n\nA ‚Äúcontributor‚Äù is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor‚Äôs ‚Äúcontributor version‚Äù.\nA contributor‚Äôs ‚Äúessential patent claims‚Äù are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, ‚Äúcontrol‚Äù includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.\nEach contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor‚Äôs essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.\nIn the following three paragraphs, a ‚Äúpatent license‚Äù is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To ‚Äúgrant‚Äù such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.\nIf you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. ‚ÄúKnowingly relying‚Äù means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient‚Äôs use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.\nIf, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.\nA patent license is ‚Äúdiscriminatory‚Äù if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.\nNothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.\n\nNo Surrender of Others‚Äô Freedom.\n\nIf conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.\n\nUse with the GNU Affero General Public License.\n\nNotwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU Affero General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the special requirements of the GNU Affero General Public License, section 13, concerning interaction through a network will apply to the combination as such.\n\nRevised Versions of this License.\n\nThe Free Software Foundation may publish revised and/or new versions of the GNU General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.\nEach version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU General Public License ‚Äúor any later version‚Äù applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU General Public License, you may choose any version ever published by the Free Software Foundation.\nIf the Program specifies that a proxy can decide which future versions of the GNU General Public License can be used, that proxy‚Äôs public statement of acceptance of a version permanently authorizes you to choose that version for the Program.\nLater license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.\n\nDisclaimer of Warranty.\n\nTHERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM ‚ÄúAS IS‚Äù WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n\nLimitation of Liability.\n\nIN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\nInterpretation of Sections 15 and 16.\n\nIf the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.\n                 END OF TERMS AND CONDITIONS\n\n        How to Apply These Terms to Your New Programs\nIf you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.\nTo do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the ‚Äúcopyright‚Äù line and a pointer to where the full notice is found.\n&lt;one line to give the program's name and a brief idea of what it does.&gt;\nCopyright (C) &lt;year&gt;  &lt;name of author&gt;\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\nAlso add information on how to contact you by electronic and paper mail.\nIf the program does terminal interaction, make it output a short notice like this when it starts in an interactive mode:\n&lt;program&gt;  Copyright (C) &lt;year&gt;  &lt;name of author&gt;\nThis program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.\nThis is free software, and you are welcome to redistribute it\nunder certain conditions; type `show c' for details.\nThe hypothetical commands show w' andshow c‚Äô should show the appropriate parts of the General Public License. Of course, your program‚Äôs commands might be different; for a GUI interface, you would use an ‚Äúabout box‚Äù.\nYou should also get your employer (if you work as a programmer) or school, if any, to sign a ‚Äúcopyright disclaimer‚Äù for the program, if necessary. For more information on this, and how to apply and follow the GNU GPL, see https://www.gnu.org/licenses/.\nThe GNU General Public License does not permit incorporating your program into proprietary programs. If your program is a subroutine library, you may consider it more useful to permit linking proprietary applications with the library. If this is what you want to do, use the GNU Lesser General Public License instead of this License. But first, please read https://www.gnu.org/licenses/why-not-lgpl.html.\n\n\n\nThe original study was published in the journal ‚ÄúFrontiers in Neurology‚Äù. They distributed the article under a CC-BY license.\n\n\n\n\n\n\nView copyright statement from journal\n\n\n\n\n\nFrom https://doi.org/10.3389/fneur.2019.00653:\n‚Äú¬© 2019 Huang, Maingard, Kok, Barras, Thijs, Chandra, Brooks and Asadi. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.‚Äù"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reproducing Huang et al. 2019",
    "section": "",
    "text": "This book captures the reproduction of:\n\nHuang S, Maingard J, Kok HK, Barras CD, Thijs V, Chandra RV, Brooks DM and Asadi H. Optimizing Resources for Endovascular Clot Retrieval for Acute Ischemic Stroke, a Discrete Event Simulation. Frontiers in Neurology 10, 653 (2019). https://doi.org/10.3389/fneur.2019.00653.\n\nUse the navigation bar above to view:\n\nOriginal study - the original study article and associated artefacts.\nReproduction - code and documentation from reproduction of the model.\nEvaluation - describes model reproduction success and compares original study against guidelines for sharing research, criteria for journal reproducibility guidelines, and article reporting guidelines.\nLogbook - chronological entries detailing reproduction work.\nSummary - summary of the computational reproducibility assessment."
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Reproducing Huang et al. 2019",
    "section": "",
    "text": "This book captures the reproduction of:\n\nHuang S, Maingard J, Kok HK, Barras CD, Thijs V, Chandra RV, Brooks DM and Asadi H. Optimizing Resources for Endovascular Clot Retrieval for Acute Ischemic Stroke, a Discrete Event Simulation. Frontiers in Neurology 10, 653 (2019). https://doi.org/10.3389/fneur.2019.00653.\n\nUse the navigation bar above to view:\n\nOriginal study - the original study article and associated artefacts.\nReproduction - code and documentation from reproduction of the model.\nEvaluation - describes model reproduction success and compares original study against guidelines for sharing research, criteria for journal reproducibility guidelines, and article reporting guidelines.\nLogbook - chronological entries detailing reproduction work.\nSummary - summary of the computational reproducibility assessment."
  },
  {
    "objectID": "index.html#project-team",
    "href": "index.html#project-team",
    "title": "Reproducing Huang et al. 2019",
    "section": "Project team",
    "text": "Project team\n\nConducting this reproduction:\n\nAmy Heather \n\nProviding support during the reproduction:\n\nThomas Monks \nAlison Harper \n\nOther members of the team on STARS:\n\nNavonil Mustafee \nAndrew Mayne"
  },
  {
    "objectID": "index.html#protocol",
    "href": "index.html#protocol",
    "title": "Reproducing Huang et al. 2019",
    "section": "Protocol",
    "text": "Protocol\nThe protocol for this work is summarised in the diagram below and archived on Zenodo:\n\nHeather, A., Monks, T., Harper, A., Mustafee, N., & Mayne, A. (2024). Protocol for assessing the computational reproducibility of discrete-event simulation models on STARS. Zenodo. https://doi.org/10.5281/zenodo.12179846.\n\n\n\n\nWorkflow for computational reproducibility assessment"
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "Reproducing Huang et al. 2019",
    "section": "Citation",
    "text": "Citation\nAPA: Heather A., Monks T., Harper A. (2024). STARS: Computational reproducibility of Huang et al.¬†2019 (version 0.1.0). URL: https://github.com/pythonhealthdatascience/stars-reproduce-huang-2019\nSee CITATION.cff and citation_bibtex.bib for alternative formats."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Reproducing Huang et al. 2019",
    "section": "License",
    "text": "License\nSee License page."
  },
  {
    "objectID": "CHANGELOG.html",
    "href": "CHANGELOG.html",
    "title": "Changelog",
    "section": "",
    "text": "All notable changes to this project will be documented in this file.\nThe format is based on Keep a Changelog, and this project adheres to Semantic Versioning. Dates formatted as YYYY-MM-DD as per ISO standard.\n\n\nFirst release with defined scope for reproduction.\n\n\n\nCode from original study\nArticle\nPlanned scope for reproduction\n\n\n\n\n\nModified template to be relevant to Huang et al.¬†2019"
  },
  {
    "objectID": "CHANGELOG.html#v0.1.0---2024-07-04",
    "href": "CHANGELOG.html#v0.1.0---2024-07-04",
    "title": "Changelog",
    "section": "",
    "text": "First release with defined scope for reproduction.\n\n\n\nCode from original study\nArticle\nPlanned scope for reproduction\n\n\n\n\n\nModified template to be relevant to Huang et al.¬†2019"
  },
  {
    "objectID": "quarto_site/reproduction_readme.html",
    "href": "quarto_site/reproduction_readme.html",
    "title": "README for reproduction",
    "section": "",
    "text": "Huang S, Maingard J, Kok HK, Barras CD, Thijs V, Chandra RV, Brooks DM and Asadi H. Optimizing Resources for Endovascular Clot Retrieval for Acute Ischemic Stroke, a Discrete Event Simulation. Frontiers in Neurology 10, 653 (2019). https://doi.org/10.3389/fneur.2019.00653.\n\nThis is a discrete-event simulation model of an endovascular clot retrieval (ECR) service. ECR is a treatment for acute ischaemic stroke. The model includes the stroke pathway, as well as three other pathways that share resources with the stroke pathway: an elective non-stroke interventional neuroradiology pathway, an emergency interventional radiology pathway, and an elective interventional radiology pathway.\nThe model is created using R Simmer.\nThe paper explores waiting times and resource utilisation - particularly focussing on the biplane angiographic suite (angioINR). A few scenarios are tried to help examine why the wait times are so high for the angioINR.\nModel structure from Huang et al.¬†2019:\n\n\n\nProcess flow diagram from Huang et al.¬†2019\n\n\n\n\n\nIn this assessment, we attempted to reproduce 8 items: 5 figures and 3 in-text results.\n\n\n\n\n\n‚îú‚îÄ‚îÄ docker\n‚îÇ   ‚îî‚îÄ‚îÄ  ...\n‚îú‚îÄ‚îÄ outputs\n‚îÇ   ‚îî‚îÄ‚îÄ  ...\n‚îú‚îÄ‚îÄ renv\n‚îÇ   ‚îî‚îÄ‚îÄ  ...\n‚îú‚îÄ‚îÄ scripts\n‚îÇ   ‚îî‚îÄ‚îÄ  ...\n‚îú‚îÄ‚îÄ tests\n‚îÇ   ‚îî‚îÄ‚îÄ  ...\n‚îú‚îÄ‚îÄ .Rprofile\n‚îú‚îÄ‚îÄ DESCRIPTION\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ reproduction.Rproj\n‚îî‚îÄ‚îÄ renv.lock\n\ndocker/ - Instructions for creation of docker container.\noutputs/ - Outputs files from the scripts (e.g.¬†.csv.gz, .png)\nrenv/ - Instructions for creation of R environment\nscripts/ - Code for the model and for reproducing items from the scope\ntests/ - Test to check that the model produces consistent results with our reproduction\n.Rprofile - Activates R environment\nDESCRIPTION - Lists packages that we installed into environment (their dependencies will have also been installed)\nREADME.md - This file!\nhuang2019.Rproj - Project settings, which specify the Python virtual environment to use when building pages from the Quarto site that include Python. If you choose to build the Quarto site (and not just run the reproduction files in this folder), you will want to update this to a path on your machine (which you can do easily by opening this file in RStudio)\nrenv.lock - Lists R version and all packages in the R environment\n\n\n\n\nNote: We have found this model to have high memory usage. When running in RStudio, we found it was using about 8GB RAM. Hence, will require a machine with sufficient memory (e.g.¬†we weren‚Äôt able to run on virtual machine as that only allocates 4GB RAM).\n\n\n\nBefore you can run the model, you will need to create an R environment with the correct version of R and the specified packages.\n\n\nAn renv environment has been provided. To create this environment locally on your machine, you should open the R project with the R environment loaded, and then run:\nrenv::restore()\nIn renv.lock, you will see the version of R listed. However, renv will not install this for you, so you will need to switch to this yourself if you wish to also use the same version of R. This reproduction has been run in R 4.4.1, and it is possible (although not definite) that later versions of R may not be compatible, or that you may encounter difficulties installing the specified package versions in later versions of R.\n\n\n\nA Dockerfile is provided, which you can use to build the Docker image. The docker image will include the correct version of R, the required packages and their versions, and an installation of RStudio which you can run from your browser. It will also include the scripts and outputs from this directory. For this option and option C, you‚Äôll need to ensure that docker is installed on your machine.\nTo create the docker image and then open up RStudio:\n\nIn the terminal, navigate to the parent directory of your reproduction/ folder\nBuild the image:\n\ndocker build --tag huang2019 . -f ./reproduction/docker/Dockerfile\n\nCreate container and open RStudio:\n\n(sleep 2 && xdg-open http://localhost:8888) & sudo docker run -it -p 8888:8787 -e DISABLE_AUTH=true --name huang2019_docker huang2019\n\n\n\nA pre-built image is available on the GitHub container registry. To use it:\n\nCreate a Personal Access Token (Classic) for your GitHub account with write:packages and delete:packages access\nOn terminal, run the following command and then enter your sudo password (if prompted), followed by the token just generated (which acts as your GitHub password)\n\nsudo docker login ghcr.io -u githubusername\n\nDownload the image:\n\nsudo docker pull ghcr.io/pythonhealthdatascience/huang2019\n\nCreate container and open RStudio:\n\n(sleep 2 && xdg-open http://localhost:8888) & sudo docker run -it -p 8888:8787 -e DISABLE_AUTH=true --name huang2019_docker ghcr.io/pythonhealthdatascience/huang2019:latest\n\n\n\n\n\n\nTo run all the model scenarios, open and execute the provided .qmd files in scripts/. You can do so within your preferred IDE (e.g.¬†RStudio).\n\n\n\nThree of the model scenarios have been included as tests within tests/testthat. You can run these tests by running the following command from your R console whilst in the reproduction/ directory:\ntestthat::test_dir(\"tests/testthat\")\nThis will run the three scenarios, save the results as temporary files, and compare the results against those we have saved. Although this will not produce any figures from the paper, and will not run all the scenarios, it will allow you to check if you are getting results consistent with our reproduction, on your own machine.\nAs the tests run, you will see the counter increments on your screen (with the column indicating whether the test is successful). For example, if tests are successul, you will see it increment in the ‚ÄúOK‚Äù column:\n‚úî | F W  S  OK | Context\n‚†è |          0 | model                                               [1] \"\"\n‚†ã |          1 | model                                               [1] \"\"\nEach test will take about 2 minutes (for the machine specs given below). Once all three tests are complete, the run time and results will display:\n‚ïê‚ïê Results ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nDuration: 371.9 s\n\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 3 ]\n\n\n\n\n\nThis reproduction was conducted on an Intel Core i7-12700H with 32GB RAM running Ubuntu 22.04.4 Linux.\nOn this machine, the reproduction run time was 29 minutes 10 seconds. This was the total time from executing all the .qmd files that run the model and attempt to produce the figures/results (18.024 + 6.165 + 4.975 minutes).\nThe run time for the tests (which only include a few model scenarios) was 6 minutes 12 seconds.\n\n\n\nTo cite the original study, please refer to the reference above. To cite this reproduction, please refer to the CITATION.cff file in the parent folder.\n\n\n\nThis repository is licensed under the GNU GPL-3.0 license."
  },
  {
    "objectID": "quarto_site/reproduction_readme.html#model-summary",
    "href": "quarto_site/reproduction_readme.html#model-summary",
    "title": "README for reproduction",
    "section": "",
    "text": "Huang S, Maingard J, Kok HK, Barras CD, Thijs V, Chandra RV, Brooks DM and Asadi H. Optimizing Resources for Endovascular Clot Retrieval for Acute Ischemic Stroke, a Discrete Event Simulation. Frontiers in Neurology 10, 653 (2019). https://doi.org/10.3389/fneur.2019.00653.\n\nThis is a discrete-event simulation model of an endovascular clot retrieval (ECR) service. ECR is a treatment for acute ischaemic stroke. The model includes the stroke pathway, as well as three other pathways that share resources with the stroke pathway: an elective non-stroke interventional neuroradiology pathway, an emergency interventional radiology pathway, and an elective interventional radiology pathway.\nThe model is created using R Simmer.\nThe paper explores waiting times and resource utilisation - particularly focussing on the biplane angiographic suite (angioINR). A few scenarios are tried to help examine why the wait times are so high for the angioINR.\nModel structure from Huang et al.¬†2019:\n\n\n\nProcess flow diagram from Huang et al.¬†2019"
  },
  {
    "objectID": "quarto_site/reproduction_readme.html#scope-of-the-reproduction",
    "href": "quarto_site/reproduction_readme.html#scope-of-the-reproduction",
    "title": "README for reproduction",
    "section": "",
    "text": "In this assessment, we attempted to reproduce 8 items: 5 figures and 3 in-text results."
  },
  {
    "objectID": "quarto_site/reproduction_readme.html#reproducing-these-results",
    "href": "quarto_site/reproduction_readme.html#reproducing-these-results",
    "title": "README for reproduction",
    "section": "",
    "text": "‚îú‚îÄ‚îÄ docker\n‚îÇ   ‚îî‚îÄ‚îÄ  ...\n‚îú‚îÄ‚îÄ outputs\n‚îÇ   ‚îî‚îÄ‚îÄ  ...\n‚îú‚îÄ‚îÄ renv\n‚îÇ   ‚îî‚îÄ‚îÄ  ...\n‚îú‚îÄ‚îÄ scripts\n‚îÇ   ‚îî‚îÄ‚îÄ  ...\n‚îú‚îÄ‚îÄ tests\n‚îÇ   ‚îî‚îÄ‚îÄ  ...\n‚îú‚îÄ‚îÄ .Rprofile\n‚îú‚îÄ‚îÄ DESCRIPTION\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ reproduction.Rproj\n‚îî‚îÄ‚îÄ renv.lock\n\ndocker/ - Instructions for creation of docker container.\noutputs/ - Outputs files from the scripts (e.g.¬†.csv.gz, .png)\nrenv/ - Instructions for creation of R environment\nscripts/ - Code for the model and for reproducing items from the scope\ntests/ - Test to check that the model produces consistent results with our reproduction\n.Rprofile - Activates R environment\nDESCRIPTION - Lists packages that we installed into environment (their dependencies will have also been installed)\nREADME.md - This file!\nhuang2019.Rproj - Project settings, which specify the Python virtual environment to use when building pages from the Quarto site that include Python. If you choose to build the Quarto site (and not just run the reproduction files in this folder), you will want to update this to a path on your machine (which you can do easily by opening this file in RStudio)\nrenv.lock - Lists R version and all packages in the R environment\n\n\n\n\nNote: We have found this model to have high memory usage. When running in RStudio, we found it was using about 8GB RAM. Hence, will require a machine with sufficient memory (e.g.¬†we weren‚Äôt able to run on virtual machine as that only allocates 4GB RAM).\n\n\n\nBefore you can run the model, you will need to create an R environment with the correct version of R and the specified packages.\n\n\nAn renv environment has been provided. To create this environment locally on your machine, you should open the R project with the R environment loaded, and then run:\nrenv::restore()\nIn renv.lock, you will see the version of R listed. However, renv will not install this for you, so you will need to switch to this yourself if you wish to also use the same version of R. This reproduction has been run in R 4.4.1, and it is possible (although not definite) that later versions of R may not be compatible, or that you may encounter difficulties installing the specified package versions in later versions of R.\n\n\n\nA Dockerfile is provided, which you can use to build the Docker image. The docker image will include the correct version of R, the required packages and their versions, and an installation of RStudio which you can run from your browser. It will also include the scripts and outputs from this directory. For this option and option C, you‚Äôll need to ensure that docker is installed on your machine.\nTo create the docker image and then open up RStudio:\n\nIn the terminal, navigate to the parent directory of your reproduction/ folder\nBuild the image:\n\ndocker build --tag huang2019 . -f ./reproduction/docker/Dockerfile\n\nCreate container and open RStudio:\n\n(sleep 2 && xdg-open http://localhost:8888) & sudo docker run -it -p 8888:8787 -e DISABLE_AUTH=true --name huang2019_docker huang2019\n\n\n\nA pre-built image is available on the GitHub container registry. To use it:\n\nCreate a Personal Access Token (Classic) for your GitHub account with write:packages and delete:packages access\nOn terminal, run the following command and then enter your sudo password (if prompted), followed by the token just generated (which acts as your GitHub password)\n\nsudo docker login ghcr.io -u githubusername\n\nDownload the image:\n\nsudo docker pull ghcr.io/pythonhealthdatascience/huang2019\n\nCreate container and open RStudio:\n\n(sleep 2 && xdg-open http://localhost:8888) & sudo docker run -it -p 8888:8787 -e DISABLE_AUTH=true --name huang2019_docker ghcr.io/pythonhealthdatascience/huang2019:latest\n\n\n\n\n\n\nTo run all the model scenarios, open and execute the provided .qmd files in scripts/. You can do so within your preferred IDE (e.g.¬†RStudio).\n\n\n\nThree of the model scenarios have been included as tests within tests/testthat. You can run these tests by running the following command from your R console whilst in the reproduction/ directory:\ntestthat::test_dir(\"tests/testthat\")\nThis will run the three scenarios, save the results as temporary files, and compare the results against those we have saved. Although this will not produce any figures from the paper, and will not run all the scenarios, it will allow you to check if you are getting results consistent with our reproduction, on your own machine.\nAs the tests run, you will see the counter increments on your screen (with the column indicating whether the test is successful). For example, if tests are successul, you will see it increment in the ‚ÄúOK‚Äù column:\n‚úî | F W  S  OK | Context\n‚†è |          0 | model                                               [1] \"\"\n‚†ã |          1 | model                                               [1] \"\"\nEach test will take about 2 minutes (for the machine specs given below). Once all three tests are complete, the run time and results will display:\n‚ïê‚ïê Results ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nDuration: 371.9 s\n\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 3 ]"
  },
  {
    "objectID": "quarto_site/reproduction_readme.html#reproduction-specs-and-runtime",
    "href": "quarto_site/reproduction_readme.html#reproduction-specs-and-runtime",
    "title": "README for reproduction",
    "section": "",
    "text": "This reproduction was conducted on an Intel Core i7-12700H with 32GB RAM running Ubuntu 22.04.4 Linux.\nOn this machine, the reproduction run time was 29 minutes 10 seconds. This was the total time from executing all the .qmd files that run the model and attempt to produce the figures/results (18.024 + 6.165 + 4.975 minutes).\nThe run time for the tests (which only include a few model scenarios) was 6 minutes 12 seconds."
  },
  {
    "objectID": "quarto_site/reproduction_readme.html#citation",
    "href": "quarto_site/reproduction_readme.html#citation",
    "title": "README for reproduction",
    "section": "",
    "text": "To cite the original study, please refer to the reference above. To cite this reproduction, please refer to the CITATION.cff file in the parent folder."
  },
  {
    "objectID": "quarto_site/reproduction_readme.html#license",
    "href": "quarto_site/reproduction_readme.html#license",
    "title": "README for reproduction",
    "section": "",
    "text": "This repository is licensed under the GNU GPL-3.0 license."
  },
  {
    "objectID": "quarto_site/study_publication.html",
    "href": "quarto_site/study_publication.html",
    "title": "Publication",
    "section": "",
    "text": "View at: https://github.com/pythonhealthdatascience/stars-reproduce-huang-2019/tree/main/original_study/desECR\nCode from: https://github.com/shiweih/desECR"
  },
  {
    "objectID": "quarto_site/study_publication.html#code-and-data",
    "href": "quarto_site/study_publication.html#code-and-data",
    "title": "Publication",
    "section": "",
    "text": "View at: https://github.com/pythonhealthdatascience/stars-reproduce-huang-2019/tree/main/original_study/desECR\nCode from: https://github.com/shiweih/desECR"
  },
  {
    "objectID": "quarto_site/study_publication.html#journal-article",
    "href": "quarto_site/study_publication.html#journal-article",
    "title": "Publication",
    "section": "Journal article",
    "text": "Journal article\nArticle from: https://doi.org/10.3389/fneur.2019.00653"
  },
  {
    "objectID": "quarto_site/study_publication.html#supplementary-materials",
    "href": "quarto_site/study_publication.html#supplementary-materials",
    "title": "Publication",
    "section": "Supplementary materials",
    "text": "Supplementary materials\nThe supplementary material is an additional image saved as a .TIFF file:\n\n\n\nSupplementary figure"
  },
  {
    "objectID": "quarto_site/study_publication.html#interactive-web-app",
    "href": "quarto_site/study_publication.html#interactive-web-app",
    "title": "Publication",
    "section": "Interactive web app",
    "text": "Interactive web app\nThe paper also links to an interactive web app for the model which can be found at: https://rebrand.ly/desECR11 (which redirects to https://compneuro.shinyapps.io/desECR11/).\nThe simulation also links to https://beta.cloudes.me/loadShare?simId=17588, stating that it can provide the details of the simulation (although this link does not work, if you login to CLOUDES, you can identify what appears to be a copy of that model under the ID 17482 or by searching ‚ÄúHuang‚Äù)."
  },
  {
    "objectID": "reproduction/scripts/reproduction_fig5.html",
    "href": "reproduction/scripts/reproduction_fig5.html",
    "title": "Reproduce Figure 5",
    "section": "",
    "text": "This is run in a separate script from the other figures due to issues with RStudio crashing when all scenarios were run from a single script.\nCurrently depends on simmer.plot() function that doesn‚Äôt work on the imported results from the csv file, so need to allow to run model to produce this plot! Hence, you will only see results if run &lt;- TRUE. Ordinarily, we leave as FALSE so that quarto site is still built quickly.\nRun time: 6.165 minutes (will vary between machines)"
  },
  {
    "objectID": "reproduction/scripts/reproduction_fig5.html#set-up",
    "href": "reproduction/scripts/reproduction_fig5.html#set-up",
    "title": "Reproduce Figure 5",
    "section": "Set up",
    "text": "Set up\n\n# Clear environment\nrm(list=ls())\n\n# Start timer\nstart.time &lt;- Sys.time()\n\n# Disable scientific notation\noptions(scipen=999)\n\n# Get the model and helper functions (but hide loading warnings for each package)\nsuppressMessages(source(\"model.R\"))\nsuppressMessages(source(\"helpers.R\"))\n\n\n# Set the seed and default dimensions for figures\nSEED = 200\n\n# Set file paths to save results\nfolder = \"../outputs\"\npath_fig5 &lt;- file.path(folder, \"fig5.png\")"
  },
  {
    "objectID": "reproduction/scripts/reproduction_fig5.html#run-models",
    "href": "reproduction/scripts/reproduction_fig5.html#run-models",
    "title": "Reproduce Figure 5",
    "section": "Run models",
    "text": "Run models\n\nrun &lt;- FALSE\n\n\nif (isTRUE(run)) {\n  baseline_f5 &lt;- run_model(seed = SEED, fig5=TRUE)\n  exclusive_f5 &lt;- run_model(exclusive_use = TRUE, seed = SEED, fig5=TRUE)\n  twoangio_f5 &lt;- run_model(angio_inr = 2, angio_ir=0, seed = SEED, fig5=TRUE)\n}"
  },
  {
    "objectID": "reproduction/scripts/reproduction_fig5.html#create-figure",
    "href": "reproduction/scripts/reproduction_fig5.html#create-figure",
    "title": "Reproduce Figure 5",
    "section": "Create figure",
    "text": "Create figure\n\nif (isTRUE(run)) {\n  # Replace resource (which has been filtered to angioINR) with scenario\n  baseline_f5$resource &lt;- \"Baseline\"\n  exclusive_f5$resource &lt;-\"Exclusive-use\"\n  twoangio_f5$resource &lt;- \"Two angio INRs\"\n  \n  # Combine into single object\n  fig5_df &lt;- dplyr::bind_rows(baseline_f5, exclusive_f5, twoangio_f5)\n  \n  # Create figure using simmer's plot\n  p &lt;- plot(fig5_df, metric=\"utilization\") +\n    xlab(\"Scenarios\") +\n    ylab(\"Utilisation\") +\n    scale_y_continuous(labels = scales::percent, limits=c(0, 0.4)) +\n    ggtitle(\"\") +\n    geom_text(aes(label=round(.data$Q50*100)), vjust=-1)\n  p\n  \n  # Save to provided path\n  ggsave(path_fig5, width=5, height=2.5)\n}"
  },
  {
    "objectID": "reproduction/scripts/reproduction_fig5.html#time-elapsed",
    "href": "reproduction/scripts/reproduction_fig5.html#time-elapsed",
    "title": "Reproduce Figure 5",
    "section": "Time elapsed",
    "text": "Time elapsed\n\nif (isTRUE(run)) {\n  end.time &lt;- Sys.time()\n  elapsed.time &lt;- round((end.time - start.time), 3)\n  elapsed.time\n}"
  },
  {
    "objectID": "evaluation/artefacts.html",
    "href": "evaluation/artefacts.html",
    "title": "STARS framework",
    "section": "",
    "text": "This page evaluates the extent to which the original study meets the recommendations from the STARS framework for the sharing of code and associated materials from discrete-event simulation models (Monks, Harper, and Mustafee (2024)).\nOf the 8 essential STARS components:\n\n2 were met fully (‚úÖ)\n6 were not met (‚ùå)\n\nOf the 5 optional STARS components:\n\n2 were met fully (‚úÖ)\n3 were not met (‚ùå)\n\n\n\n\n\n\n\n\n\n\nComponent\nDescription\nMet by study?\nEvidence/location\n\n\n\n\nEssential components\n\n\n\n\n\nOpen license\nFree and open-source software (FOSS) license (e.g.¬†MIT, GNU Public License (GPL))\n‚úÖ Fully\nGPL-3.0\n\n\nDependency management\nSpecify software libraries, version numbers and sources (e.g.¬†dependency management tools like virtualenv, conda, poetry)\n‚ùå Not met\n-\n\n\nFOSS model\nCoded in FOSS language (e.g.¬†R, Julia, Python)\n‚úÖ Fully\nR\n\n\nMinimum documentation\nMinimal instructions (e.g.¬†in README) that overview (a) what model does, (b) how to install and run model to obtain results, and (c) how to vary parameters to run new experiments\n‚ùå Not met\nNo documentation provided\n\n\nORCID\nORCID for each study author\n‚ùå Not met\n-\n\n\nCitation information\nInstructions on how to cite the research artefact (e.g.¬†CITATION.cff file)\n‚ùå Not met\n-\n\n\nRemote code repository\nCode available in a remote code repository (e.g.¬†GitHub, GitLab, BitBucket)\n‚ùå Not met\n-\n\n\nOpen science archive\nCode stored in an open science archive with FORCE11 compliant citation and guaranteed persistance of digital artefacts (e.g.¬†Figshare, Zenodo, the Open Science Framework (OSF), and the Computational Modeling in the Social and Ecological Sciences Network (CoMSES Net))\n‚ùå Not met\n-\n\n\nOptional components\n\n\n\n\n\nEnhanced documentation\nOpen and high quality documentation on how the model is implemented and works (e.g.¬†via notebooks and markdown files, brought together using software like Quarto and Jupyter Book). Suggested content includes:‚Ä¢ Plain english summary of project and model‚Ä¢ Clarifying license‚Ä¢ Citation instructions‚Ä¢ Contribution instructions‚Ä¢ Model installation instructions‚Ä¢ Structured code walk through of model‚Ä¢ Documentation of modelling cycle using TRACE‚Ä¢ Annotated simulation reporting guidelines‚Ä¢ Clear description of model validation including its intended purpose\n‚ùå Not met\n-\n\n\nDocumentation hosting\nHost documentation (e.g.¬†with GitHub pages, GitLab pages, BitBucket Cloud, Quarto Pub)\n‚ùå Not met\n-\n\n\nOnline coding environment\nProvide an online environment where users can run and change code (e.g.¬†BinderHub, Google Colaboratory, Deepnote)\n‚ùå Not met\n-\n\n\nModel interface\nProvide web application interface to the model so it is accessible to less technical simulation users\n‚úÖ Fully\nShiny application that allows you to modify parameters and produces graphs showing waiting times for each patient type at the angioINR (boxplots grouped into &lt;20, 20-40 and 40+ minutes), and resource utilisation. There is also a linked CLOUDES model of the simulation to aid user understanding.\n\n\nWeb app hosting\nHost web app online (e.g.¬†Streamlit Community Cloud, ShinyApps hosting)\n‚úÖ Fully\nHosted with ShinyApps at https://compneuro.shinyapps.io/desECR11/\n\n\n\n\n\n\n\nReferences\n\nMonks, Thomas, Alison Harper, and Navonil Mustafee. 2024. ‚ÄúTowards Sharing Tools and Artefacts for Reusable Simulations in Healthcare.‚Äù Journal of Simulation 0 (0): 1‚Äì20. https://doi.org/10.1080/17477778.2024.2347882."
  },
  {
    "objectID": "evaluation/reproduction_success.html",
    "href": "evaluation/reproduction_success.html",
    "title": "Reproduction success",
    "section": "",
    "text": "Of the 8 items in the scope, 37.5% (3 out of 8) were considered to be successfully reproduced.\nAs cited throughout, images on this page are sourced from Huang et al. (2019)."
  },
  {
    "objectID": "evaluation/reproduction_success.html#time-to-completion",
    "href": "evaluation/reproduction_success.html#time-to-completion",
    "title": "Reproduction success",
    "section": "Time-to-completion",
    "text": "Time-to-completion\nNon-interactive plot:\n\n\n\n\n\n\n\n\n\nInteractive plot:"
  },
  {
    "objectID": "evaluation/reproduction_success.html#reproduction-of-items-from-the-scope",
    "href": "evaluation/reproduction_success.html#reproduction-of-items-from-the-scope",
    "title": "Reproduction success",
    "section": "Reproduction of items from the scope",
    "text": "Reproduction of items from the scope\n\nFigure 2\nConsensus: Not reproduced\nOriginal (Huang et al. (2019)):\n\n\n\n\n\nReproduction (angio_staff was hidden right behind inr, so have removed inr):\n\n\n\n\n\n\n\nFigure 3\nConsensus: Not reproduced\nOriginal (Huang et al. (2019)):\n\n\n\n\n\nReproduction:\n\n\n\n\n\n\n\nFigure 4\nConsensus: Not reproduced\nOriginal (Huang et al. (2019)):\n\n\n\n\n\nReproduction:\n\n\n\n\n\n\n\nFigure 5\nConsensus: Successfully reproduced\nOriginal (Huang et al. (2019)):\n\n\n\n\n\nReproduction:\n\n\n\n\n\n\n\nSupplementary figure\nConsensus: Not reproduced\nOriginal (Huang et al. (2019)):\n\n\n\n\n\nReproduction (angio_staff was hidden right behind inr, so have removed inr):\n\n\n\n\n\n\n\nIn-text result 1\nConsensus: Successfully reproduced\n‚ÄúExclusive-Use Scenario. In this scenario, the overall wait time probability at angioINR was reduced compared to baseline (red line in Figure 2B compared to Figure 2A). This represents a decrease in ECR patient wait time for angioINR by an average of 6 min.‚Äù Huang et al. (2019)\nReproduction:\n\n\n\n\n\n\n\n\n\n\nscenario\nmean\ndiff_from_baseline\n\n\n\n\n0\nBaseline\n13.958269\n0.00\n\n\n1\nExclusive use\n8.117729\n-5.84\n\n\n\n\n\n\n\n\n\n\nIn-text result 2\nConsensus: Successfully reproduced\n‚ÄúTwo angioINRs Scenario. This scenario simulates the effect a facility upgrade to two biplane angiographic suites, but without additional staff changes. The wait time probability at angioINR was reduced compared to baseline (Figure 2C). The reduction represents an average of 4 min less in queue for angioINR.‚Äù Huang et al. (2019)\nReproduction:\n\n\n\n\n\n\n\n\n\n\nscenario\nmean\ndiff_from_baseline\n\n\n\n\n0\nBaseline\n13.958269\n0.00\n\n\n2\nTwo AngioINRs\n9.621122\n-4.34\n\n\n\n\n\n\n\n\n\n\nIn-text result 3\nConsensus: Not reproduced\n‚ÄúExtended Schedule Scenario. The wait time probability at angioINR in the exclusive- use scenario was further reduced by extended work hours (Figure 3B). In contrast, work extension did not affect baseline or the 2 angioINRs scenario (Figures 3A,C). For the baseline scenario, 1 and 2 h of extra work resulted in an average wait time of 1.7 and 0.9 min reduction, respectively. For the 2 angioINRs scenario, 1 and 2 h of extra work resulted in an average wait time gain of 1 and 0.3 min, respectively.‚Äù Huang et al. (2019)\nReproduction:\n\n\n\n\n\n\n\n\n\n\nscenario\nshift\nmean\ndiff_from_5pm\n\n\n\n\n0\nBaseline\n5pm\n13.958269\n0.00\n\n\n1\nBaseline\n6pm\n12.486042\n-1.47\n\n\n2\nBaseline\n7pm\n12.491421\n-1.47\n\n\n6\nTwo AngioINRs\n5pm\n9.621122\n0.00\n\n\n7\nTwo AngioINRs\n6pm\n9.216435\n-0.40\n\n\n8\nTwo AngioINRs\n7pm\n8.699223\n-0.92"
  },
  {
    "objectID": "evaluation/scope.html",
    "href": "evaluation/scope.html",
    "title": "Scope",
    "section": "",
    "text": "This page outlines the parts of the journal article which we will attempt to reproduce.\nAll images and quotes on this page are sourced from Huang et al. (2019)"
  },
  {
    "objectID": "evaluation/scope.html#within-scope",
    "href": "evaluation/scope.html#within-scope",
    "title": "Scope",
    "section": "Within scope",
    "text": "Within scope\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\n\n\n\nFIGURE 2 | Patient wait time under various simulation scenarios (A). Baseline scenario simulated using inputs from Table 1 (B). Exclusive-use scenario: IR patients can only utilize angioIR (C). Two angioINRs scenario: 2 angioINRs, no angioIRs. Standardized density of patients in queue: the probability density of patients who are waiting standardized to patients who are not waiting. Huang et al. (2019)\n\n\n\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\n\n\n\nFIGURE 3 | The effect of increasing working hours on ECR patient wait time at angioINR (A). Baseline scenario (B). Exclusive-use scenario (C). Two angioINRs scenario. Standardized density of patients in queue: the probability density of patients who are waiting standardized to patients who are not waiting. Huang et al. (2019)\n\n\n\n\n\n\n\n\n\n\n\nFigure 4\n\n\n\n\n\n\n\n\nFIGURE 4 | Disability-free life gained under various scenarios. Huang et al. (2019)\n\n\n\n\n\n\n\n\n\n\n\nFigure 5\n\n\n\n\n\n\n\n\nFIGURE 5 | A comparison of the utilization of angioINR by ECR patients under various scenarios. Huang et al. (2019)\n\n\n\n\n\n\n\n\n\n\n\nSupplementary figure\n\n\n\n\n\n\n\n\nSupplementary Figure | Increasing ECR patient volume on service bottleneck. Standardized density of patients in queue: the probability density of patients who are waiting standardized to patients who are not waiting. (A) Baseline scenario. (B) Doubling ECR patients in baseline scenario. (C) Tripping ECR patients in baseline scenario. Huang et al. (2019)\n\n\n\n\n\n\n\n\n\n\n\nIn-text result 1\n\n\n\n\n\n‚ÄúExclusive-Use Scenario. In this scenario, the overall wait time probability at angioINR was reduced compared to baseline (red line in Figure 2B compared to Figure 2A). This represents a decrease in ECR patient wait time for angioINR by an average of 6 min.‚Äù Huang et al. (2019)\n\n\n\n\n\n\n\n\n\nIn-text result 2\n\n\n\n\n\n‚ÄúTwo angioINRs Scenario. This scenario simulates the effect a facility upgrade to two biplane angiographic suites, but without additional staff changes. The wait time probability at angioINR was reduced compared to baseline (Figure 2C). The reduction represents an average of 4 min less in queue for angioINR.‚Äù Huang et al. (2019)\n\n\n\n\n\n\n\n\n\nIn-text result 3\n\n\n\n\n\n‚ÄúExtended Schedule Scenario. The wait time probability at angioINR in the exclusive- use scenario was further reduced by extended work hours (Figure 3B). In contrast, work extension did not affect baseline or the 2 angioINRs scenario (Figures 3A,C). For the baseline scenario, 1 and 2 h of extra work resulted in an average wait time of 1.7 and 0.9 min reduction, respectively. For the 2 angioINRs scenario, 1 and 2 h of extra work resulted in an average wait time gain of 1 and 0.3 min, respectively.‚Äù Huang et al. (2019)"
  },
  {
    "objectID": "evaluation/scope.html#outside-scope",
    "href": "evaluation/scope.html#outside-scope",
    "title": "Scope",
    "section": "Outside scope",
    "text": "Outside scope\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nDiagram of patient flow through the model.\n\n\n\nFIGURE 1 | A schematic diagram of our discrete event model of an ECR service from Emergency to angiography suite. CT, Computed Tomography; AIS, Acute Ischemic Stroke; LVO, Large Vessel Occlusion; ECR, Endovascular Clot Retrieval; IR, Interventional Radiology; INR, Interventional Neuroradiology. Huang et al. (2019)\n\n\n\n\n\n\n\n\n\n\n\nTable 1\n\n\n\n\n\nParameters for the model.\n\n\n\nTABLE 1 | DES model inputs. (A) Human and physical resources. (B) Patient statistics. Huang et al. (2019)\n\n\n\n\n\n\n\n\nFIGURE 2 | Patient wait time under various simulation scenarios (A). Baseline scenario simulated using inputs from Table 1 (B). Exclusive-use scenario: IR patients can only utilize angioIR (C). Two angioINRs scenario: 2 angioINRs, no angioIRs. Standardized density of patients in queue: the probability density of patients who are waiting standardized to patients who are not waiting. Huang et al. (2019)\nFIGURE 3 | The effect of increasing working hours on ECR patient wait time at angioINR (A). Baseline scenario (B). Exclusive-use scenario (C). Two angioINRs scenario. Standardized density of patients in queue: the probability density of patients who are waiting standardized to patients who are not waiting. Huang et al. (2019)\nFIGURE 4 | Disability-free life gained under various scenarios. Huang et al. (2019)\nFIGURE 5 | A comparison of the utilization of angioINR by ECR patients under various scenarios. Huang et al. (2019)\nSupplementary Figure | Increasing ECR patient volume on service bottleneck. Standardized density of patients in queue: the probability density of patients who are waiting standardized to patients who are not waiting. (A) Baseline scenario. (B) Doubling ECR patients in baseline scenario. (C) Tripping ECR patients in baseline scenario. Huang et al. (2019)\nFIGURE 1 | A schematic diagram of our discrete event model of an ECR service from Emergency to angiography suite. CT, Computed Tomography; AIS, Acute Ischemic Stroke; LVO, Large Vessel Occlusion; ECR, Endovascular Clot Retrieval; IR, Interventional Radiology; INR, Interventional Neuroradiology. Huang et al. (2019)\nTABLE 1 | DES model inputs. (A) Human and physical resources. (B) Patient statistics. Huang et al. (2019)"
  },
  {
    "objectID": "evaluation/reproduction_report.html",
    "href": "evaluation/reproduction_report.html",
    "title": "Summary report",
    "section": "",
    "text": "Huang S, Maingard J, Kok HK, Barras CD, Thijs V, Chandra RV, Brooks DM and Asadi H. Optimizing Resources for Endovascular Clot Retrieval for Acute Ischemic Stroke, a Discrete Event Simulation. Frontiers in Neurology 10, 653 (2019). https://doi.org/10.3389/fneur.2019.00653.\n\nThis is a discrete-event simulation model of an endovascular clot retrieval (ECR) service. ECR is a treatment for acute ischaemic stroke. The model includes the stroke pathway, as well as three other pathways that share resources with the stroke pathway: an elective non-stroke interventional neuroradiology pathway, an emergency interventional radiology pathway, and an elective interventional radiology pathway. The model is created using R Simmer. The paper explores waiting times and resource utilisation - particularly focussing on the biplane angiographic suite (angioINR). A few scenarios are tried to help examine why the wait times are so high for the angioINR. Images from the original study on this page are sourced from Huang et al. (2019)."
  },
  {
    "objectID": "evaluation/reproduction_report.html#study",
    "href": "evaluation/reproduction_report.html#study",
    "title": "Summary report",
    "section": "",
    "text": "Huang S, Maingard J, Kok HK, Barras CD, Thijs V, Chandra RV, Brooks DM and Asadi H. Optimizing Resources for Endovascular Clot Retrieval for Acute Ischemic Stroke, a Discrete Event Simulation. Frontiers in Neurology 10, 653 (2019). https://doi.org/10.3389/fneur.2019.00653.\n\nThis is a discrete-event simulation model of an endovascular clot retrieval (ECR) service. ECR is a treatment for acute ischaemic stroke. The model includes the stroke pathway, as well as three other pathways that share resources with the stroke pathway: an elective non-stroke interventional neuroradiology pathway, an emergency interventional radiology pathway, and an elective interventional radiology pathway. The model is created using R Simmer. The paper explores waiting times and resource utilisation - particularly focussing on the biplane angiographic suite (angioINR). A few scenarios are tried to help examine why the wait times are so high for the angioINR. Images from the original study on this page are sourced from Huang et al. (2019)."
  },
  {
    "objectID": "evaluation/reproduction_report.html#computational-reproducibility",
    "href": "evaluation/reproduction_report.html#computational-reproducibility",
    "title": "Summary report",
    "section": "Computational reproducibility",
    "text": "Computational reproducibility\nSuccessfully reproduced 37.5% (3 out of 8) of items from the scope in 24h 10m (60.4%).\nRequired troubleshooting:\n\nEnvironment - identifying and installing required packages (spent some time trying to use estimated versions, but ended up using latest)\nExtract model code - was set within code for shiny app\nGet model parameters - parameters differed between code and paper\nWriting code to implement and run scenarios\nWriting code to process results and produce figures - which took a bit of time, since there were several transformations that were complicated or not mentioned\nSeeds - introduced as results varied a fair amount between runs\n\n\nFigure 2Figure 3Figure 4Figure 5Supplementary figureIn-text result 1In-text result 2In-text result 3\n\n\nConsensus: Not reproduced\n \n\n\nConsensus: Not reproduced\n \n\n\nConsensus: Not reproduced\n \n\n\nConsensus: Successfully reproduced\n \n\n\nConsensus: Not reproduced\n \n\n\nConsensus: Successfully reproduced\n‚ÄúExclusive-Use Scenario. In this scenario, the overall wait time probability at angioINR was reduced compared to baseline (red line in Figure 2B compared to Figure 2A). This represents a decrease in ECR patient wait time for angioINR by an average of 6 min.‚Äù Huang et al. (2019)\nReproduction:\n\n\n\n\n\n\n\n\n\n\nscenario\nmean\ndiff_from_baseline\n\n\n\n\n0\nBaseline\n13.958269\n0.00\n\n\n1\nExclusive use\n8.117729\n-5.84\n\n\n\n\n\n\n\n\n\n\nConsensus: Successfully reproduced\n‚ÄúTwo angioINRs Scenario. This scenario simulates the effect a facility upgrade to two biplane angiographic suites, but without additional staff changes. The wait time probability at angioINR was reduced compared to baseline (Figure 2C). The reduction represents an average of 4 min less in queue for angioINR.‚Äù Huang et al. (2019)\nReproduction:\n\n\n\n\n\n\n\n\n\n\nscenario\nmean\ndiff_from_baseline\n\n\n\n\n0\nBaseline\n13.958269\n0.00\n\n\n2\nTwo AngioINRs\n9.621122\n-4.34\n\n\n\n\n\n\n\n\n\n\nConsensus: Not reproduced\n‚ÄúExtended Schedule Scenario. The wait time probability at angioINR in the exclusive- use scenario was further reduced by extended work hours (Figure 3B). In contrast, work extension did not affect baseline or the 2 angioINRs scenario (Figures 3A,C). For the baseline scenario, 1 and 2 h of extra work resulted in an average wait time of 1.7 and 0.9 min reduction, respectively. For the 2 angioINRs scenario, 1 and 2 h of extra work resulted in an average wait time gain of 1 and 0.3 min, respectively.‚Äù Huang et al. (2019)\nReproduction:\n\n\n\n\n\n\n\n\n\n\nscenario\nshift\nmean\ndiff_from_5pm\n\n\n\n\n0\nBaseline\n5pm\n13.958269\n0.00\n\n\n1\nBaseline\n6pm\n12.486042\n-1.47\n\n\n2\nBaseline\n7pm\n12.491421\n-1.47\n\n\n6\nTwo AngioINRs\n5pm\n9.621122\n0.00\n\n\n7\nTwo AngioINRs\n6pm\n9.216435\n-0.40\n\n\n8\nTwo AngioINRs\n7pm\n8.699223\n-0.92"
  },
  {
    "objectID": "evaluation/reproduction_report.html#evaluation-against-guidelines",
    "href": "evaluation/reproduction_report.html#evaluation-against-guidelines",
    "title": "Summary report",
    "section": "Evaluation against guidelines",
    "text": "Evaluation against guidelines\n\n\n                                                \n\n\nContext: The original study repository was evaluated against criteria from journal badges relating to how open and reproducible the model is and against guidance for sharing artefacts from the STARS framework. The original study article and supplementary materials (excluding code) were evaluated against reporting guidelines for DES models: STRESS-DES, and guidelines adapted from ISPOR-SDM."
  },
  {
    "objectID": "logbook/posts/2024_07_11/index.html",
    "href": "logbook/posts/2024_07_11/index.html",
    "title": "Day 7",
    "section": "",
    "text": "Note\n\n\n\nReproduced Figure 5 and working on Figures 2 and 4. Total time used: 23h 29m (58.7%)."
  },
  {
    "objectID": "logbook/posts/2024_07_11/index.html#running-model-with-30-replications-and-various-seeds",
    "href": "logbook/posts/2024_07_11/index.html#running-model-with-30-replications-and-various-seeds",
    "title": "Day 7",
    "section": "10.45-11.00, 11.35-11.45: Running model with 30 replications and various seeds",
    "text": "10.45-11.00, 11.35-11.45: Running model with 30 replications and various seeds\nIntermittently (condensed to 15 minutes, but was over a longer time) ran the baseline model with several different seeds, but 30 replications. However, we can see this has a fairly minimal impact on the results, with a fairly low peak observed for the AngioINR waiting times in all.\n\n\n\nFigure 2A with different seeds"
  },
  {
    "objectID": "logbook/posts/2024_07_11/index.html#working-on-figure-2b",
    "href": "logbook/posts/2024_07_11/index.html#working-on-figure-2b",
    "title": "Day 7",
    "section": "12.00-12.17: Working on Figure 2B",
    "text": "12.00-12.17: Working on Figure 2B\nFigures 2A and 2C have lower angioINR wait times but are otherwise fairly similar to the scope. However, Figure 2B is very different. It should have higher angioINR and angio staff wait times.\nI tried a few variants to see how they impacted results, although none got us what we are hoping for:\n\nChanging the number of angioINR machines\nExcluding emergency IR patients\nDoubling the number of ED arrivals\nReducing the number of angio staff (did bring up the curves, but too high, and as they are normally requested as 3 at a time, as soon as go to 6, the wait times drop again)\nReducing the number of ED staff\nIncreasing the INR angioINR appointment length to mean 120 SD 60 (from mean 60 SD 30)\nForce emergency IR to only use angioINR (and not angioIR)"
  },
  {
    "objectID": "logbook/posts/2024_07_11/index.html#evaluating-in-text-result-3",
    "href": "logbook/posts/2024_07_11/index.html#evaluating-in-text-result-3",
    "title": "Day 7",
    "section": "12.20-12.24, 13.13-13.29, 13.39-13.44: Evaluating in-text result 3",
    "text": "12.20-12.24, 13.13-13.29, 13.39-13.44: Evaluating in-text result 3\nGiven that in-text result 2 now matches up due to the fix to the double angioINR scenario, I also checked the results for in-text result 3. These complement figure 3, but provide the reduced wait times for some of the scenarios (excludes exclusive use). These are changes to wait time in min.\nThe fix to the model has definitely improved the two angioINR results (previously -2.1 and -2.2) but still are both dissimilar to the paper.\nI have previously tried running different seeds for baseline, which caused alot of fluctation in results (and although not similar to the original, I do think it could be possible to get closer if I just tried some more different seeds!). I tried also running the seed variants for the angioINR scenarios, to see what sort of fluctuation we get in those results - as if that looked to get us slightly closer, then I‚Äôd be convinced to try some more seeds for both of them (takes a little while to run, so was only planning to do if looked hopeful).\nHowever, they do all still look rather different, with nothing very similar to the paper. Hence, feel there might be another underlying cause for the differences (beyond seeds) (particularly as haven‚Äôt got related figure to match).\n\n\n\n\n\n\n\n\n\n\nScenario\nPaper reduction\nMy reduction (seed 200)\n(seed 500)\n(seed 700)\n\n\n\n\nBaseline 1h extra\n-1.7\n-1.47\n-1.8\n-2.2\n\n\nBaseline 2h extra\n-0.9\n-1.47\n-1.8\n-3.2\n\n\nAngioINR 1h extra\n+1\n-0.4\n-0.57\n-0.43\n\n\nAngioINR 2h extra\n+0.3\n-0¬∑92\n-1.1\n-0.76"
  },
  {
    "objectID": "logbook/posts/2024_07_11/index.html#figure-4",
    "href": "logbook/posts/2024_07_11/index.html#figure-4",
    "title": "Day 7",
    "section": "13.45-13.54, 14.00-14.21: Figure 4",
    "text": "13.45-13.54, 14.00-14.21: Figure 4\nThis figure uses the reduction in patient wait times (from baseline) from three scenarios, multiplying the minutes saved by 4.2. With a seed 200 and so average wait for baseline of 14 minutes‚Ä¶\n\n\n\n\n\n\n\n\n\nScenario\nWait time\nReduction from baseline (14 min)\nDisability-life years added\n\n\n\n\nExclusive use\n8.12\n- 5.84\n24.528\n\n\nTwo angioINRs\n9.62\n-4.34\n18.23\n\n\nExclusive use + 1 hour\n7.80\n-6.20\n26.04\n\n\n\nAlthough the exclusive use results look a bit lower than the figure, the pattern is similar and it‚Äôs somewhat close, so I worked to create a plot of this within reproduction.Rmd.\nLooking at the plot, I think the results are reasonably similar (middle one definite!), but the right bar may just be a bit too different (-5 ish).\n\n\n\nFigure 4"
  },
  {
    "objectID": "logbook/posts/2024_07_11/index.html#figure-5",
    "href": "logbook/posts/2024_07_11/index.html#figure-5",
    "title": "Day 7",
    "section": "15.32-16.31, 16.35-17.00: Figure 5",
    "text": "15.32-16.31, 16.35-17.00: Figure 5\nDon‚Äôt save full model results due to file size, so had to re-run the three basic model scenarios, and then combined and saved the utilisation from angioINR.\nI realised that the plot looks very similar to those from the simmer documentation, so followed those guidelines to produce it (and see code for that function plot.resources.utilization at this link). However, ordinary import using data.table creates a data.table data.frame, whilst the original from the model is a resources data.frame. This appears to be a custom simmer class, and the plot function does not work otherwise. As I couldn‚Äôt figure a way to resolve this, I set it to run the model for this function only, for now, since the original simmer.plot code is not under an open license.\nI think the result is very similar to the original - the only difference is that baseline has 24% utilisation instead of 26% - but I feel that is close enough to mark this as successfully reproduced."
  },
  {
    "objectID": "logbook/posts/2024_07_11/index.html#timings",
    "href": "logbook/posts/2024_07_11/index.html#timings",
    "title": "Day 7",
    "section": "Timings",
    "text": "Timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 1228\n\n# Times from today\ntimes = [\n    ('10.45', '11.00'),\n    ('11.35', '11.45'),\n    ('12.00', '12.17'),\n    ('12.20', '12.24'),\n    ('13.13', '13.29'),\n    ('13.39', '13.44'),\n    ('13.45', '13.54'),\n    ('14.00', '14.21'),\n    ('15.32', '16.31'),\n    ('16.35', '17.00')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 181m, or 3h 1m\nTotal used to date: 1409m, or 23h 29m\nTime remaining: 991m, or 16h 31m\nUsed 58.7% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_08_12/index.html",
    "href": "logbook/posts/2024_08_12/index.html",
    "title": "Day 14",
    "section": "",
    "text": "Sent emails on 12th July and 26th July. There was no requirement for responses to these and, as per protocol, since there were no responses within 4 weeks, we will consider the reproduction stage complete (with no further work intended on troubleshooting the remaining discrepancies)."
  },
  {
    "objectID": "logbook/posts/2024_08_12/index.html#untimed-emails-to-author",
    "href": "logbook/posts/2024_08_12/index.html#untimed-emails-to-author",
    "title": "Day 14",
    "section": "",
    "text": "Sent emails on 12th July and 26th July. There was no requirement for responses to these and, as per protocol, since there were no responses within 4 weeks, we will consider the reproduction stage complete (with no further work intended on troubleshooting the remaining discrepancies)."
  },
  {
    "objectID": "logbook/posts/2024_07_04/index.html",
    "href": "logbook/posts/2024_07_04/index.html",
    "title": "Day 2",
    "section": "",
    "text": "Note\n\n\n\nDefined scope and problem-solving renv. Total time used: 2h 29m (6.2%)"
  },
  {
    "objectID": "logbook/posts/2024_07_04/index.html#untimed-set-up-rstudio-and-test-quarto-site-with-r",
    "href": "logbook/posts/2024_07_04/index.html#untimed-set-up-rstudio-and-test-quarto-site-with-r",
    "title": "Day 2",
    "section": "Untimed: Set up RStudio and test quarto site with R",
    "text": "Untimed: Set up RStudio and test quarto site with R\nI did not time this as it is not specific to this reproduction, but additional set-up as not done reproduction in R yet (since the test-run was conducted in Python).\nThis involved installing/updating RStudio, learning how to run and work with a quarto book on that platform, and and troubleshooting any issues in getting the quarto book up and running.\n\nEnvironment\n\nUpdating to the latest version of RStudio, as suggested in the Quarto docs\nInstalling renv: install.packages(\"renv\")\nSetting the working directory: setwd(\"~/Documents/stars/stars-reproduce-huang-2019\")\nInitialised an empty R environment: renv::init(bare=TRUE)\nSet renv to use explicit dependencies: renv::settings$snapshot.type(\"explicit\")\nCreated a DESCRIPTION file\nRan renv::snapshot() which returned that project is not activated yet, so I selected option to Activate the project and use the project library. This generated an .Rprofile file.\nI then tried to open the project (File &gt; Open Project) but this failed. So I tried File &gt; New Project &gt; Existing Directory (which created an .Rproj file), then reran renv::init(bare=TRUE), then renv::snapshot(), and selected to install packages and then snapshot.\nSynced with GitHub (excluding .Rhistory, which is just a history of executed commands), using Git panel in top right corner\nAdd rmarkdown to DESCRIPTION and rebuilt environment (via renv::snapshot() and selecting to install)\n\nThen came across pkgr, and decided to give that a go, following their tutorial‚Ä¶\n\nDeleted renv and associated files (.Rprofile and renv.lock) with renv::deactivate(clean=TRUE)\nInstalled pkgr following the instructions on their latest release:\n\nsudo wget https://github.com/metrumresearchgroup/pkgr/releases/download/v3.1.1/pkgr_3.1.1_linux_amd64.tar.gz -O /tmp/pkgr.tar.gz\nsudo tar xzf /tmp/pkgr.tar.gz pkgr\nsudo mv pkgr /usr/local/bin/pkgr\nsudo chmod +x /usr/local/bin/pkgr\n\nCreated a pkgr.yml file\n\n# Version of pkgr.yml and, at this point, should always say Version: 1\nVersion: 1\n\n# pkgr will pull dependencies listed in DESCRIPTION\nDescriptions:\n- DESCRIPTION\n\n# If DESCRIPTION is provided, then this section only needs to include packages\n# that you would like to use for development purposes that are not in your\n# DESCRIPTION file (i.e. not formal dependencies of your package) - e.g. devtools\n# Packages:\n\n# Specify where to pull packages from\n# If list CRAN and MPN, will look on CRAN first, then MPN (which is useful for\n# dependencies no on CRAN). Can list a location for specific packages in Packages:\nRepos:\n  - CRAN: https://cran.rstudio.com\n  - MPN: https://mpn.metworx.com/snapshots/stable/2022-02-11 # used for mrgval\n\n# Specify Lockfile or Library to tell pkgr where to install packages\n# We are using renv to isolate our package environment - renv will tell pkgr where to install them\nLockfile:\n  Type: renv\n\nIn terminal, ran pkgr plan, but get error ARN[0000] error getting library path from renv: Error in loadNamespace(x) : there is no package called ‚Äòrenv‚Äô\n\nIf I start a new R session and run packageVersion(\"renv\"), it returns that it is installed\nTrying to reinstall with install.packages(\"renv\") makes no difference.\nTried restarting R and opening a new terminal\n\n\nI looked through issues and couldn‚Äôt spot anything, and then realised this was a fairly small package which hadn‚Äôt had any changes in half a year, so on reflection, probably not a reliable option to choose. So went back to set up similar to before of:\n\nrenv::init(bare=TRUE) with explicit snapshot\nrenv::snapshot() (and realised it didn‚Äôt update with change to DESCRIPTION before simply because I hadn‚Äôt put a comma after each package!)\n\nTo render the Quarto book (in a similar to way to how we did in VSCode), just click the Render button.\nNow, returning to what started this - trying to get the .TIFF supplementary file to display‚Ä¶\n\nAdd tiff to DESCRIPTION\nrenv::status() showed that the package was used but not installed, and renv::snapshot() with option 2 installed the package\n\n\n\nUsing specific versions\n\nAdd explict versions of R and packages to DESCRIPTION\nAttempted to downgrade tiff. renv::status() and renv::snapshot() did not noticed. From this issue, it appears that this should work for renv::install() and, indeed, that recognises it although get issue:\n\nWarning: failed to find source for 'tiff 0.1.11' in package repositories\nError: failed to retrieve package 'tiff@0.1.11'\n\nI checked the archive for tiff on CRAN and found there is a 0.1-11 (prior to the current 0.1-12)\nIf I deleted it (remove.packages(\"tiff\")) and then redid renv::snapshot(), it again would not notice the versions\nI tried to do it manually with remotes (rather than devtools as devtools has so many dependencies) - I installed remotes and then ran remotes::install_version(\"tiff\", \"0.1.11\"). This seemed successful, except packageVersion(\"tiff\") still returned 0.1.12? Although actually, on inspection, you can see it if 0.1.11. However, it wasn‚Äôt able to do that from DESCRIPTION.\nI removed it and tried again with a direct renv::install(\"tiff@0.1-11\") which was successful\nI then tried again with DESCRIPTION, but instead set it to tiff@0.1-11, which was successful likewise! And if it was tiff (==0.1-11)! So it appears its a bit fussy about matching up to the format in the CRAN archive .tar.gz files.\nI then found that renv::snapshot() ignores the version if it‚Äôs tiff (==0.1-11) but adheres if it is tiff@0.1-11 - yay!\n\nHaving finished with this experiment, I deleted and rebuilt with latest versions - but found it had errors installing them where defined like tiff@0.1-12. Hence, returned to tiff (==0.1-11), and just had to make sure to do renv::install() before renv::snapshot() (rather than rely on snapshot to install the packages).\n\n\nFixing GitHub action to render and publish the book\nWith no changes to GitHub action, had an error of:\n[14/18] quarto_site/study_publication.qmd\nError in file(filename, \"r\", encoding = encoding) : \n  cannot open the connection\nCalls: source -&gt; file\nIn addition: Warning message:\nIn file(filename, \"r\", encoding = encoding) :\n  cannot open file 'renv/activate.R': No such file or directory\nExecution halted\nError in file(filename, \"r\", encoding = encoding) : \n  cannot open the connection\nCalls: source -&gt; file\nIn addition: Warning message:\nIn file(filename, \"r\", encoding = encoding) :\n  cannot open file 'renv/activate.R': No such file or directory\nExecution halted\nProblem with running R found at /usr/bin/Rscript to check environment configurations.\nPlease check your installation of R.\n\nERROR: Error\n    at renderFiles (file:///opt/quarto/bin/quarto.js:78079:29)\n    at eventLoopTick (ext:core/01_core.js:153:7)\n    at async renderProject (file:///opt/quarto/bin/quarto.js:78477:25)\n    at async renderForPublish (file:///opt/quarto/bin/quarto.js:109332:33)\n    at async renderForPublish (file:///opt/quarto/bin/quarto.js:104864:24)\n    at async Object.publish1 [as publish] (file:///opt/quarto/bin/quarto.js:105349:26)\n    at async publishSite (file:///opt/quarto/bin/quarto.js:109369:38)\n    at async publish7 (file:///opt/quarto/bin/quarto.js:109588:61)\n    at async doPublish (file:///opt/quarto/bin/quarto.js:109548:13)\n    at async publishAction (file:///opt/quarto/bin/quarto.js:109559:9)\nError: Process completed with exit code 1\nAttempting to solve this‚Ä¶\n\nAdd installation of R and set up of R environment with actions from r-lib (trying setup-renv and setup-r-dependencies) for environment. However, it fails for installation of R dependencies with the error message:\n\nRun r-lib/actions/setup-r-dependencies@v2\nRun # Set site library path\nError in file(filename, \"r\", encoding = encoding) : \n  cannot open the connection\nCalls: source -&gt; file\nIn addition: Warning message:\nIn file(filename, \"r\", encoding = encoding) :\n  cannot open file 'renv/activate.R': No such file or directory\nExecution halted\nError: Process completed with exit code 1.\n\nBased on this forum post, I tried removing the .Rprofile from git\nThis seemed to improve slightly, although setup-r-dependencies then failed with an error in a pak subprocess seemingly for a package called ‚Äú.‚Äù. Tried switching to setup-renv (which bases on renv.lock) which was then successful! (although takes 4 minutes to install R dependencies, so 6m 55s total)"
  },
  {
    "objectID": "logbook/posts/2024_07_04/index.html#reading-the-article",
    "href": "logbook/posts/2024_07_04/index.html#reading-the-article",
    "title": "Day 2",
    "section": "14.14-14.31: Reading the article",
    "text": "14.14-14.31: Reading the article\nRead throughout and highlighted a copy of the article."
  },
  {
    "objectID": "logbook/posts/2024_07_04/index.html#define-scope-of-article",
    "href": "logbook/posts/2024_07_04/index.html#define-scope-of-article",
    "title": "Day 2",
    "section": "14.33-14.50: Define scope of article",
    "text": "14.33-14.50: Define scope of article\nWent through figures and tables to define scope (and convert and crop the .TIFF supplementary to .JPG so easier to display). From looking through text of article, identified a few extra results not in the figures: the quoted decrease in wait times. Although these are very related to the figures, as it wouldn‚Äôt be able to look at the figure and deduce the average wait time reduction, these represent additional results.\nThere was one line in the discussion that caught my attention - ‚ÄúThe quality of the ECR service appears to be robust to important parameters, such as the number of radiologists‚Äù - but I feel the interpretation of this is quite ambiguous (as to whether it is a model result or interpretation from other results), and doesn‚Äôt have anything specific to action, so will not include in scope."
  },
  {
    "objectID": "logbook/posts/2024_07_04/index.html#consensus-on-scope-with-tom",
    "href": "logbook/posts/2024_07_04/index.html#consensus-on-scope-with-tom",
    "title": "Day 2",
    "section": "15.05-15.10: Consensus on scope with Tom",
    "text": "15.05-15.10: Consensus on scope with Tom\nDiscussed with Tom (and he also had another look over afterwards). Happy with scope choices, and agree that the line from the discussion is simply too ambiguous to action."
  },
  {
    "objectID": "logbook/posts/2024_07_04/index.html#exploring-app-and-simulation-visualisation",
    "href": "logbook/posts/2024_07_04/index.html#exploring-app-and-simulation-visualisation",
    "title": "Day 2",
    "section": "15.35-15.43: Exploring app and simulation visualisation",
    "text": "15.35-15.43: Exploring app and simulation visualisation\nAs an addendum to the reading, explored the app and linked simulation configuration visualisation.\nFor the configuration, it just opened to the CLOUDES homepage, so I tried creating an account then going to the link (turns out you need an account to access). The link still did not work nor the ID, but when I search for ‚ÄúHuang‚Äù, I was able to find a diagram: https://beta.cloudes.me/loadSim?simId=17482&pageId=rTbqE (ID 17482). When run, this played through the simulation showing arrivals and queues etc."
  },
  {
    "objectID": "logbook/posts/2024_07_04/index.html#prepare-release",
    "href": "logbook/posts/2024_07_04/index.html#prepare-release",
    "title": "Day 2",
    "section": "15.44-15.47: Prepare release",
    "text": "15.44-15.47: Prepare release\nModified CHANGELOG and CITATION ahead of release."
  },
  {
    "objectID": "logbook/posts/2024_07_04/index.html#archived-on-zenodo",
    "href": "logbook/posts/2024_07_04/index.html#archived-on-zenodo",
    "title": "Day 2",
    "section": "15.55-15.58: Archived on Zenodo",
    "text": "15.55-15.58: Archived on Zenodo\nCreated GitHub release with archiving activated on Zenodo."
  },
  {
    "objectID": "logbook/posts/2024_07_04/index.html#look-over-code-and-set-up-environment",
    "href": "logbook/posts/2024_07_04/index.html#look-over-code-and-set-up-environment",
    "title": "Day 2",
    "section": "16.04-16.58: Look over code and set up environment",
    "text": "16.04-16.58: Look over code and set up environment\nNo dependency management, so will create renv based on the imports and the dates of the repository - with exception that article mentions:\n\nSimmer (version 4.1.0)\n\nThe article dates are:\n\nReceived - 31 March 2019\nAccepted - 4 June 2019\nPublished - 27 June 2019\n\nThe GitHub repository has two commits, both on 27 May 2019. As per protocol, will go with earliest of published and code, which is 27 May 2019.\nIt looks likely that all the relevant code will be in server.R (with ui.R just being for the ShinyApp, which is not in scope to reproduce, as it is not presented as a key result within the paper). As such, looking at the imports from that R script, and identifying versions on or prior to 27 May 2019‚Ä¶\n\nsimmer - https://cran.r-project.org/src/contrib/Archive/simmer/ - 4.2.2 (14 March 2019)\nsimmer.plot - https://cran.r-project.org/src/contrib/Archive/simmer.plot/ - 0.1.15 (10th March 2019)\nparallel - part of the core R distribution (so will come with version of R used)\ndplyr - https://cran.r-project.org/src/contrib/Archive/dplyr/ - 0.8.1 (14th May 2019)\nplotly - https://cran.r-project.org/src/contrib/Archive/plotly/ - 4.9.0 (10th April 2019)\ngridExtra - https://cran.r-project.org/src/contrib/Archive/gridExtra/ - 2.2.1 (29th February 2016, latest release)\nR - https://github.com/r-hub/rversions - 3.6.0 Planting of a Tree (26th April 2019)\n\nI‚Äôll set each of these to be max these versions, to help with dependency conflicts when set-up environment, but then convert to fixed versions once know what worked.\nCreated a DESCRIPTION file in reproduction/:\nTitle: huang2019\nDepends: \n    R (&lt;= 3.6)\nImports:\n    simmer (&lt;=4.2.2),\n    simmer.plot (&lt;=0.1.15),\n    dplyr (&lt;=0.8.1),\n    plotly (&lt;=4.9.0),\n    gridExtra (&lt;=2.2.1)\nWant to create another renv for that sub-folder (seperate to the renv in our main folder). To do so I ran the following commands in the console:\n\nsetwd(\"~/Documents/stars/stars-reproduce-huang-2019/reproduction\") (to move to reproduction/)\nrenv::deactivate()\nrenv::status() to confirm none were active\nrenv::init(bare=TRUE) and selected 1 for using the explicit dependencies from DESCRIPTION. This then restarted the R session and created and opened a new project: reproduction. It made the following new files and folders:\n\n\n.Rprofile (with just source(\"renv/activate.R\"))\nreproduction.Rproj\nrenv/ with the environment\n\n\nrenv::install() to install the packages and their specified versions. However, looking over the versions it planned to install, we had:\n\n\nsimmer [4.4.6.3]\nsimmer.plot [0.1.18]\ndplyr [1.1.4]\nplotly [4.10.4]\ngridExtra [2.3]\n\nI cancelled it and tried changing everything to explicit versions (==). This then matched up to what I wanted in the planned installs -\n\nsimmer [4.2.2]\nsimmer.plot [0.1.15]\ndplyr [1.1.4]\nplotly [4.9.0]\ngridExtra [2.2.1]\n\nHowever, there was an error with simmer: ERROR: compilation failed for package ‚Äòsimmer‚Äô, and so still just have renv in environment. I tried installing this specific version manually with remotes:\n\nrenv::install(\"remotes\")\nremotes::install_version(\"simmer\", \"4.2.2\")\n\nUnfortunately, the same error appeared. I then tried installing from GitHub instead of CRAN:\n\nremotes::install_github(\"r-simmer/simmer@v4.2.2\")\n\nBut this failed again as before.\nI tried focusing just on R to begin with, as I realised I have to install and change that manually. I followed this tutorial and ran in terminal:\n\nsudo snap install curl\nsudo apt-get update\nsudo apt-get install gdebi-core\nexport R_VERSION=3.6\ncurl -O https://cdn.rstudio.com/r/ubuntu-2204/pkgs/r-${R_VERSION}_1_amd64.deb\nsudo gdebi r-${R_VERSION}_1_amd64.deb\n\nHowever, I then got an error: Failed to open the software package. The package might be corrupted or you are not allowed to open the file. Check the permissions of the file.\nI switched over to the R documentation and clicked on Ubuntu and then ‚ÄúFor older R releases, see the corresponding README.‚Äù This said:\nTo obtain the latest R 3.6 packages, use:\n\ndeb https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/\nor\n\ndeb https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/\nor\n\ndeb https://cloud.r-project.org/bin/linux/ubuntu trusty-cran35/"
  },
  {
    "objectID": "logbook/posts/2024_07_04/index.html#timings",
    "href": "logbook/posts/2024_07_04/index.html#timings",
    "title": "Day 2",
    "section": "Timings",
    "text": "Timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 45\n\n# Times from today\ntimes = [\n    ('14.14', '14.31'),\n    ('14.33', '14.50'),\n    ('15.05', '15.10'),\n    ('15.35', '15.43'),\n    ('15.55', '15.58'),\n    ('16.04', '16.58')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 104m, or 1h 44m\nTotal used to date: 149m, or 2h 29m\nTime remaining: 2251m, or 37h 31m\nUsed 6.2% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_08_15/index.html",
    "href": "logbook/posts/2024_08_15/index.html",
    "title": "Day 15",
    "section": "",
    "text": "Revisiting the evaluation from each study, I spotted a change to make to the evaluation against STRESS-DES.\nCriteria: ‚Äú5.3 Model execution State the event processing mechanism used e.g.¬†three phase, event, activity, process interaction. Note that in some commercial software the event processing mechanism may not be published. In these cases authors should adhere to item 5.1 software recommendations. State all priority rules included if entities/activities compete for resources. If the model is parallel, distributed and/or use grid or cloud computing, etc., state and preferably reference the technology used. For parallel and distributed simulations the time management algorithms used. If the HLA is used then state the version of the standard, which run-time infrastructure (and version), and any supporting documents (FOMs, etc.)‚Äù Monks et al. (2019)\nOriginal decision: ‚ùå Not met\nNew decision: üü° Partially. Does not state event processing mechanism. Does describe priority rules - Methods: Model Properties: Queueing - e.g.¬†‚Äún our model, emergency IR and stroke patients have higher priority than elective patients for resources. Specifically, angioINRs are capable of both INR and IR procedures, although all patient types‚Ä¶‚Äù"
  },
  {
    "objectID": "logbook/posts/2024_08_15/index.html#untimed-amendment-to-evaluation",
    "href": "logbook/posts/2024_08_15/index.html#untimed-amendment-to-evaluation",
    "title": "Day 15",
    "section": "",
    "text": "Revisiting the evaluation from each study, I spotted a change to make to the evaluation against STRESS-DES.\nCriteria: ‚Äú5.3 Model execution State the event processing mechanism used e.g.¬†three phase, event, activity, process interaction. Note that in some commercial software the event processing mechanism may not be published. In these cases authors should adhere to item 5.1 software recommendations. State all priority rules included if entities/activities compete for resources. If the model is parallel, distributed and/or use grid or cloud computing, etc., state and preferably reference the technology used. For parallel and distributed simulations the time management algorithms used. If the HLA is used then state the version of the standard, which run-time infrastructure (and version), and any supporting documents (FOMs, etc.)‚Äù Monks et al. (2019)\nOriginal decision: ‚ùå Not met\nNew decision: üü° Partially. Does not state event processing mechanism. Does describe priority rules - Methods: Model Properties: Queueing - e.g.¬†‚Äún our model, emergency IR and stroke patients have higher priority than elective patients for resources. Specifically, angioINRs are capable of both INR and IR procedures, although all patient types‚Ä¶‚Äù"
  },
  {
    "objectID": "logbook/posts/2024_07_18/index.html",
    "href": "logbook/posts/2024_07_18/index.html",
    "title": "Day 11",
    "section": "",
    "text": "Note\n\n\n\nWorking on research compendium: Finishing up tests, and lots of troubleshooting docker."
  },
  {
    "objectID": "logbook/posts/2024_07_18/index.html#untimed-research-compendium",
    "href": "logbook/posts/2024_07_18/index.html#untimed-research-compendium",
    "title": "Day 11",
    "section": "Untimed: Research compendium",
    "text": "Untimed: Research compendium\n\nTests\nHaving re-ran all the scenarios from scratch, I replaced the files in tests/testthat/expected_results/ and then ran testthat::test_dir(\"tests/testthat\").\nis_true(compare) returned error Error inis_true(compare): unused argument (compare) so switched back to expect_equal().\nHowever, these were then all successful! Included instructions to run these tests, run time, and what you might expect to see, to the reproduction README.\n\n\nDocker\n\nTroubleshooting installation of packages when building images\nRan sudo docker build --tag huang2019 . -f ./docker/Dockerfile from reproduction/ (which is where the renv is located). Hit an error:\n15.45 Warning: failed to find source for 'Matrix 1.7-0' in package repositories\n15.45 Warning: error downloading 'https://cloud.r-project.org/src/contrib/Archive/Matrix/Matrix_1.7-0.tar.gz' [cannot open URL 'https://cloud.r-project.org/src/contrib/Archive/Matrix/Matrix_1.7-0.tar.gz']\n15.45 Error: failed to retrieve package 'Matrix@1.7-0'\n\n...\n\nERROR: failed to solve: process \"/bin/sh -c R -e \\\"renv::restore()\\\"\" did not complete successfully: exit code: 1\nI looked to the address, and found that 1.7-0 was indeed not in the Archive, but it is the latest version of the package. It is available at https://cran.r-project.org/src/contrib/Matrix_1.7-0.tar.gz or at https://cloud.r-project.org/src/contrib/Matrix_1.7-0.tar.gz. This was only the second package it tried to install - the first was MASS 7.3-60.2, and that wasn‚Äôt the latest version. Looking at other packages, it seems common that the latest version is not on CRAN archive.\nI tried out a bunch of things, but the same issue persisted throughout:\n\nI found a post with the same issue - that renv() only looks in CRAN archive in a Docker image. They suggested renv::restore(repos = c(CRAN = \"https://cloud.r-project.org\")).\n\nI changed the Dockerfie (but used single quotes for URL) and re-ran - RUN R -e \"renv::restore(repos = c(CRAN = 'https://cloud.r-project.org'))\"\nI tried with double quotes as above, but including \\ to escape the inner quotes - RUN R -e \"renv::restore(repos = c(CRAN = \\\"https://cloud.r-project.org\\\"))\"\n\nBased on some online posts, I wondered if this might be to do with system dependencies. Based on this post, I opened a fresh R session (so not in renv) and tried to install getsysreqs although it was not available for my version of R. The RStudio Package Manager (RSPM) was recommended. I also stumbled across containerit which can make a Dockerfile for you and would include the system dependencies. However, I decided first to try the simplest option, which is to just install a fairly standard list of some linux libraries that R packages need, like here.\nBased on this issue, I add ENV RENV_WATCHDOG_ENABLED FALSE to disable the renv watchdog.\n\nBased on Tom‚Äôs Dockerfile which is from Peter Solymos, I tried changing the CRAN source RUN R -e \"renv::restore(repos = c(CRAN = \\\"https://packagemanager.rstudio.com/all/__linux__/focal/latest\\\"))\". This resolved the issue, as it was able to download Matrix from CRAN. All packages successfully downloaded, but I then hit an issue installing the packages:\nERROR: this R is version 4.1.1, package 'MASS' requires R &gt;= 4.4.0\ninstall of package 'MASS' failed [error code 1]`.\nI then realised I had accidentally put R 4.1.1, when I meant to put R 4.4.1! I changed this and re-ran. This was successful until attempting to install igraph, at which it hit an error:\nError in dyn.load(file, DLLpath = DLLpath, ...) : \n  unable to load shared object '/home/code/renv/staging/2/igraph/libs/igraph.so':\n  libglpk.so.40: cannot open shared object file: No such file or directory\nI add libglpk-dev to the list of system dependencies to install then tried again. It did eventually failed again with another similar issue.\nError in dyn.load(file, DLLpath = DLLpath, ...) : \n  unable to load shared object '/home/code/renv/staging/2/stringi/libs/stringi.so':\n  libicui18n.so.66: cannot open shared object file: No such file or directory\nI briefly tried adding containerit to my renv to try that and see if it was simpler, although decided to pause on that and remove it and keep trying as before, as I kept getting errors and it wasn‚Äôt a quick-fix. I removed it from DESCRIPTION then ran renv::clean(), renv::snapshot().\nI add libicu-dev and tried again. This failed with the same error as before.\nLooking at the rocker rstudio image, it runs on ubunutu 22.04. Posit lists system dependencies for ubunutu 22.04 as apt install -y libcairo2-dev libssl-dev make libcurl4-openssl-dev libmysqlclient-dev unixodbc-dev libnode-dev default-jdk libxml2-dev git libfontconfig1-dev libfreetype6-dev libssh2-1-dev zlib1g-dev libglpk-dev libjpeg-dev imagemagick libmagick++-dev gsfonts cmake libpng-dev libtiff-dev python3 libglu1-mesa-dev libgl1-mesa-dev libgdal-dev gdal-bin libgeos-dev libproj-dev libsqlite3-dev libsodium-dev libicu-dev tcl tk tk-dev tk-table libfribidi-dev libharfbuzz-dev libudunits2-dev. I replaced the line in my Dockerfile and tried again. This failed with the same error as before (so returned it to the simpler list).\nI found this issue with the same error, where it appears there is an issue with the stringi binary being built for the wrong Ubunutu since libicui18n.so.66 is for 20.04, although the fix appeared to be that they fixed the bioconductor container, and it wasn‚Äôt super clear to me what I should do.\nBased on this example linked from this issue, I tried switching libicu-dev to libicu. However, this returned error Unable to locate package libicu. I then instead tried adding RUN R -e \"install.packages('stringi')\" before renv::restore(). That ran successfully, but hit a new error:\nError in dyn.load(file, DLLpath = DLLpath, ...) : \n  unable to load shared object '/home/code/renv/staging/2/openssl/libs/openssl.so':\n  libssl.so.1.1: cannot open shared object file: No such file or directory\nlibssl is in the list of system dependencies that were being installed. A quick google shows there are issues related to libssl.so.1.1 on Ubuntu 22.04. I tried doing the simplest solution first - installing it seperately (as that worked for stringi above).\nThe docker image then built successfully!\n\n\nTroubleshooting empty container\nHaving successfully built the image (sudo docker build --tag huang2019 . -f ./docker/Dockerfile), I then tried creating a container. After some trial and error with the command (and using sudo docker rm huang2019_docker to remove and recreate container)), I got to:\n(sleep 2 && xdg-open http://localhost:8888) & sudo docker run -it -p 8888:8787 -e DISABLE_AUTH=true --name  huang2019_docker huang2019\nThis opened up RStudio - although there were no files and none of the libraries I had added were listed in the packages (e.g.¬†no simmer)\nI spent quite a while searching for and trying suggestions on this issue, with little success. E.g.\n\nI tried running sudo docker inspect huang2019 but couldn‚Äôt spot anything amiss.\nBased on this post, I checked my .dockerignore, which has **/renv/, which should just be preventing upload of renv.\n\nI then tried copying everything into /home/rstudio rather than making a new directory in the Dockerfile and remaking the image. Got error This project does not contain a lockfile, so add path to renv::restore(). This built successfully, so I used the command above to create a container and open RStudio. This successfully included all the files!\n\n\nTroubleshooting renv in the container\nOn opening RStudio, the console now showed a long list of packages that are missing entries in the cache, saying These packages will need to be reinstalled. It then says Project '~/' loaded. [renv 1.0.7] and The following package(s) have broken symlinks into the cache, with the same list of packages again, and Use renv::repair() to try and reinstall these packages..\nRunning renv::repair(), we get the following message:\n# Library cache links ---------------------------------------------\nrenv appears to be unable to access the requested library path:\n- /home/rstudio/renv/library/linux-ubuntu-jammy/R-4.4/x86_64-pc-linux-gnu\nCheck that the 'rstudio' user has read / write access to this directory.\n\nDo you want to proceed? [Y/n]\nI spent a while googling and tried a few things including -\n\nSelecting to proceed, it installs the packages from CRAN.\nI tried making another container without disabling authentication - (sleep 2 && xdg-open http://localhost:8888) & sudo docker run -it -p 8888:8787 -e PASSWORD=password --name  huang2019_docker huang2019, then login with username rstudio password password - but had the same issue.\nLooking in the renv/ folder, it appears to only contain the package renv.\nI tried changing the server to 8787:8787 but this made no difference.\nLooking at the installed packages, it appears to not include anything we add (including stringi and openssl, which were installed seperately from the rest).\n\nBased on this issue, it appears that /home/rstudio/ is owned by the user docker rather than by us (the user rstudio).\n\nTried (sleep 2 && xdg-open http://localhost:8787) & sudo docker run --rm -it --user docker -v $(pwd)/home/$USER/foo -e USER=$USER -e USERID=$UID  -p 8787:8787 --name  huang2019_docker huang2019 but got error docker: Error response from daemon: unable to find user docker: no matching entries in passwd file.\nBased on this issue, tried changing ownership of renv directory when created in Dockerfile using RUN mkdir -p /renv/ && chown -c rstudio /renv/, but the same error message remained\n\nBased on this issue, tried changing user in Dockerfile for the renv::restore(). I realised whilst doing this that, when I‚Äôd made the renv folder, I hadn‚Äôt set it to /home/rstudio/ like I‚Äôd done for the rest of them! This might have been the issue. This produced an error while building the image:\n7.633 # Installing packages --------------------------------------------------------\n7.888 - Installing stringi ...                        OK [built from source and cached in 1.3m]\n83.54 Error: could not copy / move file '/home/rstudio/Documents/RStudio/PROJECT/renv/library/linux-ubuntu-jammy/R-4.4/x86_64-pc-linux-gnu/stringi' to '/home/rstudio/Documents/RStudio/PROJECT/renv/library/linux-ubuntu-jammy/R-4.4/x86_64-pc-linux-gnu/.renv-backup-7-stringi73f0bb75'\n83.54 move: cannot rename file '/home/rstudio/Documents/RStudio/PROJECT/renv/library/linux-ubuntu-jammy/R-4.4/x86_64-pc-linux-gnu/stringi' to '/home/rstudio/Documents/RStudio/PROJECT/renv/library/linux-ubuntu-jammy/R-4.4/x86_64-pc-linux-gnu/.renv-backup-7-stringi73f0bb75', reason 'Permission denied'\n83.54 copy: source file '/home/rstudio/Documents/RStudio/PROJECT/renv/library/linux-ubuntu-jammy/R-4.4/x86_64-pc-linux-gnu/.renv-copy-7606940ba' does not exist\nI tried temporarily removing the seperate installations of stringi and openssl, to see if renv::restore() would be successful. This was successful - up until stringi, which had the error from before re: libicui18n.so.66.\nI reintroduced the seperate installations, but tried switching install.packages(\"stringi\") to renv::install(\"stringi\"), but this had the issue from above.\nI then tried to make it more similar to the line installing renv, which does work, so did RUN R -e \"install.packages('stringi', repos = c(CRAN = 'https://cloud.r-project.org'))\", but no change.\nI then tried following the structure used to install renv in the aforementioned GitHub issue - so:\n# Set version of packages installed outside the lockfile\nENV RENV_VERSION 'v1.0.7'\nENV STRINGI_VERSION '1.8.4'\nENV OPENSSL_VERSION '2.2.0'\n\n# Install remotes (which we will use to install packages)\nRUN Rscript -e \"install.packages('remotes', repos = c(CRAN = 'https://cloud.r-project.org'))\"\n\n# Install renv from GitHub\nRUN Rscript -e \"remotes::install_github('rstudio/renv@${RENV_VERSION}')\"\n\n# Install stringi seperately due to issue with not detecting libicu\nRUN Rscript -e \"remotes::install_version('stringi', version='${STRINGI_VERSION}')\"\n\n# Install openssl seperately due to issue with not detecting libssl\nRUN Rscript -e \"remotes::install_version('openssl', version='${OPENSSL_VERSION}')\"\nHowever, we now are getting the libicui18n.so.66 issue, so it appears the problem is that I haven‚Äôt installed this into the right place for renv to find it, when I installed it seperately.\nI moved the installation to after having copied over the renv files -\n# Copy files (including renv.lock!) into the image\n# Thanks to .dockerignore, should not copy over renv/ (which is very large)\nCOPY . .\n\n# Copy renv auto-loader tools\nRUN mkdir -p renv/library\nCOPY .Rprofile .Rprofile\nCOPY renv/activate.R renv/activate.R\n\nbut we again get the first issue (could not copy / move file‚Ä¶).\n\nI tried moving it around a few different locations in the file to try and figure out what might be right. I also tried adding in an renv::activate()\n\n\n\nFix Quarto GitHub action\nReturned to the broken Quarto render action (which fails to find rmarkdown despite it having been installed with setup-renv). Some ideas:\n\nExample GitHub action for book with Python and R - although a few years old\nExample GitHub action where they installed packages directly\nRStudio tutorial for custom GitHub action workflow"
  },
  {
    "objectID": "logbook/posts/2024_07_19/index.html",
    "href": "logbook/posts/2024_07_19/index.html",
    "title": "Day 12",
    "section": "",
    "text": "Note\n\n\n\nFinishing up research compendium: final bit of troubleshooting docker, then GHCR."
  },
  {
    "objectID": "logbook/posts/2024_07_19/index.html#untimed-research-compendium",
    "href": "logbook/posts/2024_07_19/index.html#untimed-research-compendium",
    "title": "Day 12",
    "section": "Untimed: Research compendium",
    "text": "Untimed: Research compendium\n\nDocker\nResuming where I left off yesterday.\nCurrent challenge is to try and install stringi and openssl into the renv, so the later renv::restore() doesn‚Äôt fail.\n\nImportant commands when troubleshooting\nReminder of key commands:\n\nBuild image: sudo docker build --tag huang2019 . -f ./docker/Dockerfile\nDelete image: sudo docker image rm huang2019\nCreate container and open RStudio: (sleep 2 && xdg-open http://localhost:8888) & sudo docker run -it -p 8888:8787 -e DISABLE_AUTH=true --name  huang2019_docker huang2019\nDelete container: sudo docker rm huang2019_docker\n\n\n\nUnsuccessful attempts\nI tried variants of:\n\nPermissions (USER root, USER rstudio)\nInstallation instructions (remotes, renv, install.packages).\n\nI seemed to be getting success with:\nRUN R -e \"renv::activate()\"\nUSER root\nRUN R -e \"renv::install('stringi@${STRINGI_VERSION}')\"\nRUN R -e \"renv::install('openssl@${OPENSSL_VERSION}')\"\nUSER rstudio\n#RUN R -e \"remotes::install_version('openssl', version='${OPENSSL_VERSION}')\"\nRUN R -e \"renv::restore(lockfile=\\\"${PROJDIRECTORY}/renv.lock\\\", repos = c(CRAN = \\\"https://packagemanager.rstudio.com/all/__linux__/focal/latest\\\"))\"\nThis installed stringi and openssl into the renv successfully, although I seemed to need to have USER root permissions. For renv::restore(), it installed fine without needing to be USER root, but it then failed when installing stringi, so it appears to not have resolved the issue.\nError in dyn.load(file, DLLpath = DLLpath, ...) : \n    unable to load shared object '/tmp/Rtmp8ogTfT/renv-staging-7b6bc3cd/stringi/libs/stringi.so':\n  libicui18n.so.66: cannot open shared object file: No such file or directory\nI tried running all the renv installation whilst being the USER root. The image built successfully, but it did not open as an active project, and the renv/ folder only contained renv.\nI tried just running renv::install() (and not specific installs or restore), but this had errors of:\n\nrenv appears to be unable to access the requested library path: /home/rstudio/renv/library/linux-ubuntu-jammy/R-4.4/x86_64-pc-linux-gnu. Check that the 'rstudio' user has read / write access to this directory.\nError: error downloading 'https://cloud.r-project.org/src/contrib/mime_0.12.tar.gz' [cannot open URL 'https://cloud.r-project.org/src/contrib/mime_0.12.tar.gz']\n\n\n\nMinimal example (small .lock and specific files) succeeds\nI tried temporarily replacing my lockfile with a simple one with only my packages markdown and mime, and renv::restore() under user rstudio (similar to the GitHub issue from yesterday).\n\nWhen running renv::restore(), it seemed to install its own copy of renv (and didn‚Äôt use the one I‚Äôd installed above) - and indeed, I found I could remove that from the Dockerfile with no impact on the outcome\nThe library was successfully installed and accessible when I opened the container!\n\n{\n  \"R\": {\n    \"Version\": \"4.4.1\",\n    \"Repositories\": [\n      {\n        \"Name\": \"CRAN\",\n        \"URL\": \"https://cloud.r-project.org\"\n      }\n    ]\n  },\n  \"Packages\": {\n    \"markdown\": {\n      \"Package\": \"markdown\",\n      \"Version\": \"1.13\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"CRAN\",\n      \"Requirements\": [\n        \"R\",\n        \"commonmark\",\n        \"utils\",\n        \"xfun\"\n      ],\n      \"Hash\": \"074efab766a9d6360865ad39512f2a7e\"\n    },\n    \"markdown\": {\n      \"Package\": \"markdown\",\n      \"Version\": \"1.13\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"CRAN\",\n      \"Requirements\": [\n        \"R\",\n        \"commonmark\",\n        \"utils\",\n        \"xfun\"\n      ],\n      \"Hash\": \"074efab766a9d6360865ad39512f2a7e\"\n    }\n  }\n}\nWhen I did that successful run, I‚Äôd only copied over a few essential / example files\nCOPY renv.lock renv.lock\nCOPY reproduction.Rproj reproduction.Rproj\nCOPY scripts scripts\nInstead of\nCOPY . .\nHowever, when I switched back to just copying everything, I get issues that it cannot copy and move files and that permission is denied.\nI tried manually specifying everything from the folder (exc. .Rhistory and .Rproj.user)‚Ä¶\nCOPY DESCRIPTION DESCRIPTION\nCOPY docker docker\nCOPY outputs outputs\nCOPY README.md README.md\nCOPY renv.lock renv.lock\nCOPY reproduction.Rproj reproduction.Rproj\nCOPY scripts scripts\nCOPY tests tests\n‚Ä¶And that built fine.\nSo I tried again with COPY . ., but adding .Rhistory and .Rproj.user to the .dockerignore - but this failed like before. Hence, I decided to stick with specifying the files being copied (and regardless, I guess that has the benefit of being specific about what we include). It appears I must be copying over something undesirable when running COPY . . which is causing us issues, although I can‚Äôt spot what this might be.\n\n\nTroubleshooting stringi\nI ran renv::snapshot() to restore the lock file back to being complete, and then tried to build the image again. This failed with the libicui18n.so.66 error from before. I reverted the lockfile back to the simple version (so the various test runs are quicker), but adding stringi, getting the same error.\nTried adding install before restore - RUN R -e \"renv::install('stringi')\" (which automatically installs renv for us before running)‚Ä¶. and it was successfull‚Ä¶.! The library contained stringi and the other packages from the lockfile.\nSo, I then, again, restored the full lockfile, add an install for openssl, and tried once more.\nThis was successful!\n\n\nGiving read, write and save access to folders\nAll the files are present and can be run, and the renv is active with all the required files. However, if we want to save anything (e.g.¬†modifying qmd, or saving output from model with ggsave), we don‚Äôt currently have permissions.\nBased on the docker documentation, I tried using --chown with the COPY statements to specifically give permissions to the user rstudio. I also modified stringi and openssl so they have the specific versions.\nThis worked!\n\n\n\nGitHub Container Registry\nActivated and ran but hit issue on COPY renv/activate.R renv/activate.R. It cannot find the file - although the previous commands with relative file paths all ran without issue.\nERROR: failed to solve: failed to compute cache key: failed to calculate checksum of ref 70b54bab-f7c4-4333-a94b-def3ee959db6::iqqrpwic0590fua1x3sv0utsd: \"/renv/activate.R\": not found\nChecking on GitHub, the activate.R file is included. However, in my prior example of this (Shoaib et al.¬†2022), I ran the dockerfile from the main folder rather than the reproduction folder. Hence, I tried updating the Dockerfile so that the file paths are relative to reproduction‚Äôs parent, which fixed the issue.\nI then tested downloading the image from GHCR and ran one of the scripts within that, and it all worked fine. Add instructions for this to the README.\n\n\nSummary report\nFilled out the summary report template."
  },
  {
    "objectID": "logbook/posts/2024_07_09/index.html",
    "href": "logbook/posts/2024_07_09/index.html",
    "title": "Day 5",
    "section": "",
    "text": "Note\n\n\n\nWorking on figures 2 + 3 and in-text result 3. Plus (untimed) fixing Quarto book (environment issues). Total time used: 16h 15m (40.6%)."
  },
  {
    "objectID": "logbook/posts/2024_07_09/index.html#continuing-on-figure-2",
    "href": "logbook/posts/2024_07_09/index.html#continuing-on-figure-2",
    "title": "Day 5",
    "section": "09.04-09.06, 09.14-09.15: Continuing on Figure 2",
    "text": "09.04-09.06, 09.14-09.15: Continuing on Figure 2\nI was curious to see how a different seed would impact the appearance of the figures, and so tried changing the seed from 200 to 300. However, it looks fairly similar.\n\n\n\nFigure 2A Example 1\n\n\nAnd with seed 500 too:\n\n\n\nFigure 2A Example 2"
  },
  {
    "objectID": "logbook/posts/2024_07_09/index.html#untimed-fixing-github-commits-and-action-for-quarto-book",
    "href": "logbook/posts/2024_07_09/index.html#untimed-fixing-github-commits-and-action-for-quarto-book",
    "title": "Day 5",
    "section": "Untimed: Fixing GitHub commits and action for Quarto book",
    "text": "Untimed: Fixing GitHub commits and action for Quarto book\nThis is not timed as I feel it is relevant to the Quarto book and not specific to this reproduction, and so reflects an issue I might have ironed out in a test-run, if I had done a second test-run in R (and not just a Python test-run).\nHad commit the produced .csv files without recognising they were too large. Undid commits with git reset HEAD^, and switched reproduction to only save relevant rows and columns, and to save as a compressed .csv.gz file. This required adding R.utils to the environment.\nAlso, modified quarto_publish.yaml to use setup-r-dependencies, but ran into several errors with this. One I could resolve related to having not pushed the renv/ directory. However, one I have struggled to resolve, which was that there is no package called ‚Äòpak‚Äô. I‚Äôve tried switching to ubuntu-22.04, as suggested on this issue. Also tried adding pak within the setup action with extra-packages: | any::pak. Still, issue persists.\nI explored using conda to manage the R environment, and whether that might actually be easier, as I‚Äôve had alot of challenges with renv for this book (but also in general, having just defaulted to using the latest packages, due to facing major issues installing older R and packages). Based on this tutorial, I created a requirements.txt file then ran the following:\nconda create -n huang2019\nconda activate huang2019\nconda install -c conda-forge r-base==4.4.1 --file reproduction/requirements.txt\nHowever, this was running into several issues, stating that the packages were not compatabile with the chosen r-base.\nFinally, I tried switching back to setup-renv. However, this took a long time to run but it does run (and supposedly future runs can use that cache so it is quicker). However, just like before, it stalls with an error (despite seeing that the renv installation above definitely included rmarkdown):\n[19/22] reproduction/reproduction.Rmd\nError in loadNamespace(x) : there is no package called ‚Äòrmarkdown‚Äô\nCalls: loadNamespace -&gt; withRestarts -&gt; withOneRestart -&gt; doWithOneRestart\nExecution halted\nR installation:\n  Version: 4.4.1\n  Path: /opt/R/4.4.1/lib/R\n  LibPaths:\n    - /home/runner/work/_temp/Library\n    - /opt/R/4.4.1/lib/R/library\n  knitr: (None)\n  rmarkdown: (None)\nThe knitr package is not available in this R installation.\nInstall with install.packages(\"knitr\")\nThe rmarkdown package is not available in this R installation.\nInstall with install.packages(\"rmarkdown\")\nERROR: Error\n    at renderFiles (file:///opt/quarto/bin/quarto.js:78081:29)\n    at eventLoopTick (ext:core/01_core.js:153:7)\n    at async renderProject (file:///opt/quarto/bin/quarto.js:78479:25)\n    at async renderForPublish (file:///opt/quarto/bin/quarto.js:109334:33)\n    at async renderForPublish (file:///opt/quarto/bin/quarto.js:104866:24)\n    at async Object.publish1 [as publish] (file:///opt/quarto/bin/quarto.js:105351:26)\n    at async publishSite (file:///opt/quarto/bin/quarto.js:109371:38)\n    at async publish7 (file:///opt/quarto/bin/quarto.js:109590:61)\n    at async doPublish (file:///opt/quarto/bin/quarto.js:109550:13)\n    at async publishAction (file:///opt/quarto/bin/quarto.js:109561:9)\nError: Process completed with exit code 1.\nTried adding Rscript{0} as shell for running R code but this was incorrect.\nThen tried switching reproduction.Rmd to reproduction.qmd. Re-running the action, it was very very slow (14 minutes) (so seemingly not using the cache). Moreover, it still hit an error as above.\nI decided a simpler solution might be to not require R in the Quarto build, and to instead ensure that any scripts using R are pre-rendered in some way (similar to how .ipynb files behave). For .ipynb, the default behaviour is that execute: enabled: false, as in the documentation. However, when run, encountered the error as above.\nTried a few options, including to manually install knitr and rmarkdown, although returned issue that 'lib = \"/usr/local/lib/R/site-library\"' is not writable.\nDid consider other options could be to clone and build examples that are set up with a GitHub action - e.g.¬†renv example, nix example. However, for now, have decided to just disable the action and do it manually from the terminal:\n\nquarto render (rebuilds whole site)\nquarto publish (pushes to github pages)"
  },
  {
    "objectID": "logbook/posts/2024_07_09/index.html#returning-to-figure-2",
    "href": "logbook/posts/2024_07_09/index.html#returning-to-figure-2",
    "title": "Day 5",
    "section": "13.34-14.30, 14.39-14.55: Returning to Figure 2",
    "text": "13.34-14.30, 14.39-14.55: Returning to Figure 2\nComparing my figures to the original, although it is now more visually similar than it was, there is still a lot of mismatch compared with those graphs. I‚Äôm suspicious this could be due to scaling, as currently I am just using the built in geom_density() scaling to 1, rather than explicitly scaling to the number not waiting. I found the R scripts behind this function unclear.\nInstead, I spent some time working out how to scale these manually, ensuring I was definitely dividing each density by the density from wait_time 0, and this confirmed that it matched up with the results from geom_density()‚Äôs scaled. As such, it seems the issue is not due to the scaling.\n# Create figure as usual\np &lt;- create_plot(res_base,\n                  title=\"Baseline\",\n                  ylab=\"Standardised density of patient in queue\")\n\n# Get data from the plot\nplot_data &lt;- ggplot_build(p)$data[[1]]\n\n# Create dataframe with the densities for when the waitimes are 0\nno_wait &lt;- plot_data %&gt;% filter(x==0) %&gt;% select(colour, density, scaled)\n#print(no_wait)\n\n# Loop through each of the colours (which reflect the resource groups)\nfor (c in no_wait$colour) {\n  # Filter the plot data to that resource group, then divide the densities by\n  # the density from wait time 0\n  d &lt;- plot_data %&gt;%\n    filter(colour == c) %&gt;%\n    mutate(scaled2 = density / no_wait[no_wait$colour==c, \"density\"]) %&gt;%\n    ungroup() %&gt;%\n    select(scaled, scaled2)\n\n  # Find the number of rows where these values match the scaled values\n  n_match &lt;- sum(apply(d, 1, function(x) length(unique(x)) == 1))\n  n_total &lt;- nrow(d)\n  print(sprintf(\"%s out of %s results match\", n_match, n_total))\n}\nGiven the scaling seems ok, the only other options I can think of are:\n\nThe environment - although I would anticipate that to be more of an issue of a model not running, rather than fundamentally changing results\nThe model parameters - I‚Äôve had another look back over the model code, and tried changing INR night -\n\nModel was set up to have INR staff on a schedule, but in the paper they are 24 hours. I had set this up by having 1 day and 1 night staff, but I‚Äôve tried changing the code so it‚Äôs just a single resource (like how ED team and stroke team are set up)\nOtherwise, all parameters look correct compared against Table 1\n\n\nHowever, model results came out the same. I am rather stuck on this now, so will move onto Figure 3 (having first, re-run reproduction with seed=200, as per in-text result 1)."
  },
  {
    "objectID": "logbook/posts/2024_07_09/index.html#starting-on-figure-3-and-in-text-result-3",
    "href": "logbook/posts/2024_07_09/index.html#starting-on-figure-3-and-in-text-result-3",
    "title": "Day 5",
    "section": "14.56-15.26, 15.31-16.19, 16.24-16.56: Starting on Figure 3 and in-text result 3",
    "text": "14.56-15.26, 15.31-16.19, 16.24-16.56: Starting on Figure 3 and in-text result 3\nFor this scenario analysis, the ‚Äúday time working hours of all human resources are extended by up to 2h, extending resource access to all patients‚Äù (Huang et al. (2019)). Given how the model scheduling is set-up, it is assumed that this means we simply adjust the schedule to end at 5, 6 or 7pm (and that that would simply shortern the night staff time).\nI ran these scenarios, processing and saving the relevant model results.\nFor in-text result 3, I can see that the results do not match up to the paper. I am not surprised by this though - as the model had no seed control, as it is not mentioned in the paper, we can assume that it might not have been used by the original study, and so variation between the scenarios could (in part) reflect model stochasticity.\n\nimport pandas as pd\n\npd.read_csv(\"txt3.csv\")\n\n\n\n\n\n\n\n\n\nscenario\nshift\nmean\ndiff_from_5pm\n\n\n\n\n0\nBaseline\n5pm\n13.958269\n0.00\n\n\n1\nBaseline\n6pm\n12.486042\n-1.47\n\n\n2\nBaseline\n7pm\n12.491421\n-1.47\n\n\n3\nExclusive use\n5pm\n8.117729\n0.00\n\n\n4\nExclusive use\n6pm\n7.802954\n-0.31\n\n\n5\nExclusive use\n7pm\n6.432643\n-1.69\n\n\n6\nTwo AngioINRs\n5pm\n13.511560\n0.00\n\n\n7\nTwo AngioINRs\n6pm\n11.408894\n-2.10\n\n\n8\nTwo AngioINRs\n7pm\n11.256842\n-2.25\n\n\n\n\n\n\n\n\nTo test this assumption, I ran the model again for baseline with two further seeds. We can see the importance of seed control here. For example, with seed 700, we see a broader range of results, with the result for 6pm (13.32) is much higher than for the other two seeds and, compared with their 5pm results, we would‚Äôve seen less of a reduction. Similarly, if we compared the 5pm seed 700 result with the 6pm seed 500 result, we would see a much greater reduction.\n\npd.read_csv(\"txt3_seeds.csv\")\n\n\n\n\n\n\n\n\n\nscenario\n5pm\n6pm\n7pm\n\n\n\n\n0\nBaseline\n13.96\n12.49\n12.49\n\n\n1\nBaseline (seed 500)\n13.96\n12.15\n12.20\n\n\n2\nBaseline (seed 700)\n15.47\n13.32\n12.30\n\n\n\n\n\n\n\n\nFor Figure 3, it was simple to adapt the function to create it but, similar to Figure 2, there are several differences in the original study results.\n\n\n\nFigure 3"
  },
  {
    "objectID": "logbook/posts/2024_07_09/index.html#timings",
    "href": "logbook/posts/2024_07_09/index.html#timings",
    "title": "Day 5",
    "section": "Timings",
    "text": "Timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 790\n\n# Times from today\ntimes = [\n    ('09.04', '09.06'),\n    ('09.14', '09.15'),\n    ('13.34', '14.30'),\n    ('14.39', '14.55'),\n    ('14.56', '15.26'),\n    ('15.31', '16.19'),\n    ('16.24', '16.56')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 185m, or 3h 5m\nTotal used to date: 975m, or 16h 15m\nTime remaining: 1425m, or 23h 45m\nUsed 40.6% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_07_15/index.html",
    "href": "logbook/posts/2024_07_15/index.html",
    "title": "Day 9",
    "section": "",
    "text": "Note\n\n\n\nConsensus on evaluation + reflections + research compendium. Total evaluation time: 1h 45m."
  },
  {
    "objectID": "logbook/posts/2024_07_15/index.html#consensus-on-evaluation",
    "href": "logbook/posts/2024_07_15/index.html#consensus-on-evaluation",
    "title": "Day 9",
    "section": "08.22-08.30, 08.37-08.41, 10.53-10.55: Consensus on evaluation",
    "text": "08.22-08.30, 08.37-08.41, 10.53-10.55: Consensus on evaluation\nPulled together to share with Tom and Alison, to get a second opinion on these, and emailed over a link. Later, input responses below. Agreed with all decisions, so no changes required.\nBadges:\n\nhttps://pythonhealthdatascience.github.io/stars-reproduce-huang-2019/evaluation/badges.html\nNo uncertainties\n9 unmet criteria\n\nSTARS framework:\n\nhttps://pythonhealthdatascience.github.io/stars-reproduce-huang-2019/evaluation/artefacts.html\nNo uncertainities\n9 unmet criteria\n\nReporting guidelines:\n\nhttps://pythonhealthdatascience.github.io/stars-reproduce-huang-2019/evaluation/reporting.html\nFive uncertainities as below.\n4 + 7 unmet criteria\n\n\n\n\n\n\n\n\n\nItem\nMy comments\nThoughts from Tom\n\n\n\n\nSTRESS-DES 1.2 Model outputs. Define all quantitative performance measures that are reported, using equations where necessary. Specify how and when they are calculated during the model run along with how any measures of error such as confidence intervals are calculated.\nIt does describe the measures, and how these are calculated, and so I have said it met these criteria, although I did find it hard to understand/calculate the relative probability of waiting, and would‚Äôve benefited from further detail/equations. Currently marked as fully met.\nAgree with decision.\n\n\nSTRESS-DES 1.3 Experimentation aims. If the model has been used for experimentation, state the objectives that it was used to investigate. (A) Scenario based analysis ‚Äì Provide a name and description for each scenario, providing a rationale for the choice of scenarios and ensure that item 2.3 (below) is completed.\nI feel the paper does describe the scenarios clearly - my only hesitation is that I have been unable to successfully implement the exclusive use scenario - but that feels like a coding issue rather than a description issue? As, on the face of it, the article describes everything I need to know. Currently marked as fully met.\nAgree with decision. Argue that description in article is a reasonable explanation of the logic in play - ‚ÄúFirst, in the ‚Äúexclusive-use‚Äù scenario, angioINR is not available for elective IR patients. Its use is restricted to stroke, elective INR and emergency IR patients‚Äù‚Äù Huang et al. (2019)\n\n\nSTRESS-DES 3.2 Pre-processing. Provide details of any data manipulation that has taken place before its use in the simulation, e.g.¬†interpolation to account for missing data or the removal of outliers.\nNone provided, so presumed not applicable - but hard to say, as maybe there was pre-processing that simply wasn‚Äôt mentioned. But as not possible to know either way, assumed not-applicable\nAgree with decision. Give benefit of the doubt by its absence - although ideally they would state no data pre-processing was used.\n\n\nISPOR SDM 12 Is cross validation performed and reported\nWasn‚Äôt certain whether to mark this is unmet (‚ùå) or not applicable (N/A)? Currently set as unmet.Evidence - stating there is a gap in the Introduction: ‚ÄúIn contrast to other healthcare fields, a resource-use optimization model has not been implemented for comprehensive stroke services.‚Äù Huang et al. (2019)\nAgree with decision.\n\n\nISPOR SDM 15 Is the model generalizability issue discussed?\nNot sure if it is partially (üü°) or fully met (‚úÖ)? Currently marked as fully.Evidence - Discussion: ‚ÄúThe quality of the ECR service appears to be robust to important parameters, such as the number of radiologists. The simulation findings apply to ECR services that can be represented by the model in this study. As such, utilization of this model to its maximum capacity requires tailoring the model to local needs, as institutional bottlenecks differ between providers. We specifically developed this model using an open source programming language so that the source code can serve as a basis for future model refinement and modification.‚ÄùHuang et al. (2019)\nAgree with decision."
  },
  {
    "objectID": "logbook/posts/2024_07_15/index.html#timings-for-evaluation",
    "href": "logbook/posts/2024_07_15/index.html#timings-for-evaluation",
    "title": "Day 9",
    "section": "Timings for evaluation",
    "text": "Timings for evaluation\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 91\n\n# Times from today\ntimes = [\n    ('08.22', '08.30'),\n    ('08.37', '08.41'),\n    ('10.53', '10.55')]\n\ncalculate_times(used_to_date, times, limit=False)\n\nTime spent today: 14m, or 0h 14m\nTotal used to date: 105m, or 1h 45m"
  },
  {
    "objectID": "logbook/posts/2024_07_15/index.html#untimed-revisiting-r-dependency-management-options",
    "href": "logbook/posts/2024_07_15/index.html#untimed-revisiting-r-dependency-management-options",
    "title": "Day 9",
    "section": "Untimed: Revisiting R dependency management options",
    "text": "Untimed: Revisiting R dependency management options\nDid some further research into options for dependency management in R."
  },
  {
    "objectID": "logbook/posts/2024_07_15/index.html#untimed-recording-troubleshooting-and-reflections",
    "href": "logbook/posts/2024_07_15/index.html#untimed-recording-troubleshooting-and-reflections",
    "title": "Day 9",
    "section": "Untimed: Recording troubleshooting and reflections",
    "text": "Untimed: Recording troubleshooting and reflections\nCompleted reflections.qmd."
  },
  {
    "objectID": "logbook/posts/2024_07_15/index.html#untimed-revisiting-github-actions-issues",
    "href": "logbook/posts/2024_07_15/index.html#untimed-revisiting-github-actions-issues",
    "title": "Day 9",
    "section": "Untimed: Revisiting GitHub actions issues",
    "text": "Untimed: Revisiting GitHub actions issues\nTried forking and running actions from existing repositories that render and publish an R-based Quarto book on GitHub pages.\n\nhttps://github.com/ddotta/cookbook-rpolars - build failed due to unexpected value to function in one of the .qmd files\nhttps://github.com/b-rodrigues/rap4all - add workflow_dispatch to action and ran it but it failed as no gh-pages branch. Hence, copied that also (which successfully deployed) and ran the action again. This worked! Hurrah! üòÅ\n\nThen updated my action to be similar to the rap4all actions and tried it. This failed - ‚Äúconfiguration failed because libcurl was not found‚Äù. I add installation of libcurl and ran it again, but this all failed just like before, with the error there is no package called 'rmarkdown'."
  },
  {
    "objectID": "logbook/posts/2024_07_15/index.html#untimed-research-compendium",
    "href": "logbook/posts/2024_07_15/index.html#untimed-research-compendium",
    "title": "Day 9",
    "section": "Untimed: Research compendium",
    "text": "Untimed: Research compendium\nSome further work on the research compendium stage.\n\nAdd testthat to environment\nWrote basic test but to run it, RStudio had prompt that it required update of devtools. Selected ‚Äúyes‚Äù and then saved another renv::snapshot() once it completed. However, I cancelled it as realised could run without devtools (and devtools would be alot of extra dependencies!)\nRan test with testthat::test_dir(\"tests\")\n\nLinks:\n\nAnother good resource for tests in R: https://raps-with-r.dev/testing.html\nA good resource for Docker and R: https://raps-with-r.dev/repro_cont.html\nTom‚Äôs R dockerfile: https://github.com/TomMonks/reproducible_r_docker/blob/main/Dockerfile"
  },
  {
    "objectID": "original_study/desECR/figure_description.html",
    "href": "original_study/desECR/figure_description.html",
    "title": "Reproducing Huang et al. 2019",
    "section": "",
    "text": "ed = Confirmed stroke patients\ninr = Non-emergency interventional neuroradiology patients\neir = Emergency interventional radiology patients\nir = Non-emergency interventional radiology patients"
  }
]