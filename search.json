[
  {
    "objectID": "original_study/desECR/description.html",
    "href": "original_study/desECR/description.html",
    "title": "Reproducing Huang et al. 2019",
    "section": "",
    "text": "# DES for ECR Authors: S Huang, J Maingard, H Kok, C Barras, V Thijs, RV Chandra, DM Brooks, H Asadi.\nThis is an interactive discrete event simulation of resource optimization of an endovascular clot retrieval service. Details of the simulation configuration can be visualized here. Built with R-simmer, Email questions/feedback to shiweihuang at outlook dot com."
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing",
    "section": "",
    "text": "üéâ Thank you for checking out our project! üéâ\nThis page contains guidelines on how to get in touch with us and potentially contribute towards this repository.\n\n\nYou can contact the researchers on this project using the provided email addresses in CITATION.cff.\n\n\n\nIf you spot an issue, you are welcome to raise this either by:\n\nUsing GitHub Issues.\nForking the repository, make your changes and submit a pull request for review."
  },
  {
    "objectID": "CONTRIBUTING.html#email",
    "href": "CONTRIBUTING.html#email",
    "title": "Contributing",
    "section": "",
    "text": "You can contact the researchers on this project using the provided email addresses in CITATION.cff."
  },
  {
    "objectID": "CONTRIBUTING.html#suggesting-changes",
    "href": "CONTRIBUTING.html#suggesting-changes",
    "title": "Contributing",
    "section": "",
    "text": "If you spot an issue, you are welcome to raise this either by:\n\nUsing GitHub Issues.\nForking the repository, make your changes and submit a pull request for review."
  },
  {
    "objectID": "logbook/posts/2024_07_05/index.html",
    "href": "logbook/posts/2024_07_05/index.html",
    "title": "Day 3",
    "section": "",
    "text": "Note\n\n\n\nSet-up environment and run model. Total time used: 7h 23m (18.5%)"
  },
  {
    "objectID": "logbook/posts/2024_07_05/index.html#set-python-interpreter",
    "href": "logbook/posts/2024_07_05/index.html#set-python-interpreter",
    "title": "Day 3",
    "section": "09.46-09.47: Set Python interpreter",
    "text": "09.46-09.47: Set Python interpreter\nSet Python interpreter (e.g.¬†to render these from RStudio) by clicking on the project in the top right of RStudio, then selecting Project Options &gt; Python, and selecting the quarto_huang_2019 virtual environment I‚Äôd set up."
  },
  {
    "objectID": "logbook/posts/2024_07_05/index.html#returning-to-troubleshooting-r-version",
    "href": "logbook/posts/2024_07_05/index.html#returning-to-troubleshooting-r-version",
    "title": "Day 3",
    "section": "09.48-10.21: Returning to troubleshooting R version",
    "text": "09.48-10.21: Returning to troubleshooting R version\nContinuing to look at the instructions for old R releases from yesterday:\n\n‚ÄúAs of July 2023, packages for R versions below 4.0 are no longer being updated. R 3.6 packages for Ubuntu on i386 and amd64 are available for most stable Desktop releases of Ubuntu until their official end of life date. However, only the latest Long Term Support (LTS) release is fully supported. As of November 18, 2018 the supported releases are Bionic Beaver (18.04;LTS), Xenial Xerus (16.04; LTS), and Trusty Tahr (14.04; LTS).‚Äù\n\nBy running lsb_release -a, I can see that my linux version is jammy (22.04.4 LTS). Looking at the instructions from this Stackoverflow post, I‚Äôm a bit unclear as to whether I can use any of these if they‚Äôre for older versions of linux.\nFrom this help post, I then stumbled across RStudio r-builds which has R builds that say they should install fast on Ubuntu from a .deb file and are designed to easily switch between multiple versions of R. These say they support Ubunutu 22.04. I ran:\nR_VERSION=3.6.0\ncurl -O https://cdn.posit.co/r/ubuntu-2204/pkgs/r-${R_VERSION}_1_amd64.deb\nsudo apt-get install gdebi-core\nsudo gdebi r-${R_VERSION}_1_amd64.deb\nI confirmed this was installed by running /opt/R/${R_VERSION}/bin/R --version.\nI then followed their instructions to add R to the system path:\nsudo ln -s /opt/R/${R_VERSION}/bin/R /usr/local/bin/R \nsudo ln -s /opt/R/${R_VERSION}/bin/Rscript /usr/local/bin/Rscript\nI restarted RStudio and found I was now in R 3.6.0. I delete the renv (which was built with 4.4.1) and remade it.\nrenv::deactivate(clean=TRUE)\ninstall.packages(\"renv\")\nrenv::init(bare=TRUE)\nrenv::snapshot()\nThe lock file now had R 3.6.0 (previously 4.4.1) and renv 1.0.7."
  },
  {
    "objectID": "logbook/posts/2024_07_05/index.html#installing-the-packages",
    "href": "logbook/posts/2024_07_05/index.html#installing-the-packages",
    "title": "Day 3",
    "section": "10.40-11.30, 11.35-11.41: Installing the packages",
    "text": "10.40-11.30, 11.35-11.41: Installing the packages\nI ran renv::install() but it failed with: Warning: failed to find source for 'simmer.plot 0.1.15' in package repositories. Error: failed to retrieve package 'simmer.plot@0.1.15'.\nI then tried with remotes:\ninstall.packages(\"remotes\")\nremotes::install_version(\"simmer\", \"4.2.2\")\nHowever, this failed like before. Instead, I decided a different tactic - to just download them without the specified versions. I removed the versions from DESCRIPTION and ran renv::install(). However, this stopped with an error: Error: package 'evaluate' is not available.\nI then tried working through each package one by one.\nrenv::install(\"simmer\") was successful.\nrenv::install(\"simmer.plot\") failed with the issue of ‚Äòevaluate‚Äô is not available. Based on this StackOverflow post, I tried installing ‚Äòstringi‚Äô - but that didn‚Äôt end up helping. I tried install evaluate before and after restarting the R session but still stated as not available.\nUncertain on what else might fix this, I decided to actually just start again from the latest version of R and try installing the packages there and see if I could get it to work without backdating the packages. I closed RStudio and ran the commands as above but changed R_VERSION to 4.4.1. I also couldn‚Äôt run the commands for symbolic link as it said the files already exist. I restarted R but still 3.6.0. Looking in /opt/R/, I can see I now have 3.6.0 and 4.4.1.\nBased on the prior tutorial I‚Äôd found, I tried:\nexport RSTUDIO_WHICH_R=/opt/R/4.4.1/bin/R\nrstudio\nThis worked, although default when open from application bar was still set to 3.6.0. I tried changing the .profile file (nano .profile) to add export RSTUDIO_WHICH_R=/opt/R/4.4.1/bin/R but made no difference.\nI tried forcing replacement of the symbolic links then reopening RStudio:\nR_VERSION=4.4.1\nsudo ln -snf /opt/R/${R_VERSION}/bin/R /usr/local/bin/R \nsudo ln -snf /opt/R/${R_VERSION}/bin/Rscript /usr/local/bin/Rscript\nThis worked! So, trying again (with DESCRIPTION file still containing no versions)‚Ä¶\nrenv::deactivate(clean=TRUE)\ninstall.packages(\"renv\")\nrenv::init(bare=TRUE)\nrenv::snapshot()\nrenv::install()\nrenv::snapshot()"
  },
  {
    "objectID": "logbook/posts/2024_07_05/index.html#try-running-the-code",
    "href": "logbook/posts/2024_07_05/index.html#try-running-the-code",
    "title": "Day 3",
    "section": "11.41-11.46, 11.52-12.00: Try running the code",
    "text": "11.41-11.46, 11.52-12.00: Try running the code\nI copied over server.R and, on opening, it said that plyr and shiny were required but not installed, so I add these to the environment as well.\nOn reflection, I realised that the settings to only store dependencies from DESCRIPTION in renv.lock probably wouldn‚Äôt be great, in case hidden things were also installed, so changed this setting to ‚Äúimplicit‚Äù (which is default).\nI ran the file and it did the command shiny, but said Error: object 'shiny' not found. I copied over all the files and tried again. It ran the script but nothing happened. Based on the Shiny documentation, I moved the files into a folder called app and run the following in R console:\nlibrary(shiny)\nrunApp(\"app\")\nThis opened up a shiny app, but got an error ‚Äúthere is no package called ‚Äòmarkdown‚Äô‚Äù. I add this to the environment and tried again.\nThis ran the app successfully.\nHowever, from having looked at the app online, I knew that the figures it produced are not what I need to reproduce the results presented in the paper."
  },
  {
    "objectID": "logbook/posts/2024_07_05/index.html#getting-the-raw-model-results",
    "href": "logbook/posts/2024_07_05/index.html#getting-the-raw-model-results",
    "title": "Day 3",
    "section": "12.07-12.21, 13.00-13.21, 13.28-13.41: Getting the raw model results",
    "text": "12.07-12.21, 13.00-13.21, 13.28-13.41: Getting the raw model results\nI copied the function simulate_nav from server.R. Looking through it, there was only one part still using shiny - the progress bar - and I removed those lines of code, then add a call for the function at the end of the script (simulate_nav()), and ran it. This ran for a while, which was a little odd given how quick the app was.\nI tried running it with a very short run time (simulate_nav(run_t=60)) and this returned results!\nI borrowed from the plot_nav() function in server.R to help process the results.\nI add the reproduction.Rmd to the Quarto site, but this had issues since the Quarto book renv is seperate to the analysis renv. Based on this forum post, there are two possible solutions:\n\nIntegrate the .html file produced from the .Rmd into the book, so it is pre-rendered, and set the .Rmd to Render on Save.\nAdd the packages needed for the book to the analysis renv.\n\nHowever, it appears you‚Äôd have to copy the .html code into the Quarto document. So, decided on the simpler solution of adding the required packages for the book to the analysis environment. I deleted the environment in the main folder."
  },
  {
    "objectID": "logbook/posts/2024_07_05/index.html#checking-model-parameters",
    "href": "logbook/posts/2024_07_05/index.html#checking-model-parameters",
    "title": "Day 3",
    "section": "13.42-14.13: Checking model parameters",
    "text": "13.42-14.13: Checking model parameters\n\nComparing parameters\nTable 1 provides the parameters for the model. I compared these against the function inputs (as the model have no comments/docstrings, it took a little while to make sure I was matching up the right things).\nPhysical and human resources:\n\n\n\n\n\n\n\n\nParameter\nPaper\nScript\n\n\n\n\nAngiography machine for INR and IR\n1\nangio_inr = 1\n\n\nAngiography machine for IR only\n1\nangio_ir = 1\n\n\nCT\n2\nct = 2\n\n\nInterventional neuroradiologist\n1 24h\ninr = 1 and inr_night = 1\n\n\nInterventional radiologist\n2 8am-5pm 1 5pm-8am\nir = 1 and ir_night = 1\n\n\nAngiography staff\n6 8am-5pm 3 5pm-8am\nangio_staff = 10 and angio_staff_night = 3\n\n\nED team\n10 24h\ned_staff = 10\n\n\nStroke team\n1 24h\nstroke_staff = 1\n\n\n\nFor the shifts parameter: shifts = c(8,17)\nPatients:\n\n\n\nParameter\nPaper N\nPaper IAT\nScript\n\n\n\n\nED\n107,700\n5\ned_pt = 107000\n\n\nSuspected stroke\n750\n701\nst_pt = 750\n\n\nAIS\n450\n1168\nais_pt = 450\n\n\nECR\n58\n9062\necr_pt = 58\n\n\nElective INR\n104\n5054\ninr_pt = 300\n\n\nEmergency IR\n468\n1123\neir_pt= 1000\n\n\nElective IR\n3805\n138\nir_pt = 4000\n\n\n\nI also compared some other parameters mentioned in the paper:\n\nSimulated each scenario 30 times - nsim = 1\nRuntime 365 days - run_t = 10000\n\n\n\nCorrecting differences\n\nInterventional neuroradiologist: inr_night = 0\nInterventional radiologist: ir = 2\nAngiography staff: angio_staff = 6\nED: ed_pt = 107700\nElective INR: inr_pt = 104\nEmergency INR: eir_pt= 468\nElective IR: ir_pt = 3805\nReplications: nsim=30\nRun time‚Ä¶\n\nIn the paper, run time is 365 days\nIn the script, run_t = 10000 and RUN_T = run_t * 40320\nDeduced that time unit is minutes\nThis is set up for the app, where user inputs the run time in months, and 40320 minutes = 28 days\nTo more easily reproduce paper (with run time 365 days), modified script so input is in days (which are then converted to minutes for RUN_T)"
  },
  {
    "objectID": "logbook/posts/2024_07_05/index.html#fixing-environment",
    "href": "logbook/posts/2024_07_05/index.html#fixing-environment",
    "title": "Day 3",
    "section": "14.22-14.27: Fixing environment",
    "text": "14.22-14.27: Fixing environment\nThe build of the book on GitHub failed:\nConfiguration failed because libcurl was not found. Try installing:\n * deb: libcurl4-openssl-dev (Debian, Ubuntu, etc)\n * rpm: libcurl-devel (Fedora, CentOS, RHEL)\nIf libcurl is already installed, check that 'pkg-config' is in your\nPATH and PKG_CONFIG_PATH contains a libcurl.pc file. If pkg-config\nis unavailable you can set INCLUDE_DIR and LIB_DIR manually via:\nR CMD INSTALL --configure-vars='INCLUDE_DIR=... LIB_DIR=...'\nBased on this GitHub issue, add installation of this to the action."
  },
  {
    "objectID": "logbook/posts/2024_07_05/index.html#in-text-results-1-and-2",
    "href": "logbook/posts/2024_07_05/index.html#in-text-results-1-and-2",
    "title": "Day 3",
    "section": "14.29-15.11, 15.32-16.23: In-text results 1 and 2",
    "text": "14.29-15.11, 15.32-16.23: In-text results 1 and 2\nThe provided processing scripts may be able to help guide us, but not provided will create the Figures in the paper, so we do need to write that from scratch.\nAlthough the article focuses on the AngioINR, the plot includes 6 resources. The resources are provided by simmer‚Äôs get_mon_resources().\nWe‚Äôll start with Figure 2 and its scenarios - with the related in-text results 1 and 2 probbaly being the easiest to initially check.\nThe plot the angio INR wait times, which can be obtained from the arrivals dataframe.\nI created a function that runs the model with the baseline parameters identified above, then looked to the Figure 2 model variants.\n\nExclusive use\nIn this scenario, AngioINR not available to elective IR patients. It is available to stroke, selective INR and emergency IR patients.\nLooking at the model code, use of the angioINR is controlled by a seize(\"angio_inr\", 1) statement in the trajectory() for each patient. Can see that it is seized in:\n\necr_traj (ECR)\nir_traj (Elective IR)\ninr_traj (Elective INR)\neir_traj (Emergency INR)\n\nHence, add an exclusive_use statement and conditional section to remove angio_inr as an option to choose from when the scenario is active for the ir_traj trajectory.\n\n\nTwo angioINRs scenario\nThis was super easy to change with the angio_inr parameter.\n\n\nCheck in-text results 1 and 2\nRan each of the scenarios and found the mean waiting time for each resource across all replications. Results for AngioINR were:\n\nBaseline: 86.99 minutes\nExclusive use: 63.33 minutes\nTwo AngioINR: 52.78 minutes\n\nThis is markedly more than in the paper (exclusive reduces by 6 min, and two angioINRs reduces by 4 min). The median was even more different.\nThis was looking at the waiting time for all patient types though. And the interest of the paper is in stroke (‚ÄúThe elective INR, elective IR and emergency IR pathways are modeled because they utilize resources shared with the stroke pathway.‚Äù Huang et al. (2019))\nThe results have 4 categories: ed, ir, eir, and inr. Hence, it appears ED should be stroke - and indeed, in paper, the stroke pathway begins with a new patient in the emergency department (ED). When filtered just to those patients‚Ä¶\nMean wait times are:\n\nBaseline 307.83 minutes\nExclusive: 292.18 minutes\nTwo AngioINR: 319.94 minutes\n\nMedian wait times for the AngioINR are:\n\nBaseline: 206.53 minutes\nExclusive: 199.03 minutes\nTwo AngioINR: 244.27 minutes\n\nBy the median times, we‚Äôre fairly close to the paper (comparing the averages, it 7 minutes quicker). However, two angioINR is very different, and I‚Äôm a little sceptical as to whether I‚Äôve got it quite right.\n\n\n\n\n\n\nReflections\n\n\n\nDisregarding my attempts to backdate R and the packages, the provided code was actually quite simple to get up and running as a shiny app.\nHowever, it was provided with the article more for that purpose, than to be producing the items in the article, as the base parameters of the model differ, and as there is no code to process and generate the figures and results.\nI‚Äôll keep working in latest R and packages, as current focus of this stage is just to try and reproduce the items. However, it would be good to try and figure out how to successfully backdate R and the packages, as that feels like an essential thing to be able to do, that I just hadn‚Äôt managed to get to the bottom of yet."
  },
  {
    "objectID": "logbook/posts/2024_07_05/index.html#continuing-to-troubleshoot-in-text-results-1-and-2",
    "href": "logbook/posts/2024_07_05/index.html#continuing-to-troubleshoot-in-text-results-1-and-2",
    "title": "Day 3",
    "section": "16.30-16.37, 16.43-16.48, 16.55-16.57: Continuing to troubleshoot in-text results 1 and 2",
    "text": "16.30-16.37, 16.43-16.48, 16.55-16.57: Continuing to troubleshoot in-text results 1 and 2\nI realised that perhaps my issue as in incorrectly assuming that I should set inr_night to 0. There should be one INR person 24h, but because all the staff get put on a schedule, by removing the ‚Äúnight‚Äù person I technically only have someone during day time hours.\nI changed this and re-ran (with timer - can see it took 6.3 minutes).\nThis was definitely a fix - the waiting times now look far closer to what I expected (around 10 minutes rather than 200!). The median results are very small, but looking at the mean results:\n\nBaseline: 13.33 minutes\nExclusive: 8.58 minutes\nTwo AngioINR: 14.86 minutes\n\nHowever, still not quite right‚Ä¶ 4.7 minute reduction for exclusive (should be 6 minutes) and 1.5 minute increase for two machines (should be 4 minute reduction). The exclusive is pretty close, although its not yet clear to me how much it varies between runs, and whether that could be reasonably attributed to be stochasticity or not. Given we‚Äôre comparing fairly small numbers, it is a bit on the fence for me, and wouldn‚Äôt yet say I feel confident in it being successfully reproduced.\nI tried re-running it all to see how much the results differed - and it was by a fair bit actually! Up to about a minute:\n\nBaseline: 13.65 minutes\nExclusive: 9.20 minutes\nTwo AngioINR: 13.61 minutes\n\nHowever, differences are still off the reported - 4.45 minute reduction and 0.01 minute reduction."
  },
  {
    "objectID": "logbook/posts/2024_07_05/index.html#timings",
    "href": "logbook/posts/2024_07_05/index.html#timings",
    "title": "Day 3",
    "section": "Timings",
    "text": "Timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 149\n\n# Times from today\ntimes = [\n    ('09.46', '09.47'),\n    ('09.48', '10.21'),\n    ('10.40', '11.30'),\n    ('11.35', '11.41'),\n    ('11.41', '11.46'),\n    ('11.52', '12.00'),\n    ('12.07', '12.21'),\n    ('13.00', '13.21'),\n    ('13.28', '13.41'),\n    ('13.42', '14.13'),\n    ('14.22', '14.27'),\n    ('14.29', '15.11'),\n    ('15.32', '16.23'),\n    ('16.30', '16.37'),\n    ('16.43', '16.48'),\n    ('16.55', '16.57')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 294m, or 4h 54m\nTotal used to date: 443m, or 7h 23m\nTime remaining: 1957m, or 32h 37m\nUsed 18.5% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_07_10/index.html",
    "href": "logbook/posts/2024_07_10/index.html",
    "title": "Day 6",
    "section": "",
    "text": "Although the figures in the app don‚Äôt match up to the figures in the paper, I wanted to check to see if I could get any more similar results via the app.\nCould put in all the parameters, except number of simulations was limited to 10 (rather than 30) but crashes at that number, so run at their default. However, the outputs don‚Äôt really contain anything usable (e.g.¬†just know most had short wait time, and know median occupancy ratio was around 20%). However, it did make me think that‚Äôs it‚Äôs worth trying the models with the default parameters from the code (rather than the paper), just to see if that happens to look any more similar."
  },
  {
    "objectID": "logbook/posts/2024_07_10/index.html#going-back-to-the-app",
    "href": "logbook/posts/2024_07_10/index.html#going-back-to-the-app",
    "title": "Day 6",
    "section": "",
    "text": "Although the figures in the app don‚Äôt match up to the figures in the paper, I wanted to check to see if I could get any more similar results via the app.\nCould put in all the parameters, except number of simulations was limited to 10 (rather than 30) but crashes at that number, so run at their default. However, the outputs don‚Äôt really contain anything usable (e.g.¬†just know most had short wait time, and know median occupancy ratio was around 20%). However, it did make me think that‚Äôs it‚Äôs worth trying the models with the default parameters from the code (rather than the paper), just to see if that happens to look any more similar."
  },
  {
    "objectID": "logbook/posts/2024_07_10/index.html#running-the-model-with-default-parameters-from-the-code",
    "href": "logbook/posts/2024_07_10/index.html#running-the-model-with-default-parameters-from-the-code",
    "title": "Day 6",
    "section": "09.26-09.32, 09.38-9.40, 10.09-10.12: Running the model with default parameters from the code",
    "text": "09.26-09.32, 09.38-9.40, 10.09-10.12: Running the model with default parameters from the code\nRan baseline model with default parameters from the code (rather than fixing to meet paper).\nInteresting differences, for example, are that it is 1 simulation (nsim=1) but run time 10,000 days (run_t=10000) which works out to about 27 years (which is not far off running 30 simulations each of 1 year).\nHowever, can see this is absolutely wrong! Which is not surprising, but still good we checked.\n\n\n\nFigure 2A with parameters from code"
  },
  {
    "objectID": "logbook/posts/2024_07_10/index.html#in-middle-of-the-above-discussion-with-tom",
    "href": "logbook/posts/2024_07_10/index.html#in-middle-of-the-above-discussion-with-tom",
    "title": "Day 6",
    "section": "09.42-10.00 (in middle of the above): Discussion with Tom",
    "text": "09.42-10.00 (in middle of the above): Discussion with Tom\nShowed Tom the progress and he shared from additional suggestions of things to look into:\n\nCheck calculated inter-arrival times match paper\nCheck distributions are the same\nCheck length of resources (we realised not mentioned in paper - e.g.¬†timeout for appointment)\n\nAlso, reminded that the use of simEd and seed streams is not about getting the same results from the same model with the same parameters, but about controlling change when you change parameters (i.e.¬†so the only thing that changes is that parameter, and not the sampling). However, in this case, set.seed() is sufficient.\nMy additional reflections of things to try from this are to:\n\nVary length of resources\nTry not limiting to just ED patients\nDouble-check if INR procedures only have one room option (whilst IR have two rooms)\nLook at parameters used in the diagram on CLOUDES\n\nAgreed to explore these and anything else can think of, but if then still stuck, at that point to email the authors (once have tried the final figures - resource utilisation and supplementary).\nFelt could then move into evaluation against guidelines - in protocol, had mentioned waiting until after fully wrapped with the model, with rationale that it impacts on code timings, but on reflection, you could argue likewise for influence on timings of that evaluation if you waited before proceeding to it (e.g.¬†waiting for response) and had then had a gap from working on that model and were no longer as familiar."
  },
  {
    "objectID": "logbook/posts/2024_07_10/index.html#check-the-inter-arrival-times",
    "href": "logbook/posts/2024_07_10/index.html#check-the-inter-arrival-times",
    "title": "Day 6",
    "section": "10.31-10.36: Check the inter-arrival times",
    "text": "10.31-10.36: Check the inter-arrival times\n\n# Set in reproduction.qmd\ned_pt = 107700\ninr_pt = 104\neir_pt= 468\nir_pt = 3805\n\n# Set in model.R\nst_pt = 750\nais_pt = 450\necr_pt = 58\n\n# Calculate inter-arrival times (as from model.R)\nyear2min = 525600\nI_ED  = round(year2min/ed_pt)\nI_ST  = round(year2min/st_pt)\nI_AIS = round(year2min/ais_pt)\nI_ECR = round(year2min/ecr_pt)\nI_INR = round(year2min/inr_pt)\nI_EIR = round(year2min/eir_pt)\nI_IR  = round(year2min/ir_pt)\n\n# View calculated inter-arrival times\nprint(c(I_ED, I_ST, I_AIS, I_ECR, I_INR, I_EIR, I_IR))\n\n[1]    5  701 1168 9062 5054 1123  138\n\n\nThese match up with the times from the paper, as in the image below from Huang et al. (2019).\n\n\n\nTable 1"
  },
  {
    "objectID": "logbook/posts/2024_07_10/index.html#check-distributions-and-length-of-resources",
    "href": "logbook/posts/2024_07_10/index.html#check-distributions-and-length-of-resources",
    "title": "Day 6",
    "section": "10.51-12.02, 12.12-12.15: Check distributions and length of resources",
    "text": "10.51-12.02, 12.12-12.15: Check distributions and length of resources\nAs a reminder, this is the set-up of the model, with Figure 1 from Huang et al. (2019). There are several resources, including single plane (angioIR) and biplane (angioINR) angiography suites.\n\n\n\nFigure 1\n\n\nEmergency arrival (potential stroke) patients:\n\nStart as emergency arrival (new_patient_traj)\nBecome either a stroke patient (stroke_traj) or non-stroke patient (nonstroke_traj)\nThe stroke patients will then become either AIS (acute ischaemic stroke) (ais_traj) or non-AIS (timeout then leave)\nThe AIS patients will then become either ECR (endovascular clot retrieval) (ecr_traj) or TPA (tissue plasminogen activator) only (timeout then leave)\n\nOther patients (pathways included as they share resources with stroke pathway):\n\nInterventional radiology patients (ir_traj)\nEmergency interventional radiology patients (eir_traj)\nInterventional neuroradiology patients (inr_traj)\n\n\nEmergency arrival patient sampling / distributions / length\n\nModel\nEmergency arrivals (new_patient_traj):\n\nadd_generator(\"pt_ed\", new_patient_traj, function() rpois(1, I_ED) )\n\nWhere I_ED  = round(year2min/ed_pt) = 5\n\nTime with ed_staff: timeout(function() rnorm(1, 20,10)) (sample 1 from normal distribution with mean 20 and sd 10)\nProbability of stroke: sample(1:2, 1, prob = c(PROB_STROKE, (1-PROB_STROKE) )\n\nWhere PROB_STROKE = st_pt / ed_pt, which is 750/107700=0.006963788 (so probability 0.007, or 0.7%)\nInterestingly, the inter-arrival time calculated for stroke (I_ST  = round(year2min/st_pt)) is not used, and instead, the arrival of stroke patients is based on this probability sampling\n\n\nNon-stroke patients (nonstroke_traj):\n\nProbability of discharge vs ct review: sample(1:2, 1, prob = c(.9, .1)) so 0.9 or 90% leave, and then 10% get CT review before leave\nDischarge: timeout(1)\nCT review: timeout(20)\n\nStroke patients (stroke_traj):\n\nTime with stroke doctor: timeout(function() rnorm(1, 30, 10))\nCT time: timeout(function() rnorm(1, 20,10))\nProbability of AIS: sample(1:2, 1, prob = c(PROB_AIS, (1-PROB_AIS)))\n\nWhere PROB_AIS = ais_pt / st_pt = 450/750 = 0.6 (or 60%)\n\nNot ais: timeout(1)\n\nAIS patients:\n\nProbability of ECR: sample(1:2, 1, prob = c(PROB_ECR, (1-PROB_ECR))\n\nWhere PROB_ECR = ecr_pt / ais_pt = 58/450 = 0.1288889 (probability 0.13, or 13%)\n\nTPA only: timeout(1)\n\nECR patients:\n\nAngioINR time (uses angio_inr, inr, and 3 angio_staff): timeout(function() rnorm(1, 120,60))\n\n\n\nPaper\n‚ÄúThe stroke pathway begins with a new patient in the Emergency Department (ED) and ends with the patient ‚Äúseizing‚Äù an angioINR, an INR and angio staff which represents nurses and technologists. The patient must proceed through a sequence of events chronologically as follows: triage in ED, assessment by the stroke team, CT imaging, assessment for ECR eligibility and lastly, acquiring ECR resources (Figure 1). The decision to proceed to the next event is probabilistic and is acquired from logged data from a Comprehensive Stroke Service in Melbourne, Australia, between 2016 and 17 (Table 1).‚ÄùHuang et al. (2019)\nAs it stands, Table 1 just contains the number of resources and patients - but, from this paragraph, it appears it might previously have included some of these probabilities.\nI had a look online to see if I could find any pre-prints. I came across a poster abstract, but otherwise nothing that could help elucidate this. I also looked for the data from the Comprehensive Stroke Service (although I couldn‚Äôt easily come across anything with patient counts, and wasn‚Äôt certain this information would definitely be public, so limited search).\nI looked the the model on CLOUDES, and this had different parameters (although this might just be illustrative). But, for example:\n\nED arrivals - poisson with IAT 10 and 2 entities per arrival- similar to model (poisson with IAT 5 and 1 entity per arrival)\nED triage - normal mean 15 stdev 5 - differs from model (mean 20 sd 10)\nProbability stroke 0.7 (and 99.3 leave) - same as model\nTime with stroke doctor normal mean 30 sd 10 - same as model\nCT normal mean 20 sd 10 - same as model\nAIS probability 15 (and 85 leave) and then LVO probability 60 (and 40 leave) (which is described as probabilitiy true AIS) - differs from model (simply, from those who received the CT, 60% AIS and 40% exit)\nECR probabiltiy 15 (and 85 leave) - differs from model (13% ECR)\nAngioINR normal mean 120 sd 60 - same as model\n\nHowever, several of them are the same, so it seems it would be worth running the model with those parameters.\n\n\n\nOther patients\n\nModel\nInterventional radiology patients (ir_traj):\n\nadd_generator(\"pt_ir\", ir_traj, function() rpois(1, I_IR) )\n\nWhere I_IR  = round(year2min/ir_pt) = 138\n\nAngio staff time: timeout(function() rnorm(1, 20,10))\nAngioINR/IR time (uses angio_inr or angio_ir, plus ir and 3 angio_staff): timeout(function() rnorm(1, 60,30))\n\nEmergency interventional radiology patients (eir_traj):\n\nadd_generator(\"pt_eir\", eir_traj, priority = 1, function() rpois(1, I_EIR) )\n\nWhere I_EIR = round(year2min/eir_pt) = 1123\n\nAngio staff time: timeout(function() rnorm(1, 20,10))\nAngioINR/IR time (uses angio_inr or angio_ir, plus ir and 3 angio_staff): timeout(function() rnorm(1, 60,30))\n\nInterventional neuroradiology patients (inr_traj): * add_generator(\"pt_inr\", inr_traj, function() rpois(1, I_INR) ) * Where I_INR = round(year2min/inr_pt) = 5054 * Angio staff time: timeout(function() rnorm(1, 20,10)) * AngioINR time (uses angio_inr, inr and 3 angio_staff): timeout(function() rnorm(1, 60,30))\n\n\nCLOUDES\n\nNon-emergency IR arrivals - poisson IAT 120 1 entity - differs from model (138)\nEmergency IR arrivals - poisson IAT 1120 1 entity - differs from model (1123)\nNon-emergency INR arrivals - poisson IAT 5040 1 entity - differs from model (5040)\nTime with angio staff: normal mean 20 sd 10 - same as model\nRouting to rooms: non-emergency IR check for angio room for IR (which chooses between IR and INR based on shortest queue), non-emergency go into INR queue, doesn‚Äôt have route for emergency IR - differs from model but looks like this is due to limitation of software in only letting you choose one patient type or all patients\nAngio INR time - normal mean 120 sd 60 - differs from model (mean 60 sd 30) but this again might be limitation of software (only allowing one time length regardless of patient type)\nAngio IR time - normal mean 60 sd 30 - same as model\n\n\n\n\nReflections from this\nSome of these differences appear to be rounding/simplifying numbers, or limitations of the CLOUDES software. However, some are more different. My logic here is that the model code we have is for the app and some parameters differed to the paper - so I‚Äôm anticipating it‚Äôs possible that some of these other parameters may have differed too (but cannot confirm due to them not being reported in the paper). However, if there‚Äôs a chance that the CLOUDES model was based on the paper parameters (rather than app), there‚Äôs a chance it could help us match up? This seems unlikely though (given it accompanies the app).\nHowever, the only one of real interest I think (that is not simplification or limitation) is the difference in ED triage time."
  },
  {
    "objectID": "logbook/posts/2024_07_10/index.html#varying-ed-triage-length",
    "href": "logbook/posts/2024_07_10/index.html#varying-ed-triage-length",
    "title": "Day 6",
    "section": "13.00-13.15: Varying ED triage length",
    "text": "13.00-13.15: Varying ED triage length\nI modified the model.R so I could easily change the ED triage mean and SD, then ran a scenario where these were 15 and 5. However, that didn‚Äôt make much difference.\n\n\n\nFigure 2A with ED triage from CLOUDES"
  },
  {
    "objectID": "logbook/posts/2024_07_10/index.html#double-check-category-being-presented",
    "href": "logbook/posts/2024_07_10/index.html#double-check-category-being-presented",
    "title": "Day 6",
    "section": "13.16-13.28: Double check category being presented",
    "text": "13.16-13.28: Double check category being presented\nI‚Äôm pretty sure I‚Äôm presenting the right category (ED), but I looked at presenting wait times from patients in any category, or in each of the other categories.\nI temporarily removed the filtering from run_model() and then ran:\nbaseline &lt;- run_model(seed = SEED)\n\np1 &lt;- create_plot(baseline,\n                  group=\"resource\",\n                  title=\"All patients\")\np2 &lt;- create_plot(baseline %&gt;% filter(category == \"ed\"),\n                  group=\"resource\",\n                  title=\"ED\")\np3 &lt;- create_plot(baseline %&gt;% filter(category == \"ir\"),\n                  group=\"resource\",\n                  title=\"IR\")\np4 &lt;- create_plot(baseline %&gt;% filter(category == \"eir\"),\n                  group=\"resource\",\n                  title=\"EIR\")\np5 &lt;- create_plot(baseline %&gt;% filter(category == \"inr\"),\n                  group=\"resource\",\n                  title=\"INR\")\n\n# Arrange in a single figure\nggarrange(p1, p2, p3, p4, p5, nrow=1, ncol=5,\n          common.legend=TRUE, legend=\"bottom\")\nggsave(\"fig2a_categories.png\", width=18)\nThis supports that ED is the correct choice (the only other similar is EIR but logically, it does still make sense to be ED, and it doesn‚Äôt happen to be that EIR is a great match either, just similar).\n\n\n\nFigure 2A categories"
  },
  {
    "objectID": "logbook/posts/2024_07_10/index.html#double-check-inr-room-options",
    "href": "logbook/posts/2024_07_10/index.html#double-check-inr-room-options",
    "title": "Day 6",
    "section": "13.39-13.43: Double-check INR room options",
    "text": "13.39-13.43: Double-check INR room options\nLooks right compared with paper, can‚Äôt spot any issues"
  },
  {
    "objectID": "logbook/posts/2024_07_10/index.html#vary-length-of-resources-to-try-to-engineer-results",
    "href": "logbook/posts/2024_07_10/index.html#vary-length-of-resources-to-try-to-engineer-results",
    "title": "Day 6",
    "section": "13.44-14.10, 14.15-14.45: Vary length of resources to try to engineer results",
    "text": "13.44-14.10, 14.15-14.45: Vary length of resources to try to engineer results\nI can see what looks wrong in each of the figures and so, one option, is to see if I could easily attempt to engineer the results by varying the parameters slightly, to see what might make it look similar.\nLooking at Figure 2A as an example:\n\nI have lower AngioINR queue density and no visible angio staff queues (should be queues)\n\nCould try increasing the number of patients accessing the angioINR\n\nThere are INR queues (when should be none)\n\nCould try either having ED patients not use INR, or having more INR availability\n\nCT, ED staff and stroke doctor queues are similar\n\nI ran a few quick models (3 replications), just to see what comes out.\nrun_model(nsim=3, seed = SEED, ed_pt = 107700*2)\nDoubled the number of emergency department arrivals. This increased angio INR queue but moved CT and ED staff away from desired. Interestingly, no impact on angio staff.\n\n\n\nDouble ED\n\n\nrun_model(nsim=3, seed = SEED, angio_staff = 3)\nReduce number of angio staff to 3 during day, which had large impact on angioINR and INR queues, but still no visible angio staff queue.\n\n\n\nHalve angio daytime staff\n\n\nDouble ED AND less staff:\n\n\n\nDouble ED AND less staff\n\n\nLooking at model.R, these results are coming from the simpy resource itself, so this shouldn‚Äôt be due to any issues with the calculation of angio staff resource use.\nSome extra suggestions from quick chat with Tom:\n\nIncrease length of AngioINR appointment\nLook at the utilisation (e.g.¬†angio staff utilisation)\n\nHowever, if I just plot angio_staff (without the group by resource), I can see it! It just doesn‚Äôt appear in the other plot. I then realised that this is because the angio_staff and INR lines completely overlap. If we remove INR from the plot, it starts to look a bit more similar.\nHence, it seems that actually the main difference to the paper is just the angioINR queue."
  },
  {
    "objectID": "logbook/posts/2024_07_10/index.html#looking-into-figure-2c-and-3c-and-getting-in-text-result-2",
    "href": "logbook/posts/2024_07_10/index.html#looking-into-figure-2c-and-3c-and-getting-in-text-result-2",
    "title": "Day 6",
    "section": "16.03-16.19: Looking into Figure 2C and 3C, and getting in-text result 2!",
    "text": "16.03-16.19: Looking into Figure 2C and 3C, and getting in-text result 2!\nFigure 2C has double the machines but, in the paper, they have no change in angio staff levels, whilst I find that increases. That makes sense - with plenty of machines, the bottleneck is now on having the staff for those machines.\nI realised then, from reading back on the paper, that I should have replaced an angioIR machine with an angioINR machine (and not just add an extra angioINR machine).\n\n‚ÄúSecond, in the ‚Äútwo angioINRs‚Äù scenario, the angioIR is replaced with an angioINR, doubling angiography availability for ECR patients.‚Äù Huang et al. (2019)\n\nI changed this in reproduction.qmd (angio_inr=2, angio_ir=0) and re-ran the models for Figure 2 and 3. This fixed the (C) part of those figures to be more similar to the paper.\nThis then resolved in-text result 3, with a 4.3 minute reduction in the queue length (which is very similar to ‚Äú4 min less‚Äù). Hence, can consider that reproduced at this point!\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 975\n\n# Times from today\ntimes = [\n    ('09.18', '09.25'),\n    ('09.26', '09.32'),\n    ('09.38', '09.40'),\n    ('09.42', '10.00'),\n    ('10.09', '10.12'),\n    ('10.31', '10.36'),\n    ('10.51', '12.02'),\n    ('12.12', '12.15'),\n    ('13.00', '13.15'),\n    ('13.16', '13.28'),\n    ('13.39', '13.43'),\n    ('13.44', '14.10'),\n    ('14.15', '14.45'),\n    ('16.03', '16.19')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 218m, or 3h 38m\nTotal used to date: 1193m, or 19h 53m\nTime remaining: 1207m, or 20h 7m\nUsed 49.7% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_07_10/index.html#trying-to-raise-the-angioinr-queues",
    "href": "logbook/posts/2024_07_10/index.html#trying-to-raise-the-angioinr-queues",
    "title": "Day 6",
    "section": "16.25-17.00: Trying to raise the angioINR queues",
    "text": "16.25-17.00: Trying to raise the angioINR queues\nTried changing length of angio appointments for all non-ED patients to the same as ED patients - definitely not right!\n\n\n\nLonger angio\n\n\nShorterning the ED angio appointments to the non-ED length is also not helpful.\n\n\n\nShorter ED angio\n\n\nThen I ran through a bunch of different seeds, to see if that also could explain it. Some do come a little closer than others‚Ä¶ though this was only five replications. Should probably repeat this exercise, but with 30 replications!\n\n\n\nDifferent seeds\n\n\nplot_list &lt;- list()\ni &lt;- 0\nfor (s in seq(0, 800, 50)) {\n  i &lt;- i + 1\n  baseline &lt;- run_model(nsim=5, seed = s)\n  plot_list[[i]] &lt;- create_plot(baseline, group=\"resource\", title=\"\")\n}\nggarrange(plotlist=plot_list, common.legend=TRUE, legend=\"bottom\")\nggsave(\"../logbook/posts/2024_07_10/fig2a_5rep_diffseeds.png\", width=20, height=20)"
  },
  {
    "objectID": "logbook/posts/2024_07_10/index.html#timings",
    "href": "logbook/posts/2024_07_10/index.html#timings",
    "title": "Day 6",
    "section": "Timings",
    "text": "Timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 975\n\n# Times from today\ntimes = [\n    ('09.18', '09.25'),\n    ('09.26', '09.32'),\n    ('09.38', '09.40'),\n    ('09.42', '10.00'),\n    ('10.09', '10.12'),\n    ('10.31', '10.36'),\n    ('10.51', '12.02'),\n    ('12.12', '12.15'),\n    ('13.00', '13.15'),\n    ('13.16', '13.28'),\n    ('13.39', '13.43'),\n    ('13.44', '14.10'),\n    ('14.15', '14.45'),\n    ('16.03', '16.19'),\n    ('16.25', '17.00')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 253m, or 4h 13m\nTotal used to date: 1228m, or 20h 28m\nTime remaining: 1172m, or 19h 32m\nUsed 51.2% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_07_08/index.html",
    "href": "logbook/posts/2024_07_08/index.html",
    "title": "Day 4",
    "section": "",
    "text": "Note\n\n\n\nAdd seeds, got in-text result 1, working on Figure 2. Total time used: 13h 10m (32.9%)"
  },
  {
    "objectID": "logbook/posts/2024_07_08/index.html#continuing-on-in-text-results-1-and-2",
    "href": "logbook/posts/2024_07_08/index.html#continuing-on-in-text-results-1-and-2",
    "title": "Day 4",
    "section": "09.14-09.17, 09.22-09.24, 09.30-09.35: Continuing on in-text results 1 and 2",
    "text": "09.14-09.17, 09.22-09.24, 09.30-09.35: Continuing on in-text results 1 and 2\nRe-ran twice more to see again how much variation we get between runs, and how likely that could attribute for the difference against the paper. We saw-\n\n\n\n\n\n\n\n\n\n\n\nOutput\nResult 1 (Day 3)\nResult 2 (Day 3)\nResult 3 (Today)\nResult 4 (Today)\nPaper\n\n\n\n\nBaseline\n13.33 minutes\n13.65 minutes\n14.15 minutes\n14.09 minutes\n-\n\n\nExclusive\n8.58 minutes (4.75 reduction)\n9.20 minutes (4.45 reduction)\n8.79 minutes (5.36 reduction)\n8.05 minutes (6.04 reduction)\n6 minute reduction from baseline\n\n\nTwo AngioINR\n14.86 minutes (1.53 increase)\n13.61 minutes (0.04 reduction)\n14.37 minutes (0.22 increase)\n14.04 minutes (0.05 reduction)\n4 minute reduction from baseline\n\n\n\nBased on this, it‚Äôs reasonable to assume that a 6 minute reduction can be observed within the variation of model runs (in-text result 1), but that the two angioINR scenario is not matching up.\n\n\n\n\n\n\nReflections\n\n\n\nEnvironment used does not match up to paper - paper use Simmer version 4.1.0, and otherwise, other versions of packages and of R being used are more recent than publication. It is unlikely that differences in results are due to this (although not impossible). Note trying to revert the environment to older versions as a possible troubleshooting strategy if issues persist, but not yet, due to major challenges found in trying to do so prior."
  },
  {
    "objectID": "logbook/posts/2024_07_08/index.html#adding-seeds",
    "href": "logbook/posts/2024_07_08/index.html#adding-seeds",
    "title": "Day 4",
    "section": "09.50-10.49, 11.02-11.05, 11.13-11.14: Adding seeds",
    "text": "09.50-10.49, 11.02-11.05, 11.13-11.14: Adding seeds\nBased on this tutorial, add seeds to the model. This is because the result was only returned by certain runs of the model and not others, so want to add seeds now so can give a seed for which the result is reproduced. I installed simEd - renv::install(\"simEd\") and add to DESCRIPTION and renv::snapshot() - and then made the following changes to the model:\n\nlibrary(simEd)\nInput seed to function which becomes SEED, then set.seed(SEED+i) within model replications\nSampling functions changed from r to v - i.e.¬†rpois() to vpois(), with incremental stream numbers\n\nI tried running baseline, but it took a long time - after 6 minutes, it was still running (which is normally how long the whole script takes). I interrupted it and it returned Error : object 'shifts' not found. However, no change has been made to shifts code. I ran a short section of code practicing sampling and this worked fine:\nlibrary(simEd)\n\ned_pt = 107000\nyear2min = 525600\nI_ED  = round(year2min/ed_pt)\n\nset.seed(5)\nvpois(10, I_ED, stream=1)\n\nset.seed(3)\nvpois(10, I_ED, stream=1)\n\nset.seed(5)\nvpois(10, I_ED, stream=1)\nI then tried running it with 3 replications instead of 30 (baseline &lt;- run_model(nsim=3, seed=100)), and that ran fine, so it appears that introducing this library just slowed down the model alot, as 3 replications could complete in 40 seconds.\nI looked into changing the lapply() in model.R to a parallel version:\n\nparLapply requires you to specify every variable to be included, plus additional lines of code to set up and close clusters\nmcapply() just requires you to change lapply\n\nHence, I tried mcapply, but it returned Error: external pointer is not valid, which was resolved based on this post by adding wrap(). However, learnt that mclapply wouldn‚Äôt work on Windows. Moreover, it still took a fair while to run (testing with 30 replications, it‚Äôs still going at 4 minutes).\nAs such, removed simEd from model.R and environment and returned to rpois(), and instead just set a simple seed without controlling streams. The time for this to run was as per usual, which was fab. I ran the baseline model twice with the same seed and compared the results, and it came out looking (by eye, at the processed results) identical.\nI therefore ran baseline and exclusive with three different starter seeds, and the seed 200 came out closest to the paper -\n\nBaseline: 13.96 minutes\nExclusive: 8.12 minutes\nDifference: 5.84 minutes\n\nHence, I feel we can mark in-text result 1 as reproduced at this time (11.14), with starter seed of 200.\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 443\n\n# Times from today\ntimes = [\n    ('09.14', '09.17'),\n    ('09.22', '09.24'),\n    ('09.30', '09.35'),\n    ('09.50', '10.49'),\n    ('11.02', '11.05'),\n    ('11.13', '11.14')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 73m, or 1h 13m\nTotal used to date: 516m, or 8h 36m\nTime remaining: 1884m, or 31h 24m\nUsed 21.5% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_07_08/index.html#working-on-figure-2",
    "href": "logbook/posts/2024_07_08/index.html#working-on-figure-2",
    "title": "Day 4",
    "section": "11.15-12.30, 13:15-13.50, 13.55-14.55: Working on Figure 2",
    "text": "11.15-12.30, 13:15-13.50, 13.55-14.55: Working on Figure 2\nFigure 2 uses the results from the scenarios above but creates plots where:\n\nX axis is wait time in minutes (on a non-linear scale)\nY axis is standardised density of patients in queue, from 0 to 1 (on a non-linear scale)\n\ni.e.¬†‚ÄúProbability density of patients who are waiting standardised to patients who are not waiting‚Äù\ni.e.¬†‚ÄúTo facilitate graphical and descriptive comparison across models, we express waiting times as relative probabilities of waiting a given amount of time, compared to not waiting at all. Since most patients accessed services without waiting, wait time densities could be directly compared across simulations after this normalization.‚Äù\n\n\nIt‚Äôs not immediately clear exactly what this means, but I‚Äôll start with creating a density plot of waiting times for one of the resources. First though, I add some code to save the model results to CSV files so that we don‚Äôt have to re-run the model each time (since with seeds added, it should now come out the same each time anyway). I initially saved these with write.csv() but it was too slow, so then (based on this tutorial), I switched to data.table::fwrite() (‚Äúfast CSV writer‚Äù), which was much much better! Hence, used fread() to import (as should also be quicker, based on this tutorial).\nI then created a basic density plot with ggplot with ED AngioINR untransformed wait times.\nbase_angio &lt;- res_base %&gt;% filter(category == \"ed\") %&gt;% filter(resource == \"angio_inr\")\np &lt;- ggplot(base_angio, aes(x = wait_time)) +\n  geom_density()\nggsave(path_fig2a)\np\n\n\n\nFigure 2A raw wait times\n\n\n\nY axis\nI played around with various transformations, as it wasn‚Äôt immediately clear to me how they had stretched the y axis, including creating custom functions, transforming the data directly, and trying out default transform options. I eventually stumbled across scale_y_continuous(transform=\"sqrt\"), which matched up to the axis in the paper.\n\n\nStandardising the density\nI played around with a few different transformation as I tried to work out what they meant by standardised density of patients in queue. Whilst converting raw wait times to probabilities, I noticed a bunch of ever so slightly negative wait times, but given these are very small (i.e.¬†0.0000000‚Ä¶), I am not concerned.\nOne thing I tried was converting each wait time into a probability of that wait time (e.g.¬†rounding each to 2dp, then 0 wait time = probability 0.68).\n# Filter to just AngioINR for ED and round wait times to 2dp\nbase_angio &lt;- res_base %&gt;%\n  filter(category == \"ed\", resource == \"angio_inr\") %&gt;%\n  select(wait_time)\n\n# Round to 2dp\nbase_angio$wait_time &lt;- round(base_angio$wait_time, 2)\n\n# Convert raw wait times into probability of waiting that long given all\n# wait times observed\nprob_wait &lt;- base_angio %&gt;%\n  group_by(wait_time) %&gt;%\n  summarise(count = n()) %&gt;%\n  mutate(probability = count / sum(count)) %&gt;%\n  select(wait_time, probability)\n\nggplot(prob_wait, aes(x=wait_time, y=probability)) + geom_line() + geom_point()\nHowever, that really didn‚Äôt look quite right.\n\n\n\nFigure 2A wrong transformation\n\n\nLooking at the curve with the raw wait times, the shape of the curve is more similar to the paper, just with different y axis and stretched. Revisiting the paper description, it is the ‚Äúrelative probabilities of waiting a given amount of time, compared to not waiting at all‚Äù. So, it‚Äôs not just the relative probability of waiting a given amount of time, compared to any other time.\nI created a plot where the waiting times were normalised in such a way that the values range from 0 to 1, which starts to look a bit more similar to the paper -\n# Filter to just AngioINR for ED and round wait times to 2dp\nbase_angio &lt;- res_base %&gt;%\n  filter(category == \"ed\", resource == \"angio_inr\")\n\n# Set negative wait times to 0\nbase_angio$wait_time[base_angio$wait_time &lt; 0] &lt;- 0\n\n# Create the density data\ndensity_data &lt;- density(base_angio$wait_time)\n\n# Normalize the density values\nnormalized_density &lt;- density_data$y / max(density_data$y)\n\n# Create a data frame with the normalized density values\ndensity_df &lt;- data.frame(x = density_data$x, y = normalized_density)\n\n# Plot using ggplot2\nggplot(density_df, aes(x = x, y = y)) +\n  geom_line() +\n  scale_y_continuous(transform=\"sqrt\")\nggsave(path_fig2a)\n\n\n\nFigure 2A scaled to 0 to 1\n\n\nI then tried creating a dataframe of counts for each wait time, then calculated probability based on number of people with no wait time. However, many were tiny (as count e.g.¬†1 of wait time 0.00000000000002842171). Tried it with rounding first. However, it is still then the same, as most are just 0, and then e.g.¬†1 wait time 0.2, 3 wait time 0.5.\n# Filter to just AngioINR for ED and round wait times to 2dp\nbase_angio &lt;- res_base %&gt;%\n  filter(category == \"ed\", resource == \"angio_inr\")\n\n# Set negative wait times to 0\nbase_angio$wait_time[base_angio$wait_time &lt; 0] &lt;- 0\n\n# Round everything to 1dp\nbase_angio$wait_time &lt;- round(base_angio$wait_time, 1)\n\n# Get probability of no wait time\nn_zero = length(which(base_angio$wait_time == 0))\nprob_zero = n_zero / nrow(base_angio)\n\n# Convert dataframe to counts of each wait time\nwait_df = base_angio %&gt;%\n  group_by(wait_time) %&gt;%\n  summarise(count=n())\nI tried transforming by the density of 0 (density_data$y[which.min(abs(density_data$x - 0))]) but that worked out to just be the same as max(density_data$y), since 0 has the max density.\nI tried transforming the x axis, which also appears to be a sqrt transformation, although this has an issue of introducing Inf values and losing where x=0 and density=1. I explored a few different ways of doing this transformation to see if anything helps"
  },
  {
    "objectID": "logbook/posts/2024_07_08/index.html#research-into-transformations",
    "href": "logbook/posts/2024_07_08/index.html#research-into-transformations",
    "title": "Day 4",
    "section": "15.10-15.30: Research into transformations",
    "text": "15.10-15.30: Research into transformations\nAs I‚Äôm struggling with these transformations - to the x axis, and to the probability density function. As such, it seems a good idea to do a bit more research into these and what exactly they are doing, to see if that helps.\n\nSquare root axis transformation\nI read a few articles and looked at the documentation for the square root transformation, and understand that this simply applying the sqrt() function.\nYou get the same graph if you do this:\ndensity_df %&gt;%\n  mutate(x_sqrt = sqrt(x)) %&gt;%\n  ggplot(aes(x=x_sqrt, y=y)) + geom_line() + xlim(0, sqrt(200)) + scale_y_continuous(transform=\"sqrt\")\nThe only difference is the x axis labels - when we use the ggplot axis transformation, it keeps the old labels to maintain interpretation of the original data.\n\n\nDensity functions\nA probability density function is used to describe a continuous distribution. It can be used to find the likelihood of values of a continuous random variable.\nggplot::geom_density() is described as plotting a smoothed version of the histogram."
  },
  {
    "objectID": "logbook/posts/2024_07_08/index.html#returning-to-figure-2",
    "href": "logbook/posts/2024_07_08/index.html#returning-to-figure-2",
    "title": "Day 4",
    "section": "15.31-16.55: Returning to Figure 2",
    "text": "15.31-16.55: Returning to Figure 2\nI add the sqrt x axis transformation to the basic density plot, and suddenly got a result that looked alot like the article! The only differences are the range of each axis, and the min/max values for y (ranges from 0 to 0.2‚Ä¶)\n# Filter to just AngioINR for ED and round wait times to 2dp\nbase_angio &lt;- res_base %&gt;%\n  filter(category == \"ed\", resource == \"angio_inr\")\n\n# Set negative wait times to 0\nbase_angio$wait_time[base_angio$wait_time &lt; 0] &lt;- 0\n\nggplot(base_angio, aes(x = wait_time)) +\n  geom_density() +\n  scale_y_continuous(transform=\"sqrt\") +\n  scale_x_continuous(transform=\"sqrt\")\n.png\nI tried out using previous transforms but they didn‚Äôt look right. Then I came across this stack Overflow post which suggested you can scale the density estimate to a maximum of one by inputting ..scaled... This is the computed ..scaled.. value from geom_density() which provides the density estimate scaled to a maximum of 1. From the documentation, can see that ..scaled.. has been replaced with after_stat(scaled).\nThis is however assuming that scaling to 1 is the same as scaling by probability of 0 wait time (which is at least true in this case, as we saw above).\n# Filter to just AngioINR for ED and round wait times to 2dp\nbase_angio &lt;- res_base %&gt;%\n  filter(category == \"ed\", resource == \"angio_inr\")\n\n# Set negative wait times to 0\nbase_angio$wait_time[base_angio$wait_time &lt; 0] &lt;- 0\n\n# Create the plot, scaling the density estimate to a maximum of 1\nggplot(base_angio, aes(x=wait_time, y=after_stat(scaled))) +\n  geom_density() +\n  scale_y_continuous(transform=\"sqrt\") +\n  scale_x_continuous(transform=\"sqrt\")\n\n\n\nFigure 2A example 5\n\n\nI tried adding all the resources in to the plots, and converting it into a function so I can apply it to all three dataframes. To easily show the plots side-by-side with a shared legend, I installed the package ggpubr.\nInstallation of ggpubr failed with message ERROR: configuration failed for package ‚Äònloptr‚Äô. It suggested I install cmake so, as prompted, I ran sudo apt install cmake. This then installed fine.\nCreating the plots and making various tweaks to the plotting and appearance, we‚Äôre getting a bit closer to the paper.\ncreate_plot &lt;- function(df, title, xlim=c(0, 200)) {\n  #' Create sub-plots for Figure 2A\n  #' \n  #' @param df Dataframe with wait times across replications\n  #' @param xlim Tuple with limits for x axis\n\n  # Filter to just ED\n  base_angio &lt;- df %&gt;%\n    filter(category == \"ed\")\n  \n  # Set negative wait times to 0\n  base_angio$wait_time[base_angio$wait_time &lt; 0] &lt;- 0\n  \n  # Create the plot, scaling the density estimate to a maximum of 1\n  ggplot(base_angio, aes(x = wait_time,\n                         colour = resource,\n                         y = after_stat(scaled))) +\n    geom_density() +\n    # Apply square transformation to each axis, removing x points beyond limits\n    scale_y_continuous(transform = \"sqrt\") +\n    scale_x_continuous(transform = \"sqrt\",\n                       breaks = scales::breaks_width(50),\n                       limits = xlim,\n                       oob = scales::censor,\n                       guide = guide_axis(angle=45)) +\n    # Titles and styling\n    ggtitle(title) +\n    xlab(\"\") +\n    ylab(\"\") +\n    theme_bw(base_size=10)\n}\n\np1 &lt;- create_plot(res_base, title=\"Baseline\")\np2 &lt;- create_plot(res_exc, title=\"Exclusive-use\", xlim=c(0, 250))\np3 &lt;- create_plot(res_two, title=\"Double angio INRs\")\nggarrange(p1, p2, p3, nrow=1, common.legend=TRUE, legend=\"bottom\", labels=c(\"A\", \"B\", \"C\"))\nggsave(path_fig2a)\n\n\n\nFigure 2A example 6"
  },
  {
    "objectID": "logbook/posts/2024_07_08/index.html#timings",
    "href": "logbook/posts/2024_07_08/index.html#timings",
    "title": "Day 4",
    "section": "Timings",
    "text": "Timings\n\n# Minutes used prior to today\nused_to_date = 443\n\n# Times from today\ntimes = [\n    ('09.14', '09.17'),\n    ('09.22', '09.24'),\n    ('09.30', '09.35'),\n    ('09.50', '10.49'),\n    ('11.02', '11.05'),\n    ('11.13', '11.14'),\n    ('11.15', '12.30'),\n    ('13.15', '13.50'),\n    ('13.55', '14.55'),\n    ('15.10', '15.30'),\n    ('15.31', '16.55')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 347m, or 5h 47m\nTotal used to date: 790m, or 13h 10m\nTime remaining: 1610m, or 26h 50m\nUsed 32.9% of 40 hours max"
  },
  {
    "objectID": "logbook/logbook.html",
    "href": "logbook/logbook.html",
    "title": "Logbook",
    "section": "",
    "text": "These diary entries record daily progress in reproduction of the study, providing a transparent and detailed record of work.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay 7\n\n\n\n\n\n\nreproduce\n\n\n\n\n\n\n\n\n\nJul 11, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 6\n\n\n\n\n\n\nreproduce\n\n\n\n\n\n\n\n\n\nJul 10, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 5\n\n\n\n\n\n\nsetup\n\n\nreproduce\n\n\n\n\n\n\n\n\n\nJul 9, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 4\n\n\n\n\n\n\nreproduce\n\n\n\n\n\n\n\n\n\nJul 8, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 3\n\n\n\n\n\n\nreproduce\n\n\n\n\n\n\n\n\n\nJul 5, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 2\n\n\n\n\n\n\nsetup\n\n\nscope\n\n\nreproduce\n\n\n\n\n\n\n\n\n\nJul 4, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 1\n\n\n\n\n\n\nsetup\n\n\n\n\n\n\n\n\n\nJul 3, 2024\n\n\nAmy Heather\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "evaluation/reflections.html",
    "href": "evaluation/reflections.html",
    "title": "Reflections",
    "section": "",
    "text": "Please note: This is a template page and has not yet been completed\nThis page contains reflections on the facilitators and barriers to this reproduction, as well as a full list of the troubleshooting steps taken to reproduce this work."
  },
  {
    "objectID": "evaluation/reflections.html#what-would-have-helped-facilitate-this-reproduction",
    "href": "evaluation/reflections.html#what-would-have-helped-facilitate-this-reproduction",
    "title": "Reflections",
    "section": "What would have helped facilitate this reproduction?",
    "text": "What would have helped facilitate this reproduction?"
  },
  {
    "objectID": "evaluation/reflections.html#full-list-of-troubleshooting-steps",
    "href": "evaluation/reflections.html#full-list-of-troubleshooting-steps",
    "title": "Reflections",
    "section": "Full list of troubleshooting steps",
    "text": "Full list of troubleshooting steps\n\n\n\n\n\n\nView list\n\n\n\n\n\nTroubleshooting steps are grouped by theme, and the day these occurred is given in brackets at the end of each bullet."
  },
  {
    "objectID": "evaluation/badges.html",
    "href": "evaluation/badges.html",
    "title": "Journal badges",
    "section": "",
    "text": "Please note: This is a template page and has not yet been completed, so all criteria are currently set as unmet (‚ùå)\nThis page evaluates the extent to which the author-published research artefacts meet the criteria of badges related to reproducibility from various organisations and journals.\nCaveat: Please note that these criteria are based on available information about each badge online, and that we have likely differences in our procedure (e.g.¬†allowed troubleshooting for execution and reproduction, not under tight time pressure to complete). Moreover, we focus only on reproduction of the discrete-event simulation, and not on other aspects of the article. We cannot guarantee that the badges below would have been awarded in practice by these journals."
  },
  {
    "objectID": "evaluation/badges.html#criteria",
    "href": "evaluation/badges.html#criteria",
    "title": "Journal badges",
    "section": "Criteria",
    "text": "Criteria\n\n\nCode\nfrom IPython.display import display, Markdown\nimport numpy as np\nimport pandas as pd\n\n# Criteria and their definitions\ncriteria = {\n    'archive': 'Stored in a permanent archive that is publicly and openly accessible',\n    'id': 'Has a persistent identifier',\n    'license': 'Includes an open license',\n    'relevant': '''Arefacts are relevant to and contribute to the article's results''',\n    'complete': 'Complete set of materials shared (as would be needed to fully reproduce article)',\n    'structure': 'Artefacts are well structured/organised (e.g. to the extent that reuse and repurposing is facilitated, adhering to norms and standards of research community)',\n    'documentation_sufficient': 'Artefacts are sufficiently documented (i.e. to understand how it works, to enable it to be run, including package versions)',\n    'documentation_careful': 'Artefacts are carefully documented (more than sufficient - i.e. to the extent that reuse and repurposing is facilitated - e.g. changing parameters, reusing for own purpose)',\n    # This criteria is kept seperate to documentation_careful, as it specifically requires a README file\n    'documentation_readme': 'Artefacts are clearly documented and accompanied by a README file with step-by-step instructions on how to reproduce results in the manuscript',\n    'execute': 'Scripts can be successfully executed',\n    'regenerated': 'Independent party regenerated results using the authors research artefacts',\n    'hour': 'Reproduced within approximately one hour (excluding compute time)',\n}\n\n# Evaluation for this study\n# TODO: Complete evaluate for each criteria\neval = pd.Series({\n    'archive': 0,\n    'id': 0,\n    'license': 1,\n    'relevant': 1,\n    'complete': 1,\n    'structure': 0,\n    'documentation_sufficient': 0,\n    'documentation_careful': 0,\n    'documentation_readme': 0,\n    'execute': 1,\n    'regenerated': 1,\n    'hour': 0,\n})\n\n# Get list of criteria met (True/False) overall\neval_list = list(eval)\n\n# Define function for creating the markdown formatted list of criteria met\ndef create_criteria_list(criteria_dict):\n    '''\n    Creates a string which contains a Markdown formatted list with icons to\n    indicate whether each criteria was met\n\n    Parameters:\n    -----------\n    criteria_dict : dict\n        Dictionary where keys are the criteria (variable name) and values are\n        Boolean (True/False of whether this study met the criteria)\n\n    Returns:\n    --------\n    formatted_list : string\n        Markdown formatted list\n    '''\n    callout_icon = {True: '‚úÖ',\n                    False: '‚ùå'}\n    # Create list with...\n    formatted_list = ''.join([\n        '* ' +\n        callout_icon[eval[key]] + # Icon based on whether it met criteria\n        ' ' +\n        value + # Full text description of criteria\n        '\\n' for key, value in criteria_dict.items()])\n    return(formatted_list)\n\n# Define groups of criteria\ncriteria_share_how = ['archive', 'id', 'license']\ncriteria_share_what = ['relevant', 'complete']\ncriteria_doc_struc = ['structure', 'documentation_sufficient', 'documentation_careful', 'documentation_readme']\ncriteria_run = ['execute', 'regenerated', 'hour']\n\n# Create text section\ndisplay(Markdown(f'''\nTo assess whether the author's materials met the requirements of each badge, a list of criteria was produced. Between each badge (and between categories of badge), there is often alot of overlap in criteria.\n\nThis study met **{sum(eval_list)} of the {len(eval_list)}** unique criteria items. These were as follows:\n\nCriteria related to how artefacts are shared -\n\n{create_criteria_list({k: criteria[k] for k in criteria_share_how})}\n\nCriteria related to what artefacts are shared -\n\n{create_criteria_list({k: criteria[k] for k in criteria_share_what})}\n\nCriteria related to the structure and documentation of the artefacts -\n\n{create_criteria_list({k: criteria[k] for k in criteria_doc_struc})}\n\nCriteria related to running and reproducing results -\n\n{create_criteria_list({k: criteria[k] for k in criteria_run})}\n'''))\n\n\nTo assess whether the author‚Äôs materials met the requirements of each badge, a list of criteria was produced. Between each badge (and between categories of badge), there is often alot of overlap in criteria.\nThis study met 5 of the 12 unique criteria items. These were as follows:\nCriteria related to how artefacts are shared -\n\n‚ùå Stored in a permanent archive that is publicly and openly accessible\n‚ùå Has a persistent identifier\n‚úÖ Includes an open license\n\nCriteria related to what artefacts are shared -\n\n‚úÖ Arefacts are relevant to and contribute to the article‚Äôs results\n‚úÖ Complete set of materials shared (as would be needed to fully reproduce article)\n\nCriteria related to the structure and documentation of the artefacts -\n\n‚ùå Artefacts are well structured/organised (e.g.¬†to the extent that reuse and repurposing is facilitated, adhering to norms and standards of research community)\n‚ùå Artefacts are sufficiently documented (i.e.¬†to understand how it works, to enable it to be run, including package versions)\n‚ùå Artefacts are carefully documented (more than sufficient - i.e.¬†to the extent that reuse and repurposing is facilitated - e.g.¬†changing parameters, reusing for own purpose)\n‚ùå Artefacts are clearly documented and accompanied by a README file with step-by-step instructions on how to reproduce results in the manuscript\n\nCriteria related to running and reproducing results -\n\n‚úÖ Scripts can be successfully executed\n‚úÖ Independent party regenerated results using the authors research artefacts\n‚ùå Reproduced within approximately one hour (excluding compute time)"
  },
  {
    "objectID": "evaluation/badges.html#badges",
    "href": "evaluation/badges.html#badges",
    "title": "Journal badges",
    "section": "Badges",
    "text": "Badges\n\n\nCode\n# Full badge names\nbadge_names = {\n    # Open objects\n    'open_niso': 'NISO \"Open Research Objects (ORO)\"',\n    'open_niso_all': 'NISO \"Open Research Objects - All (ORO-A)\"',\n    'open_acm': 'ACM \"Artifacts Available\"',\n    'open_cos': 'COS \"Open Code\"',\n    'open_ieee': 'IEEE \"Code Available\"',\n    # Object review\n    'review_acm_functional': 'ACM \"Artifacts Evaluated - Functional\"',\n    'review_acm_reusable': 'ACM \"Artifacts Evaluated - Reusable\"',\n    'review_ieee': 'IEEE \"Code Reviewed\"',\n    # Results reproduced\n    'reproduce_niso': 'NISO \"Results Reproduced (ROR-R)\"',\n    'reproduce_acm': 'ACM \"Results Reproduced\"',\n    'reproduce_ieee': 'IEEE \"Code Reproducible\"',\n    'reproduce_psy': 'Psychological Science \"Computational Reproducibility\"'\n}\n\n# Criteria required by each badge\nbadges = {\n    # Open objects\n    'open_niso': ['archive', 'id', 'license'],\n    'open_niso_all': ['archive', 'id', 'license', 'complete'],\n    'open_acm': ['archive', 'id'],\n    'open_cos': ['archive', 'id', 'license', 'complete', 'documentation_sufficient'],\n    'open_ieee': ['complete'],\n    # Object review\n    'review_acm_functional': ['documentation_sufficient', 'relevant', 'complete', 'execute'],\n    'review_acm_reusable': ['documentation_sufficient', 'documentation_careful', 'relevant', 'complete', 'execute', 'structure'],\n    'review_ieee': ['complete', 'execute'],\n    # Results reproduced\n    'reproduce_niso': ['regenerated'],\n    'reproduce_acm': ['regenerated'],\n    'reproduce_ieee': ['regenerated'],\n    'reproduce_psy': ['regenerated', 'hour', 'structure', 'documentation_readme'],\n}\n\n# Identify which badges would be awarded based on criteria\n# Get list of badges met (True/False) overall\naward = {}\nfor badge in badges:\n    award[badge] = all([eval[key] == 1 for key in badges[badge]])\naward_list = list(award.values())\n\n# Write introduction\n# Get list of badges met (True/False) by category\naward_open = [v for k,v in award.items() if k.startswith('open_')]\naward_review = [v for k,v in award.items() if k.startswith('review_')]\naward_reproduce = [v for k,v in award.items() if k.startswith('reproduce_')]\n\n# Create and display text for introduction\ndisplay(Markdown(f'''\nIn total, the original study met the criteria for **{sum(award_list)} of the {len(award_list)} badges**. This included:\n\n* **{sum(award_open)} of the {len(award_open)}** ‚Äúopen objects‚Äù badges\n* **{sum(award_review)} of the {len(award_review)}** ‚Äúobject review‚Äù badges\n* **{sum(award_reproduce)} of the {len(award_reproduce)}** ‚Äúreproduced‚Äù badges\n'''))\n\n# Make function that creates collapsible callouts for each badge\ndef create_badge_callout(award_dict):\n    '''\n    Displays Markdown callouts created for each badge in the dictionary, showing\n    whether the criteria for that badge was met.\n\n    Parameters:\n    -----------\n    award_dict : dict\n        Dictionary where key is badge (as variable name), and value is Boolean\n        (whether badge is awarded)\n    '''\n    callout_appearance = {True: 'tip',\n                          False: 'warning'}\n    callout_icon = {True: '‚úÖ',\n                    False: '‚ùå'}\n    callout_text = {True: 'Meets all criteria:',\n                    False: 'Does not meet all criteria:'}\n\n    for key, value in award_dict.items():\n        # Create Markdown list with...\n        criteria_list = ''.join([\n            '* ' +\n            callout_icon[eval[k]] + # Icon based on whether it met criteria\n            ' ' +\n            criteria[k] + # Full text description of criteria\n            '\\n' for k in badges[key]])\n        # Create the callout and display it\n        display(Markdown(f'''\n::: {{.callout-{callout_appearance[value]} appearance=\"minimal\" collapse=true}}\n\n## {callout_icon[value]} {badge_names[key]}\n\n{callout_text[value]}\n\n{criteria_list}\n:::\n'''))\n\n# Create badge functions with introductions and callouts\ndisplay(Markdown('''\n### \"Open objects\" badges\n\nThese badges relate to research artefacts being made openly available.\n'''))\ncreate_badge_callout({k: v for (k, v) in award.items() if k.startswith('open_')})\n\ndisplay(Markdown('''\n### \"Object review\" badges\n\nThese badges relate to the research artefacts being reviewed against criteria of the badge issuer.\n'''))\ncreate_badge_callout({k: v for (k, v) in award.items() if k.startswith('review_')})\n\ndisplay(Markdown('''\n### \"Reproduced\" badges\n\nThese badges relate to an independent party regenerating the reuslts of the article using the author objects.\n'''))\ncreate_badge_callout({k: v for (k, v) in award.items() if k.startswith('reproduce_')})\n\n\nIn total, the original study met the criteria for 5 of the 12 badges. This included:\n\n1 of the 5 ‚Äúopen objects‚Äù badges\n1 of the 3 ‚Äúobject review‚Äù badges\n3 of the 4 ‚Äúreproduced‚Äù badges\n\n\n\n‚ÄúOpen objects‚Äù badges\nThese badges relate to research artefacts being made openly available.\n\n\n\n\n\n\n\n\n‚ùå NISO ‚ÄúOpen Research Objects (ORO)‚Äù\n\n\n\n\n\nDoes not meet all criteria:\n\n‚ùå Stored in a permanent archive that is publicly and openly accessible\n‚ùå Has a persistent identifier\n‚úÖ Includes an open license\n\n\n\n\n\n\n\n\n\n\n\n\n‚ùå NISO ‚ÄúOpen Research Objects - All (ORO-A)‚Äù\n\n\n\n\n\nDoes not meet all criteria:\n\n‚ùå Stored in a permanent archive that is publicly and openly accessible\n‚ùå Has a persistent identifier\n‚úÖ Includes an open license\n‚úÖ Complete set of materials shared (as would be needed to fully reproduce article)\n\n\n\n\n\n\n\n\n\n\n\n\n‚ùå ACM ‚ÄúArtifacts Available‚Äù\n\n\n\n\n\nDoes not meet all criteria:\n\n‚ùå Stored in a permanent archive that is publicly and openly accessible\n‚ùå Has a persistent identifier\n\n\n\n\n\n\n\n\n\n\n\n\n‚ùå COS ‚ÄúOpen Code‚Äù\n\n\n\n\n\nDoes not meet all criteria:\n\n‚ùå Stored in a permanent archive that is publicly and openly accessible\n‚ùå Has a persistent identifier\n‚úÖ Includes an open license\n‚úÖ Complete set of materials shared (as would be needed to fully reproduce article)\n‚ùå Artefacts are sufficiently documented (i.e.¬†to understand how it works, to enable it to be run, including package versions)\n\n\n\n\n\n\n\n\n\n\n\n\n‚úÖ IEEE ‚ÄúCode Available‚Äù\n\n\n\n\n\nMeets all criteria:\n\n‚úÖ Complete set of materials shared (as would be needed to fully reproduce article)\n\n\n\n\n\n\n‚ÄúObject review‚Äù badges\nThese badges relate to the research artefacts being reviewed against criteria of the badge issuer.\n\n\n\n\n\n\n\n\n‚ùå ACM ‚ÄúArtifacts Evaluated - Functional‚Äù\n\n\n\n\n\nDoes not meet all criteria:\n\n‚ùå Artefacts are sufficiently documented (i.e.¬†to understand how it works, to enable it to be run, including package versions)\n‚úÖ Arefacts are relevant to and contribute to the article‚Äôs results\n‚úÖ Complete set of materials shared (as would be needed to fully reproduce article)\n‚úÖ Scripts can be successfully executed\n\n\n\n\n\n\n\n\n\n\n\n\n‚ùå ACM ‚ÄúArtifacts Evaluated - Reusable‚Äù\n\n\n\n\n\nDoes not meet all criteria:\n\n‚ùå Artefacts are sufficiently documented (i.e.¬†to understand how it works, to enable it to be run, including package versions)\n‚ùå Artefacts are carefully documented (more than sufficient - i.e.¬†to the extent that reuse and repurposing is facilitated - e.g.¬†changing parameters, reusing for own purpose)\n‚úÖ Arefacts are relevant to and contribute to the article‚Äôs results\n‚úÖ Complete set of materials shared (as would be needed to fully reproduce article)\n‚úÖ Scripts can be successfully executed\n‚ùå Artefacts are well structured/organised (e.g.¬†to the extent that reuse and repurposing is facilitated, adhering to norms and standards of research community)\n\n\n\n\n\n\n\n\n\n\n\n\n‚úÖ IEEE ‚ÄúCode Reviewed‚Äù\n\n\n\n\n\nMeets all criteria:\n\n‚úÖ Complete set of materials shared (as would be needed to fully reproduce article)\n‚úÖ Scripts can be successfully executed\n\n\n\n\n\n\n‚ÄúReproduced‚Äù badges\nThese badges relate to an independent party regenerating the reuslts of the article using the author objects.\n\n\n\n\n\n\n\n\n‚úÖ NISO ‚ÄúResults Reproduced (ROR-R)‚Äù\n\n\n\n\n\nMeets all criteria:\n\n‚úÖ Independent party regenerated results using the authors research artefacts\n\n\n\n\n\n\n\n\n\n\n\n\n‚úÖ ACM ‚ÄúResults Reproduced‚Äù\n\n\n\n\n\nMeets all criteria:\n\n‚úÖ Independent party regenerated results using the authors research artefacts\n\n\n\n\n\n\n\n\n\n\n\n\n‚úÖ IEEE ‚ÄúCode Reproducible‚Äù\n\n\n\n\n\nMeets all criteria:\n\n‚úÖ Independent party regenerated results using the authors research artefacts\n\n\n\n\n\n\n\n\n\n\n\n\n‚ùå Psychological Science ‚ÄúComputational Reproducibility‚Äù\n\n\n\n\n\nDoes not meet all criteria:\n\n‚úÖ Independent party regenerated results using the authors research artefacts\n‚ùå Reproduced within approximately one hour (excluding compute time)\n‚ùå Artefacts are well structured/organised (e.g.¬†to the extent that reuse and repurposing is facilitated, adhering to norms and standards of research community)\n‚ùå Artefacts are clearly documented and accompanied by a README file with step-by-step instructions on how to reproduce results in the manuscript"
  },
  {
    "objectID": "evaluation/badges.html#sources",
    "href": "evaluation/badges.html#sources",
    "title": "Journal badges",
    "section": "Sources",
    "text": "Sources\nNational Information Standards Organisation (NISO) (NISO Reproducibility Badging and Definitions Working Group (2021))\n\n‚ÄúOpen Research Objects (ORO)‚Äù\n‚ÄúOpen Research Objects - All (ORO-A)‚Äù\n‚ÄúResults Reproduced (ROR-R)‚Äù\n\nAssociation for Computing Machinery (ACM) (Association for Computing Machinery (ACM) (2020))\n\n‚ÄúArtifacts Available‚Äù\n‚ÄúArtifacts Evaluated - Functional‚Äù\n‚ÄúArtifacts Evaluated - Resuable‚Äù\n‚ÄúResults Reproduced‚Äù\n\nCenter for Open Science (COS) (Blohowiak et al. (2023))\n\n‚ÄúOpen Code‚Äù\n\nInstitute of Electrical and Electronics Engineers (IEEE) (Institute of Electrical and Electronics Engineers (IEEE) (n.d.))\n\n‚ÄúCode Available‚Äù\n‚ÄúCode Reviewed‚Äù\n‚ÄúCode Reproducible‚Äù\n\nPsychological Science (Hardwicke and Vazire (2023) and Association for Psychological Science (APS) (2023))\n\n‚ÄúComputational Reproducibility‚Äù"
  },
  {
    "objectID": "evaluation/reporting.html",
    "href": "evaluation/reporting.html",
    "title": "Reporting guidelines",
    "section": "",
    "text": "Please note: This is a template page and has not yet been completed\nThis page evaluates the extent to which the journal article meets the criteria from two discrete-event simulation study reporting guidelines:"
  },
  {
    "objectID": "evaluation/reporting.html#stress-des",
    "href": "evaluation/reporting.html#stress-des",
    "title": "Reporting guidelines",
    "section": "STRESS-DES",
    "text": "STRESS-DES\nOf the 24 items in the checklist:\n\n\nX were met fully (‚úÖ)\nX were partially met (üü°)\nX were not met (‚ùå)\nX were not applicable (N/A)\n\n\n\n\n\n\n\n\n\n\n\nItem\nRecommendation\nMet by study?\nEvidence\n\n\n\n\nObjectives\n\n\n\n\n\n1.1 Purpose of the model\nExplain the background and objectives for the model\n\n\n\n\n1.2 Model outputs\nDefine all quantitative performance measures that are reported, using equations where necessary. Specify how and when they are calculated during the model run along with how any measures of error such as confidence intervals are calculated.\n\n\n\n\n1.3 Experimentation aims\nIf the model has been used for experimentation, state the objectives that it was used to investigate.(A) Scenario based analysis ‚Äì Provide a name and description for each scenario, providing a rationale for the choice of scenarios and ensure that item 2.3 (below) is completed.(B) Design of experiments ‚Äì Provide details of the overall design of the experiments with reference to performance measures and their parameters (provide further details in data below).(C) Simulation Optimisation ‚Äì (if appropriate) Provide full details of what is to be optimised, the parameters that were included and the algorithm(s) that was be used. Where possible provide a citation of the algorithm(s).\n\n\n\n\nLogic\n\n\n\n\n\n2.1 Base model overview diagram\nDescribe the base model using appropriate diagrams and description. This could include one or more process flow, activity cycle or equivalent diagrams sufficient to describe the model to readers. Avoid complicated diagrams in the main text. The goal is to describe the breadth and depth of the model with respect to the system being studied.\n\n\n\n\n2.2 Base model logic\nGive details of the base model logic. Give additional model logic details sufficient to communicate to the reader how the model works.\n\n\n\n\n2.3 Scenario logic\nGive details of the logical difference between the base case model and scenarios (if any). This could be incorporated as text or where differences are substantial could be incorporated in the same manner as 2.2.\n\n\n\n\n2.4 Algorithms\nProvide further detail on any algorithms in the model that (for example) mimic complex or manual processes in the real world (i.e.¬†scheduling of arrivals/ appointments/ operations/ maintenance, operation of a conveyor system, machine breakdowns, etc.). Sufficient detail should be included (or referred to in other published work) for the algorithms to be reproducible. Pseudo-code may be used to describe an algorithm.\n\n\n\n\n2.5.1 Components - entities\nGive details of all entities within the simulation including a description of their role in the model and a description of all their attributes.\n\n\n\n\n2.5.2 Components - activities\nDescribe the activities that entities engage in within the model. Provide details of entity routing into and out of the activity.\n\n\n\n\n2.5.3 Components - resources\nList all the resources included within the model and which activities make use of them.\n\n\n\n\n2.5.4 Components - queues\nGive details of the assumed queuing discipline used in the model (e.g.¬†First in First Out, Last in First Out, prioritisation, etc.). Where one or more queues have a different discipline from the rest, provide a list of queues, indicating the queuing discipline used for each. If reneging, balking or jockeying occur, etc., provide details of the rules. Detail any delays or capacity constraints on the queues.\n\n\n\n\n2.5.5 Components - entry/exit points\nGive details of the model boundaries i.e.¬†all arrival and exit points of entities. Detail the arrival mechanism (e.g.¬†‚Äòthinning‚Äô to mimic a non-homogenous Poisson process or balking)\n\n\n\n\nData\n\n\n\n\n\n3.1 Data sources\nList and detail all data sources. Sources may include:‚Ä¢ Interviews with stakeholders,‚Ä¢ Samples of routinely collected data,‚Ä¢ Prospectively collected samples for the purpose of the simulation study,‚Ä¢ Public domain data published in either academic or organisational literature. Provide, where possible, the link and DOI to the data or reference to published literature.All data source descriptions should include details of the sample size, sample date ranges and use within the study.\n\n\n\n\n3.2 Pre-processing\nProvide details of any data manipulation that has taken place before its use in the simulation, e.g.¬†interpolation to account for missing data or the removal of outliers.\n\n\n\n\n3.3 Input parameters\nList all input variables in the model. Provide a description of their use and include parameter values. For stochastic inputs provide details of any continuous, discrete or empirical distributions used along with all associated parameters. Give details of all time dependent parameters and correlation.Clearly state:‚Ä¢ Base case data‚Ä¢ Data use in experimentation, where different from the base case.‚Ä¢ Where optimisation or design of experiments has been used, state the range of values that parameters can take.‚Ä¢ Where theoretical distributions are used, state how these were selected and prioritised above other candidate distributions.\n\n\n\n\n3.4 Assumptions\nWhere data or knowledge of the real system is unavailable what assumptions are included in the model? This might include parameter values, distributions or routing logic within the model.\n\n\n\n\nExperimentation\n\n\n\n\n\n4.1 Initialisation\nReport if the system modelled is terminating or non-terminating. State if a warm-up period has been used, its length and the analysis method used to select it. For terminating systems state the stopping condition.State what if any initial model conditions have been included, e.g., pre-loaded queues and activities. Report whether initialisation of these variables is deterministic or stochastic.\n\n\n\n\n4.2 Run length\nDetail the run length of the simulation model and time units.\n\n\n\n\n4.3 Estimation approach\nState the method used to account for the stochasticity: For example, two common methods are multiple replications or batch means. Where multiple replications have been used, state the number of replications and for batch means, indicate the batch length and whether the batch means procedure is standard, spaced or overlapping. For both procedures provide a justification for the methods used and the number of replications/size of batches.\n\n\n\n\nImplementation\n\n\n\n\n\n5.1 Software or programming language\nState the operating system and version and build number.State the name, version and build number of commercial or open source DES software that the model is implemented in.State the name and version of general-purpose programming languages used (e.g.¬†Python 3.5).Where frameworks and libraries have been used provide all details including version numbers.\n\n\n\n\n5.2 Random sampling\nState the algorithm used to generate random samples in the software/programming language used e.g.¬†Mersenne Twister.If common random numbers are used, state how seeds (or random number streams) are distributed among sampling processes.\n\n\n\n\n5.3 Model execution\nState the event processing mechanism used e.g.¬†three phase, event, activity, process interaction.Note that in some commercial software the event processing mechanism may not be published. In these cases authors should adhere to item 5.1 software recommendations.State all priority rules included if entities/activities compete for resources.If the model is parallel, distributed and/or use grid or cloud computing, etc., state and preferably reference the technology used. For parallel and distributed simulations the time management algorithms used. If the HLA is used then state the version of the standard, which run-time infrastructure (and version), and any supporting documents (FOMs, etc.)\n\n\n\n\n5.4 System specification\nState the model run time and specification of hardware used. This is particularly important for large scale models that require substantial computing power. For parallel, distributed and/or use grid or cloud computing, etc. state the details of all systems used in the implementation (processors, network, etc.)\n\n\n\n\nCode access\n\n\n\n\n\n6.1 Computer model sharing statement\nDescribe how someone could obtain the model described in the paper, the simulation software and any other associated software (or hardware) needed to reproduce the results. Provide, where possible, the link and DOIs to these."
  },
  {
    "objectID": "evaluation/reporting.html#des-checklist-derived-from-ispor-sdm",
    "href": "evaluation/reporting.html#des-checklist-derived-from-ispor-sdm",
    "title": "Reporting guidelines",
    "section": "DES checklist derived from ISPOR-SDM",
    "text": "DES checklist derived from ISPOR-SDM\nOf the 18 items in the checklist:\n\n\nX were met fully (‚úÖ)\nX was partially met (üü°)\nX was not met (‚ùå)\nX were not applicable (N/A)\n\n\n\n\n\n\n\n\n\n\n\nItem\nAssessed if‚Ä¶\nMet by study?\nEvidence/location\n\n\n\n\nModel conceptualisation\n\n\n\n\n\n1 Is the focused health-related decision problem clarified?\n‚Ä¶the decision problem under investigation was defined. DES studies included different types of decision problems, eg, those listed in previously developed taxonomies.\n\n\n\n\n2 Is the modeled healthcare setting/health condition clarified?\n‚Ä¶the physical context/scope (eg, a certain healthcare unit or a broader system) or disease spectrum simulated was described.\n\n\n\n\n3 Is the model structure described?\n‚Ä¶the model‚Äôs conceptual structure was described in the form of either graphical or text presentation.\n\n\n\n\n4 Is the time horizon given?\n‚Ä¶the time period covered by the simulation was reported.\n\n\n\n\n5 Are all simulated strategies/scenarios specified?\n‚Ä¶the comparators under test were described in terms of their components, corresponding variations, etc\n\n\n\n\n6 Is the target population described?\n‚Ä¶the entities simulated and their main attributes were characterized.\n\n\n\n\nParamaterisation and uncertainty assessment\n\n\n\n\n\n7 Are data sources informing parameter estimations provided?\n‚Ä¶the sources of all data used to inform model inputs were reported.\n\n\n\n\n8 Are the parameters used to populate model frameworks specified?\n‚Ä¶all relevant parameters fed into model frameworks were disclosed.\n\n\n\n\n9 Are model uncertainties discussed?\n‚Ä¶the uncertainty surrounding parameter estimations and adopted statistical methods (eg, 95% confidence intervals or possibility distributions) were reported.\n\n\n\n\n10 Are sensitivity analyses performed and reported?\n‚Ä¶the robustness of model outputs to input uncertainties was examined, for example via deterministic (based on parameters‚Äô plausible ranges) or probabilistic (based on a priori-defined probability distributions) sensitivity analyses, or both.\n\n\n\n\nValidation\n\n\n\n\n\n11 Is face validity evaluated and reported?\n‚Ä¶it was reported that the model was subjected to the examination on how well model designs correspond to the reality and intuitions. It was assumed that this type of validation should be conducted by external evaluators with no stake in the study.\n\n\n\n\n12 Is cross validation performed and reported\n‚Ä¶comparison across similar modeling studies which deal with the same decision problem was undertaken.\n\n\n\n\n13 Is external validation performed and reported?\n‚Ä¶the modeler(s) examined how well the model‚Äôs results match the empirical data of an actual event modeled.\n\n\n\n\n14 Is predictive validation performed or attempted?\n‚Ä¶the modeler(s) examined the consistency of a model‚Äôs predictions of a future event and the actual outcomes in the future. If this was not undertaken, it was assessed whether the reasons were discussed.\n\n\n\n\nGeneralisability and stakeholder involvement\n\n\n\n\n\n15 Is the model generalizability issue discussed?\n‚Ä¶the modeler(s) discussed the potential of the resulting model for being applicable to other settings/populations (single/multiple application).\n\n\n\n\n16 Are decision makers or other stakeholders involved in modeling?\n‚Ä¶the modeler(s) reported in which part throughout the modeling process decision makers and other stakeholders (eg, subject experts) were engaged.\n\n\n\n\n17 Is the source of funding stated?\n‚Ä¶the sponsorship of the study was indicated.\n\n\n\n\n18 Are model limitations discussed?\n‚Ä¶limitations of the assessed model, especially limitations of interest to decision makers, were discussed."
  },
  {
    "objectID": "reproduction/reproduction.html",
    "href": "reproduction/reproduction.html",
    "title": "Reproduction",
    "section": "",
    "text": "# Clear environment\nrm(list=ls())\n\n# Start timer\nstart.time &lt;- Sys.time()\n\n# Disable scientific notation\noptions(scipen=999)\n\n# Import required libraries (if not otherwise import in model.R)\nlibrary(ggplot2)\nlibrary(data.table)\nlibrary(ggpubr)\nlibrary(tidyr, include.only = c(\"pivot_wider\"))\n\n# Get the model function (but hide loading warnings for each package)\nsuppressMessages(source(\"model.R\"))\n\n\n# Set the seed\nSEED = 200\n\n# Set file paths to save results\n\nfolder = \"outputs\"\n\npath_baseline_f2 &lt;- file.path(folder, \"fig2_baseline.csv.gz\")\npath_exclusive_f2 &lt;- file.path(folder, \"fig2_exclusive.csv.gz\")\npath_twoangio_f2 &lt;- file.path(folder, \"fig2_twoangio.csv.gz\")\npath_baseline_f2_code &lt;- file.path(folder, \"fig2_baseline_codedefault.csv.gz\")\npath_baseline_f2_cloudes &lt;- file.path(folder, \"fig2_baseline_cloudes.csv.gz\")\n\npath_baseline_f3 &lt;- file.path(folder, \"fig3_baseline.csv.gz\")\npath_exclusive_f3 &lt;- file.path(folder, \"fig3_exclusive.csv.gz\")\npath_twoangio_f3 &lt;- file.path(folder, \"fig3_twoangio.csv.gz\")\npath_baseline_f3_seed500 &lt;- file.path(folder, \"fig3_baseline_seed500.csv.gz\")\npath_baseline_f3_seed700 &lt;- file.path(folder, \"fig3_baseline_seed700.csv.gz\")\n\npath_txt2 &lt;- file.path(folder, \"txt2.csv\")\npath_txt3 &lt;- file.path(folder, \"txt3.csv\")\npath_txt3_seeds &lt;- file.path(folder, \"txt3_seeds.csv\")\npath_fig2 &lt;- file.path(folder, \"fig2.png\")\npath_fig2a_code &lt;- file.path(folder, \"fig2a_codeparam.png\")\npath_fig2a_cloudes &lt;- file.path(folder, \"fig2a_cloudes.png\")\npath_fig2a_seeds &lt;- file.path(folder, \"fig2a_seeds.png\")\npath_fig3 &lt;- file.path(folder, \"fig3.png\")"
  },
  {
    "objectID": "reproduction/reproduction.html#set-up",
    "href": "reproduction/reproduction.html#set-up",
    "title": "Reproduction",
    "section": "",
    "text": "# Clear environment\nrm(list=ls())\n\n# Start timer\nstart.time &lt;- Sys.time()\n\n# Disable scientific notation\noptions(scipen=999)\n\n# Import required libraries (if not otherwise import in model.R)\nlibrary(ggplot2)\nlibrary(data.table)\nlibrary(ggpubr)\nlibrary(tidyr, include.only = c(\"pivot_wider\"))\n\n# Get the model function (but hide loading warnings for each package)\nsuppressMessages(source(\"model.R\"))\n\n\n# Set the seed\nSEED = 200\n\n# Set file paths to save results\n\nfolder = \"outputs\"\n\npath_baseline_f2 &lt;- file.path(folder, \"fig2_baseline.csv.gz\")\npath_exclusive_f2 &lt;- file.path(folder, \"fig2_exclusive.csv.gz\")\npath_twoangio_f2 &lt;- file.path(folder, \"fig2_twoangio.csv.gz\")\npath_baseline_f2_code &lt;- file.path(folder, \"fig2_baseline_codedefault.csv.gz\")\npath_baseline_f2_cloudes &lt;- file.path(folder, \"fig2_baseline_cloudes.csv.gz\")\n\npath_baseline_f3 &lt;- file.path(folder, \"fig3_baseline.csv.gz\")\npath_exclusive_f3 &lt;- file.path(folder, \"fig3_exclusive.csv.gz\")\npath_twoangio_f3 &lt;- file.path(folder, \"fig3_twoangio.csv.gz\")\npath_baseline_f3_seed500 &lt;- file.path(folder, \"fig3_baseline_seed500.csv.gz\")\npath_baseline_f3_seed700 &lt;- file.path(folder, \"fig3_baseline_seed700.csv.gz\")\n\npath_txt2 &lt;- file.path(folder, \"txt2.csv\")\npath_txt3 &lt;- file.path(folder, \"txt3.csv\")\npath_txt3_seeds &lt;- file.path(folder, \"txt3_seeds.csv\")\npath_fig2 &lt;- file.path(folder, \"fig2.png\")\npath_fig2a_code &lt;- file.path(folder, \"fig2a_codeparam.png\")\npath_fig2a_cloudes &lt;- file.path(folder, \"fig2a_cloudes.png\")\npath_fig2a_seeds &lt;- file.path(folder, \"fig2a_seeds.png\")\npath_fig3 &lt;- file.path(folder, \"fig3.png\")"
  },
  {
    "objectID": "reproduction/reproduction.html#functions",
    "href": "reproduction/reproduction.html#functions",
    "title": "Reproduction",
    "section": "Functions",
    "text": "Functions\n\nrun_model &lt;- function(angio_inr = 1,\n                      angio_ir = 1,\n                      ir = 2,\n                      angio_staff = 6,\n                      ed_pt = 107700,\n                      inr_pt = 104,\n                      eir_pt= 468,\n                      ir_pt = 3805,\n                      shifts = c(8,17),\n                      run_t = 365,\n                      nsim = 30,\n                      exclusive_use = FALSE,\n                      seed = 42,\n                      ed_triage = c(20,10)) {\n  #' Run model and get results\n  #' \n  #' @param angio_inr number of AngioINR machines\n  #' @param angio_ir number of AngioIR machines\n  #' @param ir number of interventional radiologists on day shift\n  #' @param angio_staff number of angiography staff on day shift\n  #' @param ed_pt number of ED patients\n  #' @param inr_pt number of elective INR patients\n  #' @param eir_pt number of emergency IR patients\n  #' @param ir_pt number of elective IR patients\n  #' @param shifts tuple with start and finish time of day shift\n  #' @param run_t simulation runtime in days\n  #' @param nsim number of replications\n  #' @param exclusive_use whether angioINR has exclusive use (i.e. no elective\n  #' IR patients allowed to use the machine)\n  #' @param seed integer that provides seed to be incremented on in each\n  #' replication (e.g. run 1 is seed+=1, run 2 is seed+=2)\n  #' @param ed_triage tuple with mean and SD of normal distribution for\n  #'sampling the length of the emergency department triage\n\n  # Run the model\n  list_containing_output &lt;- simulate_nav(\n    angio_inr = angio_inr,\n    angio_ir = angio_ir,\n    ir = ir,\n    angio_staff = angio_staff,\n    ed_pt = ed_pt,\n    inr_pt = inr_pt,\n    eir_pt = eir_pt,\n    ir_pt = ir_pt,\n    shifts = shifts,\n    run_t = run_t,\n    nsim = nsim,\n    exclusive_use = exclusive_use,\n    seed = seed,\n    ed_triage = ed_triage\n  )\n  # Get arrivals (not interested in resources - list_containing_output[[2]]))\n  # Filter to the relevant results (ED + resource and wait_time)\n  arrivals &lt;- data.frame(list_containing_output[[1]]) %&gt;%\n    filter(category == \"ed\") %&gt;%\n    select(resource, wait_time)\n\n  # Return list with two dataframes\n  return(arrivals)\n}\n\n\nprocess_f3_data &lt;- function(df5, df6, df7, save_path) {\n  #' Process model results to create data for Figure 3\n  #' \n  #' @param df5 Dataframe with results from model where shifts end at 5pm\n  #' @param df6 Dataframe with results from model where shifts end at 6pm\n  #' @param df7 Dataframe with results from model where shifts end at 7pm\n  #' @param save_path String with path to save results to\n\n  # Add shift time\n  df5$shift = \"5pm\"\n  df6$shift = \"6pm\"\n  df7$shift = \"7pm\"\n\n  # Combine into single dataframe, then filter to just angioINR wait times\n  baseline_hours &lt;- dplyr::bind_rows(df5, df6, df7) %&gt;%\n    filter(resource == \"angio_inr\")\n\n  # Save to provided path\n  data.table::fwrite(baseline_hours, save_path)\n}\n\n\nimport_results &lt;- function(path, scenario) {\n  #' Import the file and add a column with the scenario\n  #' \n  #' @param path path to file to import\n  #' @param scenario string to population \"scenario\" column with\n  return(data.table::fread(path) %&gt;% mutate(scenario=scenario))\n}\n\n\ncreate_plot &lt;- function(df, group, title, xlab=\"\", ylab=\"\", xlim=c(0, 200),\n                        breaks_width=50) {\n  #' Create sub-plots for Figure 2A\n  #' \n  #' @param df Dataframe with wait times across replications\n  #' @param group String indicating which column to group by in plot\n  #' @param title String to use as title for plot\n  #' @param xlab String to use as title for X axis\n  #' @param ylab String to use as title for Y axis\n  #' @param xlim Tuple with limits for x axis\n  #' @param breaks_width Integer indicating frequency of X ticks\n  \n  # Set negative wait times to 0\n  df$wait_time[df$wait_time &lt; 0] &lt;- 0\n  \n  # Create the plot, scaling the density estimate to a maximum of 1\n  # Remove INR resources as they hide the angio_staff line, which is on top\n  # in the figures in the paper\n  ggplot(df %&gt;% filter(resource!=\"inr\"),\n         aes(x = wait_time,\n             colour = .data[[group]],\n             y = after_stat(scaled))) +\n    geom_density() +\n    geom_density(aes(x = wait_time, y = after_stat(scaled))) +\n    # Apply square transformation to each axis, removing x points beyond limits\n    scale_y_continuous(transform = \"sqrt\") +\n    scale_x_continuous(transform = \"sqrt\",\n                       breaks = scales::breaks_width(breaks_width),\n                       limits = xlim,\n                       oob = scales::censor,\n                       guide = guide_axis(angle=45)) +\n    # Titles and styling\n    ggtitle(title) +\n    xlab(xlab) +\n    ylab(ylab) +\n    theme_minimal(base_size=10) +\n    theme(plot.title = element_text(hjust = 0.5),\n          axis.text.x = element_text(colour=\"black\"),\n          axis.text.y = element_text(colour=\"black\"),\n          legend.title=element_blank()) +\n    guides(colour = guide_legend(nrow = 1))\n}\n\nRun model scenarios.\n\nif (isTRUE(run)) {\n  # Run model\n  baseline &lt;- run_model(seed = SEED)\n  baseline_6pm &lt;- run_model(shifts = c(8,18), seed = SEED)\n  baseline_7pm &lt;- run_model(shifts = c(8,19), seed = SEED)\n\n  exclusive &lt;- run_model(exclusive_use = TRUE, seed = SEED)\n  exclusive_6pm &lt;- run_model(shifts = c(8,18), exclusive_use = TRUE, seed = SEED)\n  exclusive_7pm &lt;- run_model(shifts = c(8,19), exclusive_use = TRUE, seed = SEED)\n\n  twoangio &lt;- run_model(angio_inr = 2, angio_ir=0, seed = SEED)\n  twoangio_6pm &lt;- run_model(shifts = c(8,18), angio_inr = 2, angio_ir=0, seed = SEED)\n  twoangio_7pm &lt;- run_model(shifts = c(8,19), angio_inr = 2, angio_ir=0, seed = SEED)\n}\n\n\n# (in seperate cell to above as otherwise seemed to crash)\nif (isTRUE(run)) {\n  # Save results for Figure 2\n  data.table::fwrite(baseline, path_baseline_f2)\n  data.table::fwrite(exclusive, path_exclusive_f2)\n  data.table::fwrite(twoangio, path_twoangio_f2)\n\n  # Process and save results for Figure 3\n  process_f3_data(baseline, baseline_6pm, baseline_7pm, path_baseline_f3)\n  process_f3_data(exclusive, exclusive_6pm, exclusive_7pm, path_exclusive_f3)\n  process_f3_data(twoangio, twoangio_6pm, twoangio_7pm, path_twoangio_f3)\n}\n\nRun baseline with varying hours and seeds.\n\nif (isTRUE(run)) {\n  # Baseline with varying hours and seed 500\n  s500_baseline &lt;- run_model(seed = 500)\n  s500_baseline_6pm &lt;- run_model(shifts = c(8,18), seed = 500)\n  s500_baseline_7pm &lt;- run_model(shifts = c(8,19), seed = 500)\n\n  # Baseline with varying hours and seed 700\n  s700_baseline &lt;- run_model(seed = 700)\n  s700_baseline_6pm &lt;- run_model(shifts = c(8,18), seed = 700)\n  s700_baseline_7pm &lt;- run_model(shifts = c(8,19), seed = 700)\n}\n\n\n# Save results (in seperate cell to above as otherwise seemed to crash)\nif (isTRUE(run)) {\n  process_f3_data(s500_baseline, s500_baseline_6pm, s500_baseline_7pm,\n                  path_baseline_f3_seed500)\n  process_f3_data(s700_baseline, s700_baseline_6pm, s700_baseline_7pm,\n                  path_baseline_f3_seed700)\n}\n\nRun baseline with default parameters from the code (rather than parameters from the paper).\n\nif (isTRUE(run)) {\n  list_containing_output &lt;- simulate_nav(seed=200)\n  baseline_code &lt;- data.frame(list_containing_output[[1]]) %&gt;%\n    filter(category == \"ed\") %&gt;%\n    select(resource, wait_time)\n}\n\n\nif (isTRUE(run)) {\n  data.table::fwrite(baseline_code, path_baseline_f2_code)\n}\n\nRun baseline with ED triage team from CLOUDES\n\nif (isTRUE(run)) {\n  baseline_cloudes &lt;- run_model(seed = 700, ed_triage=c(15, 5))\n}\n\n\nif (isTRUE(run)) {\n  data.table::fwrite(baseline_cloudes, path_baseline_f2_cloudes)\n}\n\nRun baseline with several different seeds\n\nbaseline_seeds &lt;- function(seed, run, folder) {\n  #' Run baseline model with different seeds\n  #' \n  #' @param seed integer, starting seed for replications\n  #' @param run boolean, whether to run model\n  #' @param folder string, folder to store results in\n  if (isTRUE(run)) {\n    seed_baseline &lt;- run_model(seed = seed)\n    seed_path &lt;- file.path(folder, paste(\n      \"fig2_baseline_seed\", seed, \".csv.gz\", sep=\"\"))\n    data.table::fwrite(seed_baseline, seed_path)\n  }\n}\n\n\nbaseline_seeds(0, run, folder)\nbaseline_seeds(50, run, folder)\nbaseline_seeds(100, run, folder)\nbaseline_seeds(150, run, folder)\nbaseline_seeds(450, run, folder)\nbaseline_seeds(700, run, folder)\nbaseline_seeds(750, run, folder)"
  },
  {
    "objectID": "reproduction/reproduction.html#import-results",
    "href": "reproduction/reproduction.html#import-results",
    "title": "Reproduction",
    "section": "Import results",
    "text": "Import results\nImport the results, adding a column to each to indicate the scenario.\n\nbase_f2 &lt;- import_results(path_baseline_f2,\n                          \"Baseline\")\nexc_f2 &lt;- import_results(path_exclusive_f2,\n                         \"Exclusive use\")\ntwo_f2 &lt;- import_results(path_twoangio_f2,\n                         \"Two AngioINRs\")\nbase_f2_code &lt;- import_results(path_baseline_f2_code,\n                               \"Baseline (code param)\")\nbase_f2_cloudes &lt;- import_results(path_baseline_f2_cloudes,\n                                  \"Baseline (ED triage CLOUDES)\")\n\nbase_f3 &lt;- import_results(path_baseline_f3,\n                          \"Baseline\")\nexc_f3 &lt;- import_results(path_exclusive_f3,\n                         \"Exclusive use\")\ntwo_f3 &lt;- import_results(path_twoangio_f3,\n                         \"Two AngioINRs\")\n\nbase_f3_s500  &lt;- import_results(path_baseline_f3_seed500,\n                                \"Baseline (seed 500)\")\nbase_f3_s700  &lt;- import_results(path_baseline_f3_seed700,\n                                \"Baseline (seed 700)\")"
  },
  {
    "objectID": "reproduction/reproduction.html#in-text-results",
    "href": "reproduction/reproduction.html#in-text-results",
    "title": "Reproduction",
    "section": "In-text results",
    "text": "In-text results\nIn-text results 1 and 2\n\ntxt2 &lt;- dplyr::bind_rows(base_f2, exc_f2, two_f2) %&gt;%\n  filter(resource==\"angio_inr\") %&gt;%\n  group_by(scenario) %&gt;%\n  summarize(mean = mean(wait_time)) %&gt;%\n  mutate(diff_from_baseline = round(mean - mean[1], 2))\n\n# Save and display result\ndata.table::fwrite(txt2, path_txt2)\ntxt2\n\n# A tibble: 3 √ó 3\n  scenario       mean diff_from_baseline\n  &lt;chr&gt;         &lt;dbl&gt;              &lt;dbl&gt;\n1 Baseline      14.0                0   \n2 Exclusive use  8.12              -5.84\n3 Two AngioINRs  9.62              -4.34\n\n\nIn-text result 3\n\ntxt3 &lt;- dplyr::bind_rows(base_f3, exc_f3, two_f3) %&gt;%\n  filter(resource==\"angio_inr\") %&gt;%\n  group_by(scenario, shift) %&gt;%\n  summarize(mean = mean(wait_time)) %&gt;%\n  mutate(diff_from_5pm = round(mean - mean[1], 2))\n\n`summarise()` has grouped output by 'scenario'. You can override using the\n`.groups` argument.\n\n# Save and display result\ndata.table::fwrite(txt3, path_txt3)\ntxt3\n\n# A tibble: 9 √ó 4\n# Groups:   scenario [3]\n  scenario      shift  mean diff_from_5pm\n  &lt;chr&gt;         &lt;chr&gt; &lt;dbl&gt;         &lt;dbl&gt;\n1 Baseline      5pm   14.0           0   \n2 Baseline      6pm   12.5          -1.47\n3 Baseline      7pm   12.5          -1.47\n4 Exclusive use 5pm    8.12          0   \n5 Exclusive use 6pm    7.80         -0.31\n6 Exclusive use 7pm    6.43         -1.69\n7 Two AngioINRs 5pm    9.62          0   \n8 Two AngioINRs 6pm    9.22         -0.4 \n9 Two AngioINRs 7pm    8.70         -0.92\n\n\n\nDemonstrate variation between model runs\nWe can see the importance of seed control here. For example, with seed 700, we see a broader range of results, with the result for 6pm (13.32) is much higher than for the other two seeds and, compared with their 5pm results, we would‚Äôve seen less of a reduction. Similarly, if we compared the 5pm seed 700 result with the 6pm seed 500 result, we would see a much greater reduction.\nThe same applies to the prior scenarios for Figure 2. I‚Äôve held the seeds still between them, so the only change is the scenario.\nHowever, as we are not sure, it is also reasonable to assume that a seed might have been used for the paper, but just not mentioned in the article or included in the code.\n\n# Process results\nbase_seeds &lt;- dplyr::bind_rows(base_f3, base_f3_s500, base_f3_s700) %&gt;%\n  filter(resource==\"angio_inr\") %&gt;%\n  group_by(scenario, shift) %&gt;%\n  summarize(mean = round(mean(wait_time),2)) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'scenario'. You can override using the\n`.groups` argument.\n\n# Convert to wide format and display\nbase_seeds_wide &lt;- pivot_wider(base_seeds, id_cols=\"scenario\",\n                               values_from=c(\"mean\"), names_from=\"shift\")\ndata.table::fwrite(base_seeds_wide, path_txt3_seeds)\nbase_seeds_wide\n\n# A tibble: 3 √ó 4\n  scenario            `5pm` `6pm` `7pm`\n  &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Baseline             14.0  12.5  12.5\n2 Baseline (seed 500)  14.0  12.2  12.2\n3 Baseline (seed 700)  15.5  13.3  12.3"
  },
  {
    "objectID": "reproduction/reproduction.html#figure-2",
    "href": "reproduction/reproduction.html#figure-2",
    "title": "Reproduction",
    "section": "Figure 2",
    "text": "Figure 2\n\n# Create sub-plots\np1 &lt;- create_plot(base_f2,\n                  group=\"resource\",\n                  title=\"Baseline\",\n                  ylab=\"Standardised density of patient in queue\")\np2 &lt;- create_plot(exc_f2,\n                  group=\"resource\",\n                  title=\"Exclusive-use\",\n                  xlab=\"Patient wait time (min)\",\n                  xlim=c(0, 250))\np3 &lt;- create_plot(two_f2,\n                  group=\"resource\",\n                  title=\"Double angio INRs\")\n\n# Arrange in a single figure\nggarrange(p1, p2, p3, nrow=1,\n          common.legend=TRUE, legend=\"bottom\",\n          labels=c(\"A\", \"B\", \"C\"))\n\nWarning: Removed 1 row containing non-finite outside the scale range (`stat_density()`).\nRemoved 1 row containing non-finite outside the scale range (`stat_density()`).\nRemoved 1 row containing non-finite outside the scale range (`stat_density()`).\nRemoved 1 row containing non-finite outside the scale range (`stat_density()`).\nRemoved 1 row containing non-finite outside the scale range (`stat_density()`).\nRemoved 1 row containing non-finite outside the scale range (`stat_density()`).\nRemoved 1 row containing non-finite outside the scale range (`stat_density()`).\nRemoved 1 row containing non-finite outside the scale range (`stat_density()`).\n\n\n\n\n\n\n\n\nggsave(path_fig2)\n\nSaving 7 x 5 in image\n\n\n\nDemonstrate that geom_density scaled is scaling against density of 0 wait time\n\n# Create figure as usual\np &lt;- create_plot(base_f2,\n                 group=\"resource\",\n                 title=\"Baseline\",\n                 ylab=\"Standardised density of patient in queue\")\n\n# Get data from the plot\nplot_data &lt;- ggplot_build(p)$data[[1]]\n\nWarning: Removed 1 row containing non-finite outside the scale range (`stat_density()`).\nRemoved 1 row containing non-finite outside the scale range (`stat_density()`).\n\n# Create dataframe with the densities for when the waitimes are 0\nno_wait &lt;- plot_data %&gt;% filter(x==0) %&gt;% select(colour, density, scaled)\n\n# Loop through each of the colours (which reflect the resource groups)\nfor (c in no_wait$colour) {\n  # Filter the plot data to that resource group, then divide the densities by\n  # the density from wait time 0\n  d &lt;- plot_data %&gt;%\n    filter(colour == c) %&gt;%\n    mutate(scaled2 = density / no_wait[no_wait$colour==c, \"density\"]) %&gt;%\n    ungroup() %&gt;%\n    select(scaled, scaled2)\n\n  # Find the number of rows where these values match the scaled values\n  n_match &lt;- sum(apply(d, 1, function(x) length(unique(x)) == 1))\n  n_total &lt;- nrow(d)\n  print(sprintf(\"%s out of %s results match\", n_match, n_total))\n}\n\n[1] \"512 out of 512 results match\"\n[1] \"512 out of 512 results match\"\n[1] \"512 out of 512 results match\"\n[1] \"512 out of 512 results match\"\n[1] \"512 out of 512 results match\"\n\n\n\n\nFigure 2 (baseline only) with default code parameters\n\ncreate_plot(base_f2_code,\n            group=\"resource\",\n            title=\"Baseline (code parameters)\",\n            xlab=\"Patient wait time (min)\",\n            ylab=\"Standardised density of patient in queue\",\n            xlim=c(0, 1000))\n\nWarning: Removed 4 rows containing non-finite outside the scale range\n(`stat_density()`).\nRemoved 4 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\nggsave(path_fig2a_code)\n\nSaving 7 x 5 in image\n\n\nWarning: Removed 4 rows containing non-finite outside the scale range\n(`stat_density()`).\nRemoved 4 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\nFigure 2 (baseline only) with ED triage length from CLOUDES\n\ncreate_plot(base_f2_cloudes,\n            group=\"resource\",\n            title=\"Baseline (ED triage from CLOUDES)\",\n            xlab=\"Patient wait time (min)\",\n            ylab=\"Standardised density of patient in queue\",\n            xlim=c(0, 200))\n\n\n\n\n\n\n\nggsave(path_fig2a_cloudes)\n\nSaving 7 x 5 in image\n\n\n\n\nFigure 2 (baseline only) with various different seeds\nThis imports the files too (as several files)\n\nplot_list &lt;- list()\ni &lt;- 0\nseeds &lt;- c(0, 50, 100, 150, 450, 700, 750)\nfor (s in seeds) {\n  i &lt;- i + 1\n  seed_path &lt;- file.path(folder, paste(\n      \"fig2_baseline_seed\", s, \".csv.gz\", sep=\"\"))\n  seed_baseline &lt;- import_results(seed_path, \"Baseline\")\n  plot_list[[i]] &lt;- create_plot(seed_baseline, group=\"resource\", title=s)\n}\n\n\nggarrange(plotlist=plot_list, common.legend=TRUE, legend=\"bottom\")\n\nWarning: Removed 4 rows containing non-finite outside the scale range\n(`stat_density()`).\nRemoved 4 rows containing non-finite outside the scale range\n(`stat_density()`).\nRemoved 4 rows containing non-finite outside the scale range\n(`stat_density()`).\nRemoved 4 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\nWarning: Removed 1 row containing non-finite outside the scale range (`stat_density()`).\nRemoved 1 row containing non-finite outside the scale range (`stat_density()`).\nRemoved 1 row containing non-finite outside the scale range (`stat_density()`).\nRemoved 1 row containing non-finite outside the scale range (`stat_density()`).\n\n\nWarning: Removed 4 rows containing non-finite outside the scale range\n(`stat_density()`).\nRemoved 4 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\nWarning: Removed 5 rows containing non-finite outside the scale range\n(`stat_density()`).\nRemoved 5 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\nggsave(path_fig2a_seeds, width=13, height=20)"
  },
  {
    "objectID": "reproduction/reproduction.html#figure-3",
    "href": "reproduction/reproduction.html#figure-3",
    "title": "Reproduction",
    "section": "Figure 3",
    "text": "Figure 3\n\n# Create sub-plots\np1 &lt;- create_plot(base_f3,\n                  group=\"shift\",\n                  title=\"Baseline\",\n                  ylab=\"Standardised density of patient in queue\")\np2 &lt;- create_plot(exc_f3,\n                  group=\"shift\",\n                  title=\"Exclusive-use\",\n                  xlab=\"Patient wait time (min)\",\n                  xlim=c(0, 300),\n                  breaks_width=100)\np3 &lt;- create_plot(two_f3,\n                  group=\"shift\",\n                  title=\"Double angio INRs\",\n                  xlim=c(0, 250))\n\n# Arrange in a single figure\nggarrange(p1, p2, p3, nrow=1,\n          common.legend=TRUE, legend=\"bottom\",\n          labels=c(\"A\", \"B\", \"C\"))\n\nWarning: Removed 5 rows containing non-finite outside the scale range\n(`stat_density()`).\nRemoved 5 rows containing non-finite outside the scale range\n(`stat_density()`).\nRemoved 5 rows containing non-finite outside the scale range\n(`stat_density()`).\nRemoved 5 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\nWarning: Removed 1 row containing non-finite outside the scale range (`stat_density()`).\nRemoved 1 row containing non-finite outside the scale range (`stat_density()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\nRemoved 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\nggsave(path_fig3)\n\nSaving 7 x 5 in image"
  },
  {
    "objectID": "reproduction/reproduction.html#time-elapsed",
    "href": "reproduction/reproduction.html#time-elapsed",
    "title": "Reproduction",
    "section": "Time elapsed",
    "text": "Time elapsed\n\nif (isTRUE(run)) {\n  end.time &lt;- Sys.time()\n  elapsed.time &lt;- round((end.time - start.time), 3)\n  elapsed.time\n}"
  },
  {
    "objectID": "quarto_site/license.html",
    "href": "quarto_site/license.html",
    "title": "Open Source License",
    "section": "",
    "text": "This repository is licensed under the [license].\n\n\n\n\n\n\nView license\n\n\n\n\n\nGNU GENERAL PUBLIC LICENSE Version 3, 29 June 2007 Copyright ¬© 2007 Free Software Foundation, Inc.¬†http://fsf.org/\nEveryone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\nPreamble\nThe GNU General Public License is a free, copyleft license for software and other kinds of works.\nThe licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change all versions of a program‚Äìto make sure it remains free software for all its users. We, the Free Software Foundation, use the GNU General Public License for most of our software; it applies also to any other work released this way by its authors. You can apply it to your programs, too.\nWhen we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.\nTo protect your rights, we need to prevent others from denying you these rights or asking you to surrender the rights. Therefore, you have certain responsibilities if you distribute copies of the software, or if you modify it: responsibilities to respect the freedom of others.\nFor example, if you distribute copies of such a program, whether gratis or for a fee, you must pass on to the recipients the same freedoms that you received. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights.\nDevelopers that use the GNU GPL protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License giving you legal permission to copy, distribute and/or modify it.\nFor the developers‚Äô and authors‚Äô protection, the GPL clearly explains that there is no warranty for this free software. For both users‚Äô and authors‚Äô sake, the GPL requires that modified versions be marked as changed, so that their problems will not be attributed erroneously to authors of previous versions.\nSome devices are designed to deny users access to install or run modified versions of the software inside them, although the manufacturer can do so. This is fundamentally incompatible with the aim of protecting users‚Äô freedom to change the software. The systematic pattern of such abuse occurs in the area of products for individuals to use, which is precisely where it is most unacceptable. Therefore, we have designed this version of the GPL to prohibit the practice for those products. If such problems arise substantially in other domains, we stand ready to extend this provision to those domains in future versions of the GPL, as needed to protect the freedom of users.\nFinally, every program is threatened constantly by software patents. States should not allow patents to restrict development and use of software on general-purpose computers, but in those that do, we wish to avoid the special danger that patents applied to a free program could make it effectively proprietary. To prevent this, the GPL assures that patents cannot be used to render the program non-free.\nThe precise terms and conditions for copying, distribution and modification follow.\nTERMS AND CONDITIONS\n\nDefinitions.\n\n‚ÄúThis License‚Äù refers to version 3 of the GNU General Public License.\n‚ÄúCopyright‚Äù also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.\n‚ÄúThe Program‚Äù refers to any copyrightable work licensed under this License. Each licensee is addressed as ‚Äúyou‚Äù. ‚ÄúLicensees‚Äù and ‚Äúrecipients‚Äù may be individuals or organizations.\nTo ‚Äúmodify‚Äù a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a ‚Äúmodified version‚Äù of the earlier work or a work ‚Äúbased on‚Äù the earlier work.\nA ‚Äúcovered work‚Äù means either the unmodified Program or a work based on the Program.\nTo ‚Äúpropagate‚Äù a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.\nTo ‚Äúconvey‚Äù a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.\nAn interactive user interface displays ‚ÄúAppropriate Legal Notices‚Äù to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.\n\nSource Code. The ‚Äúsource code‚Äù for a work means the preferred form of the work for making modifications to it. ‚ÄúObject code‚Äù means any non-source form of a work.\n\nA ‚ÄúStandard Interface‚Äù means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.\nThe ‚ÄúSystem Libraries‚Äù of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A ‚ÄúMajor Component‚Äù, in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.\nThe ‚ÄúCorresponding Source‚Äù for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work‚Äôs System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.\nThe Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.\nThe Corresponding Source for a work in source code form is that same work.\n\nBasic Permissions. All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.\n\nYou may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.\nConveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.\n\nProtecting Users‚Äô Legal Rights From Anti-Circumvention Law. No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.\n\nWhen you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work‚Äôs users, your or third parties‚Äô legal rights to forbid circumvention of technological measures.\n\nConveying Verbatim Copies. You may convey verbatim copies of the Program‚Äôs source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.\n\nYou may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.\n\nConveying Modified Source Versions. You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:\n\n\nThe work must carry prominent notices stating that you modified it, and giving a relevant date.\nThe work must carry prominent notices stating that it is released under this License and any conditions added under section 7. This requirement modifies the requirement in section 4 to ‚Äúkeep intact all notices‚Äù.\nYou must license the entire work, as a whole, under this License to anyone who comes into possession of a copy. This License will therefore apply, along with any applicable section 7 additional terms, to the whole of the work, and all its parts, regardless of how they are packaged. This License gives no permission to license the work in any other way, but it does not invalidate such permission if you have separately received it.\nIf the work has interactive user interfaces, each must display Appropriate Legal Notices; however, if the Program has interactive interfaces that do not display Appropriate Legal Notices, your work need not make them do so.\n\nA compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an ‚Äúaggregate‚Äù if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation‚Äôs users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.\n\nConveying Non-Source Forms. You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:\n\n\nConvey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by the Corresponding Source fixed on a durable physical medium customarily used for software interchange.\nConvey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by a written offer, valid for at least three years and valid for as long as you offer spare parts or customer support for that product model, to give anyone who possesses the object code either (1) a copy of the Corresponding Source for all the software in the product that is covered by this License, on a durable physical medium customarily used for software interchange, for a price no more than your reasonable cost of physically performing this conveying of source, or (2) access to copy the Corresponding Source from a network server at no charge.\nConvey individual copies of the object code with a copy of the written offer to provide the Corresponding Source. This alternative is allowed only occasionally and noncommercially, and only if you received the object code with such an offer, in accord with subsection 6b.\nConvey the object code by offering access from a designated place (gratis or for a charge), and offer equivalent access to the Corresponding Source in the same way through the same place at no further charge. You need not require recipients to copy the Corresponding Source along with the object code. If the place to copy the object code is a network server, the Corresponding Source may be on a different server (operated by you or a third party) that supports equivalent copying facilities, provided you maintain clear directions next to the object code saying where to find the Corresponding Source. Regardless of what server hosts the Corresponding Source, you remain obligated to ensure that it is available for as long as needed to satisfy these requirements.\nConvey the object code using peer-to-peer transmission, provided you inform other peers where the object code and Corresponding Source of the work are being offered to the general public at no charge under subsection 6d.\n\nA separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.\nA ‚ÄúUser Product‚Äù is either (1) a ‚Äúconsumer product‚Äù, which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, ‚Äúnormally used‚Äù refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.\n‚ÄúInstallation Information‚Äù for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.\nIf you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).\nThe requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.\nCorresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.\n\nAdditional Terms. ‚ÄúAdditional permissions‚Äù are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.\n\nWhen you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.\nNotwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:\n\nDisclaiming warranty or limiting liability differently from the terms of sections 15 and 16 of this License; or\nRequiring preservation of specified reasonable legal notices or author attributions in that material or in the Appropriate Legal Notices displayed by works containing it; or\nProhibiting misrepresentation of the origin of that material, or requiring that modified versions of such material be marked in reasonable ways as different from the original version; or\nLimiting the use for publicity purposes of names of licensors or authors of the material; or\nDeclining to grant rights under trademark law for use of some trade names, trademarks, or service marks; or\nRequiring indemnification of licensors and authors of that material by anyone who conveys the material (or modified versions of it) with contractual assumptions of liability to the recipient, for any liability that these contractual assumptions directly impose on those licensors and authors.\n\nAll other non-permissive additional terms are considered ‚Äúfurther restrictions‚Äù within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.\nIf you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.\nAdditional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.\n\nTermination. You may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).\n\nHowever, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.\nMoreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.\nTermination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.\n\nAcceptance Not Required for Having Copies. You are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.\nAutomatic Licensing of Downstream Recipients. Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.\n\nAn ‚Äúentity transaction‚Äù is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party‚Äôs predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.\nYou may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.\n\nPatents. A ‚Äúcontributor‚Äù is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor‚Äôs ‚Äúcontributor version‚Äù.\n\nA contributor‚Äôs ‚Äúessential patent claims‚Äù are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, ‚Äúcontrol‚Äù includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.\nEach contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor‚Äôs essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.\nIn the following three paragraphs, a ‚Äúpatent license‚Äù is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To ‚Äúgrant‚Äù such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.\nIf you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. ‚ÄúKnowingly relying‚Äù means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient‚Äôs use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.\nIf, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.\nA patent license is ‚Äúdiscriminatory‚Äù if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.\nNothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.\n\nNo Surrender of Others‚Äô Freedom. If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.\nUse with the GNU Affero General Public License. Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU Affero General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the special requirements of the GNU Affero General Public License, section 13, concerning interaction through a network will apply to the combination as such.\nRevised Versions of this License. The Free Software Foundation may publish revised and/or new versions of the GNU General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.\n\nEach version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU General Public License ‚Äúor any later version‚Äù applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU General Public License, you may choose any version ever published by the Free Software Foundation.\nIf the Program specifies that a proxy can decide which future versions of the GNU General Public License can be used, that proxy‚Äôs public statement of acceptance of a version permanently authorizes you to choose that version for the Program.\nLater license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.\n\nDisclaimer of Warranty. THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM ‚ÄúAS IS‚Äù WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.\nLimitation of Liability. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\nInterpretation of Sections 15 and 16. If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.\n\nEND OF TERMS AND CONDITIONS\nHow to Apply These Terms to Your New Programs\nIf you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.\nTo do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the ‚Äúcopyright‚Äù line and a pointer to where the full notice is found.\n&lt;one line to give the program‚Äôs name and a brief idea of what it does.&gt; Copyright (C)  \nThis program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\nYou should have received a copy of the GNU General Public License along with this program. If not, see http://www.gnu.org/licenses/.\nAlso add information on how to contact you by electronic and paper mail.\nIf the program does terminal interaction, make it output a short notice like this when it starts in an interactive mode:\n Copyright (C)   This program comes with ABSOLUTELY NO WARRANTY; for details type show w'.  This is free software, and you are welcome to redistribute it under certain conditions; typeshow c‚Äô for details.\nThe hypothetical commands show w' andshow c‚Äô should show the appropriate parts of the General Public License. Of course, your program‚Äôs commands might be different; for a GUI interface, you would use an ‚Äúabout box‚Äù.\nYou should also get your employer (if you work as a programmer) or school, if any, to sign a ‚Äúcopyright disclaimer‚Äù for the program, if necessary. For more information on this, and how to apply and follow the GNU GPL, see http://www.gnu.org/licenses/.\nThe GNU General Public License does not permit incorporating your program into proprietary programs. If your program is a subroutine library, you may consider it more useful to permit linking proprietary applications with the library. If this is what you want to do, use the GNU Lesser General Public License instead of this License. But first, please read http://www.gnu.org/philosophy/why-not-lgpl.html.\n\n\n\n\nThis is aligned with the original study, who shared their code under [license].\n\n\n\n\n\n\nView license\n\n\n\n\n\n\n[Embedded license]\n\n\n\n\nThe original study was published in the journal ‚Äú[Journal name]‚Äù. They distributed the article under [Add more details about license]\n\n\n\n\n\n\nView copyright statement from journal"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reproducing Huang et al. 2019",
    "section": "",
    "text": "This book captures the reproduction of:\n\nHuang S, Maingard J, Kok HK, Barras CD, Thijs V, Chandra RV, Brooks DM and Asadi H. Optimizing Resources for Endovascular Clot Retrieval for Acute Ischemic Stroke, a Discrete Event Simulation. Frontiers in Neurology 10, 653 (2019). https://doi.org/10.3389/fneur.2019.00653.\n\nUse the navigation bar above to view:\n\nOriginal study - the original study article and associated artefacts.\nReproduction - code and documentation from reproduction of the model.\nEvaluation - describes model reproduction success and compares original study against guidelines for sharing research, criteria for journal reproducibility guidelines, and article reporting guidelines.\nLogbook - chronological entries detailing reproduction work.\nSummary - summary of the computational reproducibility assessment."
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Reproducing Huang et al. 2019",
    "section": "",
    "text": "This book captures the reproduction of:\n\nHuang S, Maingard J, Kok HK, Barras CD, Thijs V, Chandra RV, Brooks DM and Asadi H. Optimizing Resources for Endovascular Clot Retrieval for Acute Ischemic Stroke, a Discrete Event Simulation. Frontiers in Neurology 10, 653 (2019). https://doi.org/10.3389/fneur.2019.00653.\n\nUse the navigation bar above to view:\n\nOriginal study - the original study article and associated artefacts.\nReproduction - code and documentation from reproduction of the model.\nEvaluation - describes model reproduction success and compares original study against guidelines for sharing research, criteria for journal reproducibility guidelines, and article reporting guidelines.\nLogbook - chronological entries detailing reproduction work.\nSummary - summary of the computational reproducibility assessment."
  },
  {
    "objectID": "index.html#project-team",
    "href": "index.html#project-team",
    "title": "Reproducing Huang et al. 2019",
    "section": "Project team",
    "text": "Project team\n\n\nConducting this reproduction:\n\nAmy Heather \n\nProviding support during the reproduction:\n\nThomas Monks \nAlison Harper \n\nOther members of the team on STARS:\n\nNavonil Mustafee \nAndrew Mayne"
  },
  {
    "objectID": "index.html#protocol",
    "href": "index.html#protocol",
    "title": "Reproducing Huang et al. 2019",
    "section": "Protocol",
    "text": "Protocol\nThe protocol for this work is summarised in the diagram below and archived on Zenodo:\n\nHeather, A., Monks, T., Harper, A., Mustafee, N., & Mayne, A. (2024). Protocol for assessing the computational reproducibility of discrete-event simulation models on STARS. Zenodo. https://doi.org/10.5281/zenodo.12179846.\n\n\n\n\nWorkflow for computational reproducibility assessment"
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "Reproducing Huang et al. 2019",
    "section": "Citation",
    "text": "Citation\nAPA: Heather A., Monks T., Harper A. (2024). STARS: Computational reproducibility of Huang et al.¬†2019 (version 0.1.0). URL: https://github.com/pythonhealthdatascience/stars-reproduce-huang-2019\nSee CITATION.cff and citation_bibtex.bib for alternative formats."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Reproducing Huang et al. 2019",
    "section": "License",
    "text": "License\nSee License page."
  },
  {
    "objectID": "CHANGELOG.html",
    "href": "CHANGELOG.html",
    "title": "Changelog",
    "section": "",
    "text": "All notable changes to this project will be documented in this file.\nThe format is based on Keep a Changelog, and this project adheres to Semantic Versioning. Dates formatted as YYYY-MM-DD as per ISO standard.\n\n\nFirst release with defined scope for reproduction.\n\n\n\nCode from original study\nArticle\nPlanned scope for reproduction\n\n\n\n\n\nModified template to be relevant to Huang et al.¬†2019"
  },
  {
    "objectID": "CHANGELOG.html#v0.1.0---2024-07-04",
    "href": "CHANGELOG.html#v0.1.0---2024-07-04",
    "title": "Changelog",
    "section": "",
    "text": "First release with defined scope for reproduction.\n\n\n\nCode from original study\nArticle\nPlanned scope for reproduction\n\n\n\n\n\nModified template to be relevant to Huang et al.¬†2019"
  },
  {
    "objectID": "quarto_site/reproduction_readme.html",
    "href": "quarto_site/reproduction_readme.html",
    "title": "README for reproduction",
    "section": "",
    "text": "Please note: This is a template README and has not yet been completed\n\n\n\nTBC\n\n\n\nTBC\n\n\n\n\n\nTBC\n\n\n\nTBC\n\n\n\nTBC\n\n\n\n\nTBC\n\n\n\n\nTBC\n\n\n\nTBC"
  },
  {
    "objectID": "quarto_site/reproduction_readme.html#model-summary",
    "href": "quarto_site/reproduction_readme.html#model-summary",
    "title": "README for reproduction",
    "section": "",
    "text": "TBC"
  },
  {
    "objectID": "quarto_site/reproduction_readme.html#scope-of-the-reproduction",
    "href": "quarto_site/reproduction_readme.html#scope-of-the-reproduction",
    "title": "README for reproduction",
    "section": "",
    "text": "TBC"
  },
  {
    "objectID": "quarto_site/reproduction_readme.html#reproducing-these-results",
    "href": "quarto_site/reproduction_readme.html#reproducing-these-results",
    "title": "README for reproduction",
    "section": "",
    "text": "TBC\n\n\n\nTBC\n\n\n\nTBC"
  },
  {
    "objectID": "quarto_site/reproduction_readme.html#reproduction-specs-and-runtime",
    "href": "quarto_site/reproduction_readme.html#reproduction-specs-and-runtime",
    "title": "README for reproduction",
    "section": "",
    "text": "TBC"
  },
  {
    "objectID": "quarto_site/reproduction_readme.html#citation",
    "href": "quarto_site/reproduction_readme.html#citation",
    "title": "README for reproduction",
    "section": "",
    "text": "TBC"
  },
  {
    "objectID": "quarto_site/reproduction_readme.html#license",
    "href": "quarto_site/reproduction_readme.html#license",
    "title": "README for reproduction",
    "section": "",
    "text": "TBC"
  },
  {
    "objectID": "quarto_site/study_publication.html",
    "href": "quarto_site/study_publication.html",
    "title": "Publication",
    "section": "",
    "text": "View at: https://github.com/pythonhealthdatascience/stars-reproduce-huang-2019/tree/main/original_study/desECR\nCode from: https://github.com/shiweih/desECR"
  },
  {
    "objectID": "quarto_site/study_publication.html#code-and-data",
    "href": "quarto_site/study_publication.html#code-and-data",
    "title": "Publication",
    "section": "",
    "text": "View at: https://github.com/pythonhealthdatascience/stars-reproduce-huang-2019/tree/main/original_study/desECR\nCode from: https://github.com/shiweih/desECR"
  },
  {
    "objectID": "quarto_site/study_publication.html#journal-article",
    "href": "quarto_site/study_publication.html#journal-article",
    "title": "Publication",
    "section": "Journal article",
    "text": "Journal article\nArticle from: https://doi.org/10.3389/fneur.2019.00653"
  },
  {
    "objectID": "quarto_site/study_publication.html#supplementary-materials",
    "href": "quarto_site/study_publication.html#supplementary-materials",
    "title": "Publication",
    "section": "Supplementary materials",
    "text": "Supplementary materials\nThe supplementary material is an additional image saved as a .TIFF file:\n\n\n\nSupplementary figure"
  },
  {
    "objectID": "quarto_site/study_publication.html#interactive-web-app",
    "href": "quarto_site/study_publication.html#interactive-web-app",
    "title": "Publication",
    "section": "Interactive web app",
    "text": "Interactive web app\nThe paper also links to an interactive web app for the model which can be found at: https://rebrand.ly/desECR11 (which redirects to https://compneuro.shinyapps.io/desECR11/).\nThe simulation also links to https://beta.cloudes.me/loadShare?simId=17588, stating that it can provide the details of the simulation (although this link does not work, if you login to CLOUDES, you can identify what appears to be a copy of that model under the ID 17482 or by searching ‚ÄúHuang‚Äù)."
  },
  {
    "objectID": "evaluation/artefacts.html",
    "href": "evaluation/artefacts.html",
    "title": "STARS framework",
    "section": "",
    "text": "Please note: This is a template page and has not yet been completed\nThis page evaluates the extent to which the original study meets the recommendations from the STARS framework for the sharing of code and associated materials from discrete-event simulation models (Monks, Harper, and Mustafee (2024)).\nOf the 8 essential STARS components:\n\n\nX were met fully (‚úÖ)\nX was met partially (üü°)\nX was not met (‚ùå)\n\nOf the 5 optional STARS components:\n\n\nX was met fully (‚úÖ)\nX were not met (‚ùå)\n\n\n\n\n\n\n\n\n\n\n\nComponent\nDescription\nMet by study?\nEvidence/location\n\n\n\n\nEssential components\n\n\n\n\n\nOpen license\nFree and open-source software (FOSS) license (e.g.¬†MIT, GNU Public License (GPL))\n\n\n\n\nDependency management\nSpecify software libraries, version numbers and sources (e.g.¬†dependency management tools like virtualenv, conda, poetry)\n\n\n\n\nFOSS model\nCoded in FOSS language (e.g.¬†R, Julia, Python)\n\n\n\n\nMinimum documentation\nMinimal instructions (e.g.¬†in README) that overview (a) what model does, (b) how to install and run model to obtain results, and (c) how to vary parameters to run new experiments\n\n\n\n\nORCID\nORCID for each study author\n\n\n\n\nCitation information\nInstructions on how to cite the research artefact (e.g.¬†CITATION.cff file)\n\n\n\n\nRemote code repository\nCode available in a remote code repository (e.g.¬†GitHub, GitLab, BitBucket)\n\n\n\n\nOpen science archive\nCode stored in an open science archive with FORCE11 compliant citation and guaranteed persistance of digital artefacts (e.g.¬†Figshare, Zenodo, the Open Science Framework (OSF), and the Computational Modeling in the Social and Ecological Sciences Network (CoMSES Net))\n\n\n\n\nOptional components\n\n\n\n\n\nEnhanced documentation\nOpen and high quality documentation on how the model is implemented and works (e.g.¬†via notebooks and markdown files, brought together using software like Quarto and Jupyter Book). Suggested content includes:‚Ä¢ Plain english summary of project and model‚Ä¢ Clarifying license‚Ä¢ Citation instructions‚Ä¢ Contribution instructions‚Ä¢ Model installation instructions‚Ä¢ Structured code walk through of model‚Ä¢ Documentation of modelling cycle using TRACE‚Ä¢ Annotated simulation reporting guidelines‚Ä¢ Clear description of model validation including its intended purpose\n\n\n\n\nDocumentation hosting\nHost documentation (e.g.¬†with GitHub pages, GitLab pages, BitBucket Cloud, Quarto Pub)\n\n\n\n\nOnline coding environment\nProvide an online environment where users can run and change code (e.g.¬†BinderHub, Google Colaboratory, Deepnote)\n\n\n\n\nModel interface\nProvide web application interface to the model so it is accessible to less technical simulation users\n\n\n\n\nWeb app hosting\nHost web app online (e.g.¬†Streamlit Community Cloud, ShinyApps hosting)\n\n\n\n\n\n\n\n\n\nReferences\n\nMonks, Thomas, Alison Harper, and Navonil Mustafee. 2024. ‚ÄúTowards Sharing Tools and Artefacts for Reusable Simulations in Healthcare.‚Äù Journal of Simulation 0 (0): 1‚Äì20. https://doi.org/10.1080/17477778.2024.2347882."
  },
  {
    "objectID": "evaluation/reproduction_success.html",
    "href": "evaluation/reproduction_success.html",
    "title": "Reproduction success",
    "section": "",
    "text": "Please note: This is a template page and has not yet been completed\nOf the X items in the scope, X% (X out of X) were considered to be successfully reproduced."
  },
  {
    "objectID": "evaluation/reproduction_success.html#time-to-completion",
    "href": "evaluation/reproduction_success.html#time-to-completion",
    "title": "Reproduction success",
    "section": "Time-to-completion",
    "text": "Time-to-completion\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nfrom plotly import graph_objects as go\n\n# List of time to complete each item ------------------------------------------\ntime_list = [\n    [516, 'In-text result 1'],\n    [1193, 'In-text result 2'],\n    [np.nan, 'Figure 2'],\n    [np.nan, 'Figure 3'],\n    [np.nan, 'Figure 4'],\n    [np.nan, 'Figure 5'],\n    [np.nan, 'Supplementary figure'],\n    [np.nan, 'In-text result 3']]\n\n\n# Create times dataframe...\n\n# Convert to dataframe\ntimes = pd.DataFrame(time_list, columns=['time', 'item'])\n\n# Sort the DataFrame by 'time' to correctly plot the ECDF\ntimes_clean = times.sort_values(by='time').reset_index(drop=True)\n\n# Compute the ECDF values\ntimes_clean['ecdf'] = np.arange(1, len(times_clean) + 1) / len(times_clean) * 100\n\n# AMEND figure 2 set ecdf to that of the last item\n#times_clean.loc[times_clean['item'] == 'Figure 2', 'ecdf'] = np.nan\n#times_clean.loc[times_clean['item'] == 'Figure 2', 'ecdf'] = 25\n\n# Insert the starting point (0, 0) in the ECDF\ntimes_clean.loc[-1] = [0, 'Start', 0]\ntimes_clean.index = times_clean.index + 1\ntimes_clean = times_clean.sort_index()\n\n# Convert time to hours\ntimes_clean['hours'] = times_clean['time']/60\n\n\n# Add points for the 90 degree turns in the line...\n\n# Adjust index so it goes 0 2 4 6...\ntimes_original = times_clean.copy()\ntimes_original.index = times_clean.index*2\n\ntimes_extra = times_clean.copy()\n# Get result from row 1 to final, and add another row with np.nan\nnext_result = times_clean['time'][1:].reset_index(drop=True)\nnext_result[len(next_result)] = np.nan\n# Replace time with those values (i.e. the value from the row below)\ntimes_extra['time'] = next_result\ntimes_extra['item'] = np.nan\n# Then adjust index so it goes 1 3 5 7...\ntimes_extra.index = (times_extra.index*2)+1\n\n# Combine (so have alternating rows from each due to odd and even indexes)\nplt_times = pd.concat([times_original, times_extra]).sort_index()\n\n# Convert time to hours\nplt_times['hours'] = plt_times['time']/60\n\nNon-interactive plot:\n\n# Create non-interactive plot with matplotlib...\n\nfig, ax = plt.subplots()\nax.plot(plt_times['hours'], plt_times['ecdf'], marker='.', markevery=2)\nplt.xlim(0, 40)\nplt.ylim(0, 100)\nplt.xlabel('Time elapsed (hours)')\nplt.ylabel('Percentage of items reproduced')\nax.yaxis.set_major_formatter(mtick.PercentFormatter())\nplt.show()\n\n\n\n\n\n\n\n\nInteractive plot:\n\n# Create interactive plot with plotly express...\n\n# Create plot using combination of scatter and line\nfig = go.Figure()\nfig.add_trace(go.Scatter(\n    x=plt_times['hours'], y=plt_times['ecdf'], mode='lines', hoverinfo='skip'))\nfig.add_trace(go.Scatter(\n    x=times_clean['hours'], y=times_clean['ecdf'],\n    mode='markers', marker=dict(color='blue', size=6),\n    hovertext=times_clean['item'], hoverlabel=dict(namelength=0),\n    hovertemplate = (\n        '%{hovertext}&lt;br&gt;Time: %{x:.1f} hours&lt;br&gt;Completion: %{y:.1f}%')))\n\n# Amend appearance\nfig.update_layout(xaxis_range=[0, 40], yaxis_range=[0, 100],\n                  showlegend=False)\n\n# Show plot, hiding toolbar and not allowing zoom\nfig.layout.xaxis.fixedrange = True\nfig.layout.yaxis.fixedrange = True\nfig.show(config={'displayModeBar': False})"
  },
  {
    "objectID": "evaluation/reproduction_success.html#reproduction-of-items-from-the-scope",
    "href": "evaluation/reproduction_success.html#reproduction-of-items-from-the-scope",
    "title": "Reproduction success",
    "section": "Reproduction of items from the scope",
    "text": "Reproduction of items from the scope"
  },
  {
    "objectID": "evaluation/reproduction_success.html#figure-x",
    "href": "evaluation/reproduction_success.html#figure-x",
    "title": "Reproduction success",
    "section": "Figure X",
    "text": "Figure X\nOriginal figure:\n\nReproduction:"
  },
  {
    "objectID": "evaluation/scope.html",
    "href": "evaluation/scope.html",
    "title": "Scope",
    "section": "",
    "text": "This page outlines the parts of the journal article which we will attempt to reproduce.\nAll images and quotes on this page are sourced from Huang et al. (2019)"
  },
  {
    "objectID": "evaluation/scope.html#within-scope",
    "href": "evaluation/scope.html#within-scope",
    "title": "Scope",
    "section": "Within scope",
    "text": "Within scope\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\n\n\n\nFIGURE 2 | Patient wait time under various simulation scenarios (A). Baseline scenario simulated using inputs from Table 1 (B). Exclusive-use scenario: IR patients can only utilize angioIR (C). Two angioINRs scenario: 2 angioINRs, no angioIRs. Standardized density of patients in queue: the probability density of patients who are waiting standardized to patients who are not waiting. Huang et al. (2019)\n\n\n\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\n\n\n\nFIGURE 3 | The effect of increasing working hours on ECR patient wait time at angioINR (A). Baseline scenario (B). Exclusive-use scenario (C). Two angioINRs scenario. Standardized density of patients in queue: the probability density of patients who are waiting standardized to patients who are not waiting. Huang et al. (2019)\n\n\n\n\n\n\n\n\n\n\n\nFigure 4\n\n\n\n\n\n\n\n\nFIGURE 4 | Disability-free life gained under various scenarios. Huang et al. (2019)\n\n\n\n\n\n\n\n\n\n\n\nFigure 5\n\n\n\n\n\n\n\n\nFIGURE 5 | A comparison of the utilization of angioINR by ECR patients under various scenarios. Huang et al. (2019)\n\n\n\n\n\n\n\n\n\n\n\nSupplementary figure\n\n\n\n\n\n\n\n\nSupplementary Figure | Increasing ECR patient volume on service bottleneck. Standardized density of patients in queue: the probability density of patients who are waiting standardized to patients who are not waiting. (A) Baseline scenario. (B) Doubling ECR patients in baseline scenario. (C) Tripping ECR patients in baseline scenario. Huang et al. (2019)\n\n\n\n\n\n\n\n\n\n\n\nIn-text result 1\n\n\n\n\n\n‚ÄúExclusive-Use Scenario. In this scenario, the overall wait time probability at angioINR was reduced compared to baseline (red line in Figure 2B compared to Figure 2A). This represents a decrease in ECR patient wait time for angioINR by an average of 6 min.‚Äù Huang et al. (2019)\n\n\n\n\n\n\n\n\n\nIn-text result 2\n\n\n\n\n\n‚ÄúTwo angioINRs Scenario. This scenario simulates the effect a facility upgrade to two biplane angiographic suites, but without additional staff changes. The wait time probability at angioINR was reduced compared to baseline (Figure 2C). The reduction represents an average of 4 min less in queue for angioINR.‚Äù Huang et al. (2019)\n\n\n\n\n\n\n\n\n\nIn-text result 3\n\n\n\n\n\n‚ÄúExtended Schedule Scenario. The wait time probability at angioINR in the exclusive- use scenario was further reduced by extended work hours (Figure 3B). In contrast, work extension did not affect baseline or the 2 angioINRs scenario (Figures 3A,C). For the baseline scenario, 1 and 2 h of extra work resulted in an average wait time of 1.7 and 0.9 min reduction, respectively. For the 2 angioINRs scenario, 1 and 2 h of extra work resulted in an average wait time gain of 1 and 0.3 min, respectively.‚Äù Huang et al. (2019)"
  },
  {
    "objectID": "evaluation/scope.html#outside-scope",
    "href": "evaluation/scope.html#outside-scope",
    "title": "Scope",
    "section": "Outside scope",
    "text": "Outside scope\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nDiagram of patient flow through the model.\n\n\n\nFIGURE 1 | A schematic diagram of our discrete event model of an ECR service from Emergency to angiography suite. CT, Computed Tomography; AIS, Acute Ischemic Stroke; LVO, Large Vessel Occlusion; ECR, Endovascular Clot Retrieval; IR, Interventional Radiology; INR, Interventional Neuroradiology. Huang et al. (2019)\n\n\n\n\n\n\n\n\n\n\n\nTable 1\n\n\n\n\n\nParameters for the model.\n\n\n\nTABLE 1 | DES model inputs. (A) Human and physical resources. (B) Patient statistics. Huang et al. (2019)\n\n\n\n\n\n\n\n\nFIGURE 2 | Patient wait time under various simulation scenarios (A). Baseline scenario simulated using inputs from Table 1 (B). Exclusive-use scenario: IR patients can only utilize angioIR (C). Two angioINRs scenario: 2 angioINRs, no angioIRs. Standardized density of patients in queue: the probability density of patients who are waiting standardized to patients who are not waiting. Huang et al. (2019)\nFIGURE 3 | The effect of increasing working hours on ECR patient wait time at angioINR (A). Baseline scenario (B). Exclusive-use scenario (C). Two angioINRs scenario. Standardized density of patients in queue: the probability density of patients who are waiting standardized to patients who are not waiting. Huang et al. (2019)\nFIGURE 4 | Disability-free life gained under various scenarios. Huang et al. (2019)\nFIGURE 5 | A comparison of the utilization of angioINR by ECR patients under various scenarios. Huang et al. (2019)\nSupplementary Figure | Increasing ECR patient volume on service bottleneck. Standardized density of patients in queue: the probability density of patients who are waiting standardized to patients who are not waiting. (A) Baseline scenario. (B) Doubling ECR patients in baseline scenario. (C) Tripping ECR patients in baseline scenario. Huang et al. (2019)\nFIGURE 1 | A schematic diagram of our discrete event model of an ECR service from Emergency to angiography suite. CT, Computed Tomography; AIS, Acute Ischemic Stroke; LVO, Large Vessel Occlusion; ECR, Endovascular Clot Retrieval; IR, Interventional Radiology; INR, Interventional Neuroradiology. Huang et al. (2019)\nTABLE 1 | DES model inputs. (A) Human and physical resources. (B) Patient statistics. Huang et al. (2019)"
  },
  {
    "objectID": "evaluation/reproduction_report.html",
    "href": "evaluation/reproduction_report.html",
    "title": "Summary report",
    "section": "",
    "text": "Please note: This is a template page and has not yet been completed"
  },
  {
    "objectID": "evaluation/reproduction_report.html#study",
    "href": "evaluation/reproduction_report.html#study",
    "title": "Summary report",
    "section": "Study",
    "text": "Study\n\n[Authors]. [Title]. [Journal] [Volume], [Edition] ([Year]). &lt;[URL]&gt;.\n\n[Paragraph summarising model]"
  },
  {
    "objectID": "evaluation/reproduction_report.html#computational-reproducibility",
    "href": "evaluation/reproduction_report.html#computational-reproducibility",
    "title": "Summary report",
    "section": "Computational reproducibility",
    "text": "Computational reproducibility\nSuccessfully reproduced X out of X (X%) of items from the scope in Xh Xm (X%).\nRequired troubleshooting:\n\n[List of required changes to code]\n\n\nItem XItem YFigure 4\n\n\n[One sentence description of item X]\n[Display side-by-side] \n\n\n[Set-up as for Item X]\n\n\n[Set-up as for Item X]"
  },
  {
    "objectID": "evaluation/reproduction_report.html#evaluation-against-guidelines",
    "href": "evaluation/reproduction_report.html#evaluation-against-guidelines",
    "title": "Summary report",
    "section": "Evaluation against guidelines",
    "text": "Evaluation against guidelines\n\n\n                                                \n\n\nContext: The original study repository was evaluated against criteria from journal badges relating to how open and reproducible the model is and against guidance for sharing artefacts from the STARS framework. The original study article and supplementary materials (excluding code) were evaluated against reporting guidelines for DES models: STRESS-DES, and guidelines adapted from ISPOR-SDM."
  },
  {
    "objectID": "logbook/posts/2024_07_11/index.html",
    "href": "logbook/posts/2024_07_11/index.html",
    "title": "Day 7",
    "section": "",
    "text": "Note\n\n\n\nTBC. Total time used: TBC. (TBC.)."
  },
  {
    "objectID": "logbook/posts/2024_07_11/index.html#running-model-with-30-replications-and-various-seeds",
    "href": "logbook/posts/2024_07_11/index.html#running-model-with-30-replications-and-various-seeds",
    "title": "Day 7",
    "section": "10.45-11.00: Running model with 30 replications and various seeds",
    "text": "10.45-11.00: Running model with 30 replications and various seeds\nIntermittently (condensed to 15 minutes, but was over a longer time) ran the baseline model with several different seeds, but 30 replications.\n\n\n\nFigure 2A with different seeds"
  },
  {
    "objectID": "logbook/posts/2024_07_11/index.html#timings",
    "href": "logbook/posts/2024_07_11/index.html#timings",
    "title": "Day 7",
    "section": "Timings",
    "text": "Timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 1228\n\n# Times from today\ntimes = [\n    ('10.45', '11.00')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 15m, or 0h 15m\nTotal used to date: 1243m, or 20h 43m\nTime remaining: 1157m, or 19h 17m\nUsed 51.8% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_07_04/index.html",
    "href": "logbook/posts/2024_07_04/index.html",
    "title": "Day 2",
    "section": "",
    "text": "Note\n\n\n\nDefined scope and problem-solving renv. Total time used: 2h 29m (6.2%)"
  },
  {
    "objectID": "logbook/posts/2024_07_04/index.html#untimed-set-up-rstudio-and-test-quarto-site-with-r",
    "href": "logbook/posts/2024_07_04/index.html#untimed-set-up-rstudio-and-test-quarto-site-with-r",
    "title": "Day 2",
    "section": "Untimed: Set up RStudio and test quarto site with R",
    "text": "Untimed: Set up RStudio and test quarto site with R\nI did not time this as it is not specific to this reproduction, but additional set-up as not done reproduction in R yet (since the test-run was conducted in Python).\nThis involved installing/updating RStudio, learning how to run and work with a quarto book on that platform, and and troubleshooting any issues in getting the quarto book up and running.\n\nEnvironment\n\nUpdating to the latest version of RStudio, as suggested in the Quarto docs\nInstalling renv: install.packages(\"renv\")\nSetting the working directory: setwd(\"~/Documents/stars/stars-reproduce-huang-2019\")\nInitialised an empty R environment: renv::init(bare=TRUE)\nSet renv to use explicit dependencies: renv::settings$snapshot.type(\"explicit\")\nCreated a DESCRIPTION file\nRan renv::snapshot() which returned that project is not activated yet, so I selected option to Activate the project and use the project library. This generated an .Rprofile file.\nI then tried to open the project (File &gt; Open Project) but this failed. So I tried File &gt; New Project &gt; Existing Directory (which created an .Rproj file), then reran renv::init(bare=TRUE), then renv::snapshot(), and selected to install packages and then snapshot.\nSynced with GitHub (excluding .Rhistory, which is just a history of executed commands), using Git panel in top right corner\nAdd rmarkdown to DESCRIPTION and rebuilt environment (via renv::snapshot() and selecting to install)\n\nThen came across pkgr, and decided to give that a go, following their tutorial‚Ä¶\n\nDeleted renv and associated files (.Rprofile and renv.lock) with renv::deactivate(clean=TRUE)\nInstalled pkgr following the instructions on their latest release:\n\nsudo wget https://github.com/metrumresearchgroup/pkgr/releases/download/v3.1.1/pkgr_3.1.1_linux_amd64.tar.gz -O /tmp/pkgr.tar.gz\nsudo tar xzf /tmp/pkgr.tar.gz pkgr\nsudo mv pkgr /usr/local/bin/pkgr\nsudo chmod +x /usr/local/bin/pkgr\n\nCreated a pkgr.yml file\n\n# Version of pkgr.yml and, at this point, should always say Version: 1\nVersion: 1\n\n# pkgr will pull dependencies listed in DESCRIPTION\nDescriptions:\n- DESCRIPTION\n\n# If DESCRIPTION is provided, then this section only needs to include packages\n# that you would like to use for development purposes that are not in your\n# DESCRIPTION file (i.e. not formal dependencies of your package) - e.g. devtools\n# Packages:\n\n# Specify where to pull packages from\n# If list CRAN and MPN, will look on CRAN first, then MPN (which is useful for\n# dependencies no on CRAN). Can list a location for specific packages in Packages:\nRepos:\n  - CRAN: https://cran.rstudio.com\n  - MPN: https://mpn.metworx.com/snapshots/stable/2022-02-11 # used for mrgval\n\n# Specify Lockfile or Library to tell pkgr where to install packages\n# We are using renv to isolate our package environment - renv will tell pkgr where to install them\nLockfile:\n  Type: renv\n\nIn terminal, ran pkgr plan, but get error ARN[0000] error getting library path from renv: Error in loadNamespace(x) : there is no package called ‚Äòrenv‚Äô\n\nIf I start a new R session and run packageVersion(\"renv\"), it returns that it is installed\nTrying to reinstall with install.packages(\"renv\") makes no difference.\nTried restarting R and opening a new terminal\n\n\nI looked through issues and couldn‚Äôt spot anything, and then realised this was a fairly small package which hadn‚Äôt had any changes in half a year, so on reflection, probably not a reliable option to choose. So went back to set up similar to before of:\n\nrenv::init(bare=TRUE) with explicit snapshot\nrenv::snapshot() (and realised it didn‚Äôt update with change to DESCRIPTION before simply because I hadn‚Äôt put a comma after each package!)\n\nTo render the Quarto book (in a similar to way to how we did in VSCode), just click the Render button.\nNow, returning to what started this - trying to get the .TIFF supplementary file to display‚Ä¶\n\nAdd tiff to DESCRIPTION\nrenv::status() showed that the package was used but not installed, and renv::snapshot() with option 2 installed the package\n\n\n\nUsing specific versions\n\nAdd explict versions of R and packages to DESCRIPTION\nAttempted to downgrade tiff. renv::status() and renv::snapshot() did not noticed. From this issue, it appears that this should work for renv::install() and, indeed, that recognises it although get issue:\n\nWarning: failed to find source for 'tiff 0.1.11' in package repositories\nError: failed to retrieve package 'tiff@0.1.11'\n\nI checked the archive for tiff on CRAN and found there is a 0.1-11 (prior to the current 0.1-12)\nIf I deleted it (remove.packages(\"tiff\")) and then redid renv::snapshot(), it again would not notice the versions\nI tried to do it manually with remotes (rather than devtools as devtools has so many dependencies) - I installed remotes and then ran remotes::install_version(\"tiff\", \"0.1.11\"). This seemed successful, except packageVersion(\"tiff\") still returned 0.1.12? Although actually, on inspection, you can see it if 0.1.11. However, it wasn‚Äôt able to do that from DESCRIPTION.\nI removed it and tried again with a direct renv::install(\"tiff@0.1-11\") which was successful\nI then tried again with DESCRIPTION, but instead set it to tiff@0.1-11, which was successful likewise! And if it was tiff (==0.1-11)! So it appears its a bit fussy about matching up to the format in the CRAN archive .tar.gz files.\nI then found that renv::snapshot() ignores the version if it‚Äôs tiff (==0.1-11) but adheres if it is tiff@0.1-11 - yay!\n\nHaving finished with this experiment, I deleted and rebuilt with latest versions - but found it had errors installing them where defined like tiff@0.1-12. Hence, returned to tiff (==0.1-11), and just had to make sure to do renv::install() before renv::snapshot() (rather than rely on snapshot to install the packages).\n\n\nFixing GitHub action to render and publish the book\nWith no changes to GitHub action, had an error of:\n[14/18] quarto_site/study_publication.qmd\nError in file(filename, \"r\", encoding = encoding) : \n  cannot open the connection\nCalls: source -&gt; file\nIn addition: Warning message:\nIn file(filename, \"r\", encoding = encoding) :\n  cannot open file 'renv/activate.R': No such file or directory\nExecution halted\nError in file(filename, \"r\", encoding = encoding) : \n  cannot open the connection\nCalls: source -&gt; file\nIn addition: Warning message:\nIn file(filename, \"r\", encoding = encoding) :\n  cannot open file 'renv/activate.R': No such file or directory\nExecution halted\nProblem with running R found at /usr/bin/Rscript to check environment configurations.\nPlease check your installation of R.\n\nERROR: Error\n    at renderFiles (file:///opt/quarto/bin/quarto.js:78079:29)\n    at eventLoopTick (ext:core/01_core.js:153:7)\n    at async renderProject (file:///opt/quarto/bin/quarto.js:78477:25)\n    at async renderForPublish (file:///opt/quarto/bin/quarto.js:109332:33)\n    at async renderForPublish (file:///opt/quarto/bin/quarto.js:104864:24)\n    at async Object.publish1 [as publish] (file:///opt/quarto/bin/quarto.js:105349:26)\n    at async publishSite (file:///opt/quarto/bin/quarto.js:109369:38)\n    at async publish7 (file:///opt/quarto/bin/quarto.js:109588:61)\n    at async doPublish (file:///opt/quarto/bin/quarto.js:109548:13)\n    at async publishAction (file:///opt/quarto/bin/quarto.js:109559:9)\nError: Process completed with exit code 1\nAttempting to solve this‚Ä¶\n\nAdd installation of R and set up of R environment with actions from r-lib (trying setup-renv and setup-r-dependencies) for environment. However, it fails for installation of R dependencies with the error message:\n\nRun r-lib/actions/setup-r-dependencies@v2\nRun # Set site library path\nError in file(filename, \"r\", encoding = encoding) : \n  cannot open the connection\nCalls: source -&gt; file\nIn addition: Warning message:\nIn file(filename, \"r\", encoding = encoding) :\n  cannot open file 'renv/activate.R': No such file or directory\nExecution halted\nError: Process completed with exit code 1.\n\nBased on this forum post, I tried removing the .Rprofile from git\nThis seemed to improve slightly, although setup-r-dependencies then failed with an error in a pak subprocess seemingly for a package called ‚Äú.‚Äù. Tried switching to setup-renv (which bases on renv.lock) which was then successful! (although takes 4 minutes to install R dependencies, so 6m 55s total)"
  },
  {
    "objectID": "logbook/posts/2024_07_04/index.html#reading-the-article",
    "href": "logbook/posts/2024_07_04/index.html#reading-the-article",
    "title": "Day 2",
    "section": "14.14-14.31: Reading the article",
    "text": "14.14-14.31: Reading the article\nRead throughout and highlighted a copy of the article."
  },
  {
    "objectID": "logbook/posts/2024_07_04/index.html#define-scope-of-article",
    "href": "logbook/posts/2024_07_04/index.html#define-scope-of-article",
    "title": "Day 2",
    "section": "14.33-14.50: Define scope of article",
    "text": "14.33-14.50: Define scope of article\nWent through figures and tables to define scope (and convert and crop the .TIFF supplementary to .JPG so easier to display). From looking through text of article, identified a few extra results not in the figures: the quoted decrease in wait times. Although these are very related to the figures, as it wouldn‚Äôt be able to look at the figure and deduce the average wait time reduction, these represent additional results.\nThere was one line in the discussion that caught my attention - ‚ÄúThe quality of the ECR service appears to be robust to important parameters, such as the number of radiologists‚Äù - but I feel the interpretation of this is quite ambiguous (as to whether it is a model result or interpretation from other results), and doesn‚Äôt have anything specific to action, so will not include in scope."
  },
  {
    "objectID": "logbook/posts/2024_07_04/index.html#consensus-on-scope-with-tom",
    "href": "logbook/posts/2024_07_04/index.html#consensus-on-scope-with-tom",
    "title": "Day 2",
    "section": "15.05-15.10: Consensus on scope with Tom",
    "text": "15.05-15.10: Consensus on scope with Tom\nDiscussed with Tom (and he also had another look over afterwards). Happy with scope choices, and agree that the line from the discussion is simply too ambiguous to action."
  },
  {
    "objectID": "logbook/posts/2024_07_04/index.html#exploring-app-and-simulation-visualisation",
    "href": "logbook/posts/2024_07_04/index.html#exploring-app-and-simulation-visualisation",
    "title": "Day 2",
    "section": "15.35-15.43: Exploring app and simulation visualisation",
    "text": "15.35-15.43: Exploring app and simulation visualisation\nAs an addendum to the reading, explored the app and linked simulation configuration visualisation.\nFor the configuration, it just opened to the CLOUDES homepage, so I tried creating an account then going to the link (turns out you need an account to access). The link still did not work nor the ID, but when I search for ‚ÄúHuang‚Äù, I was able to find a diagram: https://beta.cloudes.me/loadSim?simId=17482&pageId=rTbqE (ID 17482). When run, this played through the simulation showing arrivals and queues etc."
  },
  {
    "objectID": "logbook/posts/2024_07_04/index.html#prepare-release",
    "href": "logbook/posts/2024_07_04/index.html#prepare-release",
    "title": "Day 2",
    "section": "15.44-15.47: Prepare release",
    "text": "15.44-15.47: Prepare release\nModified CHANGELOG and CITATION ahead of release."
  },
  {
    "objectID": "logbook/posts/2024_07_04/index.html#archived-on-zenodo",
    "href": "logbook/posts/2024_07_04/index.html#archived-on-zenodo",
    "title": "Day 2",
    "section": "15.55-15.58: Archived on Zenodo",
    "text": "15.55-15.58: Archived on Zenodo\nCreated GitHub release with archiving activated on Zenodo."
  },
  {
    "objectID": "logbook/posts/2024_07_04/index.html#look-over-code-and-set-up-environment",
    "href": "logbook/posts/2024_07_04/index.html#look-over-code-and-set-up-environment",
    "title": "Day 2",
    "section": "16.04-16.58: Look over code and set up environment",
    "text": "16.04-16.58: Look over code and set up environment\nNo dependency management, so will create renv based on the imports and the dates of the repository - with exception that article mentions:\n\nSimmer (version 4.1.0)\n\nThe article dates are:\n\nReceived - 31 March 2019\nAccepted - 4 June 2019\nPublished - 27 June 2019\n\nThe GitHub repository has two commits, both on 27 May 2019. As per protocol, will go with earliest of published and code, which is 27 May 2019.\nIt looks likely that all the relevant code will be in server.R (with ui.R just being for the ShinyApp, which is not in scope to reproduce, as it is not presented as a key result within the paper). As such, looking at the imports from that R script, and identifying versions on or prior to 27 May 2019‚Ä¶\n\nsimmer - https://cran.r-project.org/src/contrib/Archive/simmer/ - 4.2.2 (14 March 2019)\nsimmer.plot - https://cran.r-project.org/src/contrib/Archive/simmer.plot/ - 0.1.15 (10th March 2019)\nparallel - part of the core R distribution (so will come with version of R used)\ndplyr - https://cran.r-project.org/src/contrib/Archive/dplyr/ - 0.8.1 (14th May 2019)\nplotly - https://cran.r-project.org/src/contrib/Archive/plotly/ - 4.9.0 (10th April 2019)\ngridExtra - https://cran.r-project.org/src/contrib/Archive/gridExtra/ - 2.2.1 (29th February 2016, latest release)\nR - https://github.com/r-hub/rversions - 3.6.0 Planting of a Tree (26th April 2019)\n\nI‚Äôll set each of these to be max these versions, to help with dependency conflicts when set-up environment, but then convert to fixed versions once know what worked.\nCreated a DESCRIPTION file in reproduction/:\nTitle: huang2019\nDepends: \n    R (&lt;= 3.6)\nImports:\n    simmer (&lt;=4.2.2),\n    simmer.plot (&lt;=0.1.15),\n    dplyr (&lt;=0.8.1),\n    plotly (&lt;=4.9.0),\n    gridExtra (&lt;=2.2.1)\nWant to create another renv for that sub-folder (seperate to the renv in our main folder). To do so I ran the following commands in the console:\n\nsetwd(\"~/Documents/stars/stars-reproduce-huang-2019/reproduction\") (to move to reproduction/)\nrenv::deactivate()\nrenv::status() to confirm none were active\nrenv::init(bare=TRUE) and selected 1 for using the explicit dependencies from DESCRIPTION. This then restarted the R session and created and opened a new project: reproduction. It made the following new files and folders:\n\n\n.Rprofile (with just source(\"renv/activate.R\"))\nreproduction.Rproj\nrenv/ with the environment\n\n\nrenv::install() to install the packages and their specified versions. However, looking over the versions it planned to install, we had:\n\n\nsimmer [4.4.6.3]\nsimmer.plot [0.1.18]\ndplyr [1.1.4]\nplotly [4.10.4]\ngridExtra [2.3]\n\nI cancelled it and tried changing everything to explicit versions (==). This then matched up to what I wanted in the planned installs -\n\nsimmer [4.2.2]\nsimmer.plot [0.1.15]\ndplyr [1.1.4]\nplotly [4.9.0]\ngridExtra [2.2.1]\n\nHowever, there was an error with simmer: ERROR: compilation failed for package ‚Äòsimmer‚Äô, and so still just have renv in environment. I tried installing this specific version manually with remotes:\n\nrenv::install(\"remotes\")\nremotes::install_version(\"simmer\", \"4.2.2\")\n\nUnfortunately, the same error appeared. I then tried installing from GitHub instead of CRAN:\n\nremotes::install_github(\"r-simmer/simmer@v4.2.2\")\n\nBut this failed again as before.\nI tried focusing just on R to begin with, as I realised I have to install and change that manually. I followed this tutorial and ran in terminal:\n\nsudo snap install curl\nsudo apt-get update\nsudo apt-get install gdebi-core\nexport R_VERSION=3.6\ncurl -O https://cdn.rstudio.com/r/ubuntu-2204/pkgs/r-${R_VERSION}_1_amd64.deb\nsudo gdebi r-${R_VERSION}_1_amd64.deb\n\nHowever, I then got an error: Failed to open the software package. The package might be corrupted or you are not allowed to open the file. Check the permissions of the file.\nI switched over to the R documentation and clicked on Ubuntu and then ‚ÄúFor older R releases, see the corresponding README.‚Äù This said:\nTo obtain the latest R 3.6 packages, use:\n\ndeb https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/\nor\n\ndeb https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/\nor\n\ndeb https://cloud.r-project.org/bin/linux/ubuntu trusty-cran35/"
  },
  {
    "objectID": "logbook/posts/2024_07_04/index.html#timings",
    "href": "logbook/posts/2024_07_04/index.html#timings",
    "title": "Day 2",
    "section": "Timings",
    "text": "Timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 45\n\n# Times from today\ntimes = [\n    ('14.14', '14.31'),\n    ('14.33', '14.50'),\n    ('15.05', '15.10'),\n    ('15.35', '15.43'),\n    ('15.55', '15.58'),\n    ('16.04', '16.58')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 104m, or 1h 44m\nTotal used to date: 149m, or 2h 29m\nTime remaining: 2251m, or 37h 31m\nUsed 6.2% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_07_03/index.html",
    "href": "logbook/posts/2024_07_03/index.html",
    "title": "Day 1",
    "section": "",
    "text": "Note\n\n\n\nSet-up repository and add article and code. Total time used: 0h 45m (1.9%)"
  },
  {
    "objectID": "logbook/posts/2024_07_03/index.html#set-up-and-update-repository",
    "href": "logbook/posts/2024_07_03/index.html#set-up-and-update-repository",
    "title": "Day 1",
    "section": "11.53-12.20, 12.27-12.33: Set-up and update repository",
    "text": "11.53-12.20, 12.27-12.33: Set-up and update repository\nI have previously (Friday 21st June 2024) sent an email to the corresponding author (Dr.¬†Shiwei Huang) to inform about the study, using the template email from our protocol.\nToday, used template repository to create this repository and updated it to be relevant to Huang et al.¬†2019 - updated..\n\nREADME\nHome page (index.qmd)\nLogbook\nCITATION.cff\n_quarto.yml\n\nFrom a quick look at their code repository, can see they use a GNU General Public License version 3. The requirements of this license are to:\n\nInclude a copy of the full license\nState all significant changes made to the software\nMake the original source code available when distributing binaries based on that work\nInclude a copy of the original copyright notice\n\nIt allows the code to be changed and distributed to others (as long as release under GPL v3 also). Hence, updated license (and references to it) to GNU GPL 3.0 accordingly.\nCreated environment for book."
  },
  {
    "objectID": "logbook/posts/2024_07_03/index.html#upload-model-code",
    "href": "logbook/posts/2024_07_03/index.html#upload-model-code",
    "title": "Day 1",
    "section": "12.34-12.36: Upload model code",
    "text": "12.34-12.36: Upload model code\nUploaded copy of https://github.com/shiweih/desECR to original_study/."
  },
  {
    "objectID": "logbook/posts/2024_07_03/index.html#check-journal-article-license-and-upload",
    "href": "logbook/posts/2024_07_03/index.html#check-journal-article-license-and-upload",
    "title": "Day 1",
    "section": "12.43-12.47, 14.53-14.59: Check journal article license and upload",
    "text": "12.43-12.47, 14.53-14.59: Check journal article license and upload\nThe journal article was published in Frontiers in Neurology and is available at https://doi.org/10.3389/fneur.2019.00653. It has the following copyright statement:\n\n‚Äú¬© 2019 Huang, Maingard, Kok, Barras, Thijs, Chandra, Brooks and Asadi. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.‚Äù\n\nHence, we are free to upload this article and images to the repository (ensuring we cite throughout whenever using them), as well as the supplementary material.\nI set this up to be displayed within the quarto site."
  },
  {
    "objectID": "logbook/posts/2024_07_03/index.html#timings",
    "href": "logbook/posts/2024_07_03/index.html#timings",
    "title": "Day 1",
    "section": "Timings",
    "text": "Timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 0\n\n# Times from today\ntimes = [\n    ('11.53', '12.20'),\n    ('12.27', '12.33'),\n    ('12.34', '12.36'),\n    ('12.43', '12.47'),\n    ('14.53', '14.59')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 45m, or 0h 45m\nTotal used to date: 45m, or 0h 45m\nTime remaining: 2355m, or 39h 15m\nUsed 1.9% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_07_09/index.html",
    "href": "logbook/posts/2024_07_09/index.html",
    "title": "Day 5",
    "section": "",
    "text": "Note\n\n\n\nWorking on figures 2 + 3 and in-text result 3. Plus (untimed) fixing Quarto book (environment issues). Total time used: 16h 15m (40.6%)."
  },
  {
    "objectID": "logbook/posts/2024_07_09/index.html#continuing-on-figure-2",
    "href": "logbook/posts/2024_07_09/index.html#continuing-on-figure-2",
    "title": "Day 5",
    "section": "09.04-09.06, 09.14-09.15: Continuing on Figure 2",
    "text": "09.04-09.06, 09.14-09.15: Continuing on Figure 2\nI was curious to see how a different seed would impact the appearance of the figures, and so tried changing the seed from 200 to 300. However, it looks fairly similar.\n\n\n\nFigure 2A Example 1\n\n\nAnd with seed 500 too:\n\n\n\nFigure 2A Example 2"
  },
  {
    "objectID": "logbook/posts/2024_07_09/index.html#untimed-fixing-github-commits-and-action-for-quarto-book",
    "href": "logbook/posts/2024_07_09/index.html#untimed-fixing-github-commits-and-action-for-quarto-book",
    "title": "Day 5",
    "section": "Untimed: Fixing GitHub commits and action for Quarto book",
    "text": "Untimed: Fixing GitHub commits and action for Quarto book\nThis is not timed as I feel it is relevant to the Quarto book and not specific to this reproduction, and so reflects an issue I might have ironed out in a test-run, if I had done a second test-run in R (and not just a Python test-run).\nHad commit the produced .csv files without recognising they were too large. Undid commits with git reset HEAD^, and switched reproduction to only save relevant rows and columns, and to save as a compressed .csv.gz file. This required adding R.utils to the environment.\nAlso, modified quarto_publish.yaml to use setup-r-dependencies, but ran into several errors with this. One I could resolve related to having not pushed the renv/ directory. However, one I have struggled to resolve, which was that there is no package called ‚Äòpak‚Äô. I‚Äôve tried switching to ubuntu-22.04, as suggested on this issue. Also tried adding pak within the setup action with extra-packages: | any::pak. Still, issue persists.\nI explored using conda to manage the R environment, and whether that might actually be easier, as I‚Äôve had alot of challenges with renv for this book (but also in general, having just defaulted to using the latest packages, due to facing major issues installing older R and packages). Based on this tutorial, I created a requirements.txt file then ran the following:\nconda create -n huang2019\nconda activate huang2019\nconda install -c conda-forge r-base==4.4.1 --file reproduction/requirements.txt\nHowever, this was running into several issues, stating that the packages were not compatabile with the chosen r-base.\nFinally, I tried switching back to setup-renv. However, this took a long time to run but it does run (and supposedly future runs can use that cache so it is quicker). However, just like before, it stalls with an error (despite seeing that the renv installation above definitely included rmarkdown):\n[19/22] reproduction/reproduction.Rmd\nError in loadNamespace(x) : there is no package called ‚Äòrmarkdown‚Äô\nCalls: loadNamespace -&gt; withRestarts -&gt; withOneRestart -&gt; doWithOneRestart\nExecution halted\nR installation:\n  Version: 4.4.1\n  Path: /opt/R/4.4.1/lib/R\n  LibPaths:\n    - /home/runner/work/_temp/Library\n    - /opt/R/4.4.1/lib/R/library\n  knitr: (None)\n  rmarkdown: (None)\nThe knitr package is not available in this R installation.\nInstall with install.packages(\"knitr\")\nThe rmarkdown package is not available in this R installation.\nInstall with install.packages(\"rmarkdown\")\nERROR: Error\n    at renderFiles (file:///opt/quarto/bin/quarto.js:78081:29)\n    at eventLoopTick (ext:core/01_core.js:153:7)\n    at async renderProject (file:///opt/quarto/bin/quarto.js:78479:25)\n    at async renderForPublish (file:///opt/quarto/bin/quarto.js:109334:33)\n    at async renderForPublish (file:///opt/quarto/bin/quarto.js:104866:24)\n    at async Object.publish1 [as publish] (file:///opt/quarto/bin/quarto.js:105351:26)\n    at async publishSite (file:///opt/quarto/bin/quarto.js:109371:38)\n    at async publish7 (file:///opt/quarto/bin/quarto.js:109590:61)\n    at async doPublish (file:///opt/quarto/bin/quarto.js:109550:13)\n    at async publishAction (file:///opt/quarto/bin/quarto.js:109561:9)\nError: Process completed with exit code 1.\nTried adding Rscript{0} as shell for running R code but this was incorrect.\nThen tried switching reproduction.Rmd to reproduction.qmd. Re-running the action, it was very very slow (14 minutes) (so seemingly not using the cache). Moreover, it still hit an error as above.\nI decided a simpler solution might be to not require R in the Quarto build, and to instead ensure that any scripts using R are pre-rendered in some way (similar to how .ipynb files behave). For .ipynb, the default behaviour is that execute: enabled: false, as in the documentation. However, when run, encountered the error as above.\nTried a few options, including to manually install knitr and rmarkdown, although returned issue that 'lib = \"/usr/local/lib/R/site-library\"' is not writable.\nDid consider other options could be to clone and build examples that are set up with a GitHub action - e.g.¬†renv example, nix example. However, for now, have decided to just disable the action and do it manually from the terminal:\n\nquarto render (rebuilds whole site)\nquarto publish (pushes to github pages)"
  },
  {
    "objectID": "logbook/posts/2024_07_09/index.html#returning-to-figure-2",
    "href": "logbook/posts/2024_07_09/index.html#returning-to-figure-2",
    "title": "Day 5",
    "section": "13.34-14.30, 14.39-14.55: Returning to Figure 2",
    "text": "13.34-14.30, 14.39-14.55: Returning to Figure 2\nComparing my figures to the original, although it is now more visually similar than it was, there is still a lot of mismatch compared with those graphs. I‚Äôm suspicious this could be due to scaling, as currently I am just using the built in geom_density() scaling to 1, rather than explicitly scaling to the number not waiting. I found the R scripts behind this function unclear.\nInstead, I spent some time working out how to scale these manually, ensuring I was definitely dividing each density by the density from wait_time 0, and this confirmed that it matched up with the results from geom_density()‚Äôs scaled. As such, it seems the issue is not due to the scaling.\n# Create figure as usual\np &lt;- create_plot(res_base,\n                  title=\"Baseline\",\n                  ylab=\"Standardised density of patient in queue\")\n\n# Get data from the plot\nplot_data &lt;- ggplot_build(p)$data[[1]]\n\n# Create dataframe with the densities for when the waitimes are 0\nno_wait &lt;- plot_data %&gt;% filter(x==0) %&gt;% select(colour, density, scaled)\n#print(no_wait)\n\n# Loop through each of the colours (which reflect the resource groups)\nfor (c in no_wait$colour) {\n  # Filter the plot data to that resource group, then divide the densities by\n  # the density from wait time 0\n  d &lt;- plot_data %&gt;%\n    filter(colour == c) %&gt;%\n    mutate(scaled2 = density / no_wait[no_wait$colour==c, \"density\"]) %&gt;%\n    ungroup() %&gt;%\n    select(scaled, scaled2)\n\n  # Find the number of rows where these values match the scaled values\n  n_match &lt;- sum(apply(d, 1, function(x) length(unique(x)) == 1))\n  n_total &lt;- nrow(d)\n  print(sprintf(\"%s out of %s results match\", n_match, n_total))\n}\nGiven the scaling seems ok, the only other options I can think of are:\n\nThe environment - although I would anticipate that to be more of an issue of a model not running, rather than fundamentally changing results\nThe model parameters - I‚Äôve had another look back over the model code, and tried changing INR night -\n\nModel was set up to have INR staff on a schedule, but in the paper they are 24 hours. I had set this up by having 1 day and 1 night staff, but I‚Äôve tried changing the code so it‚Äôs just a single resource (like how ED team and stroke team are set up)\nOtherwise, all parameters look correct compared against Table 1\n\n\nHowever, model results came out the same. I am rather stuck on this now, so will move onto Figure 3 (having first, re-run reproduction with seed=200, as per in-text result 1)."
  },
  {
    "objectID": "logbook/posts/2024_07_09/index.html#starting-on-figure-3-and-in-text-result-3",
    "href": "logbook/posts/2024_07_09/index.html#starting-on-figure-3-and-in-text-result-3",
    "title": "Day 5",
    "section": "14.56-15.26, 15.31-16.19, 16.24-16.56: Starting on Figure 3 and in-text result 3",
    "text": "14.56-15.26, 15.31-16.19, 16.24-16.56: Starting on Figure 3 and in-text result 3\nFor this scenario analysis, the ‚Äúday time working hours of all human resources are extended by up to 2h, extending resource access to all patients‚Äù (Huang et al. (2019)). Given how the model scheduling is set-up, it is assumed that this means we simply adjust the schedule to end at 5, 6 or 7pm (and that that would simply shortern the night staff time).\nI ran these scenarios, processing and saving the relevant model results.\nFor in-text result 3, I can see that the results do not match up to the paper. I am not surprised by this though - as the model had no seed control, as it is not mentioned in the paper, we can assume that it might not have been used by the original study, and so variation between the scenarios could (in part) reflect model stochasticity.\n\nimport pandas as pd\n\npd.read_csv(\"txt3.csv\")\n\n\n\n\n\n\n\n\n\nscenario\nshift\nmean\ndiff_from_5pm\n\n\n\n\n0\nBaseline\n5pm\n13.958269\n0.00\n\n\n1\nBaseline\n6pm\n12.486042\n-1.47\n\n\n2\nBaseline\n7pm\n12.491421\n-1.47\n\n\n3\nExclusive use\n5pm\n8.117729\n0.00\n\n\n4\nExclusive use\n6pm\n7.802954\n-0.31\n\n\n5\nExclusive use\n7pm\n6.432643\n-1.69\n\n\n6\nTwo AngioINRs\n5pm\n13.511560\n0.00\n\n\n7\nTwo AngioINRs\n6pm\n11.408894\n-2.10\n\n\n8\nTwo AngioINRs\n7pm\n11.256842\n-2.25\n\n\n\n\n\n\n\n\nTo test this assumption, I ran the model again for baseline with two further seeds. We can see the importance of seed control here. For example, with seed 700, we see a broader range of results, with the result for 6pm (13.32) is much higher than for the other two seeds and, compared with their 5pm results, we would‚Äôve seen less of a reduction. Similarly, if we compared the 5pm seed 700 result with the 6pm seed 500 result, we would see a much greater reduction.\n\npd.read_csv(\"txt3_seeds.csv\")\n\n\n\n\n\n\n\n\n\nscenario\n5pm\n6pm\n7pm\n\n\n\n\n0\nBaseline\n13.96\n12.49\n12.49\n\n\n1\nBaseline (seed 500)\n13.96\n12.15\n12.20\n\n\n2\nBaseline (seed 700)\n15.47\n13.32\n12.30\n\n\n\n\n\n\n\n\nFor Figure 3, it was simple to adapt the function to create it but, similar to Figure 2, there are several differences in the original study results.\n\n\n\nFigure 3"
  },
  {
    "objectID": "logbook/posts/2024_07_09/index.html#timings",
    "href": "logbook/posts/2024_07_09/index.html#timings",
    "title": "Day 5",
    "section": "Timings",
    "text": "Timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 790\n\n# Times from today\ntimes = [\n    ('09.04', '09.06'),\n    ('09.14', '09.15'),\n    ('13.34', '14.30'),\n    ('14.39', '14.55'),\n    ('14.56', '15.26'),\n    ('15.31', '16.19'),\n    ('16.24', '16.56')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 185m, or 3h 5m\nTotal used to date: 975m, or 16h 15m\nTime remaining: 1425m, or 23h 45m\nUsed 40.6% of 40 hours max"
  },
  {
    "objectID": "original_study/desECR/figure_description.html",
    "href": "original_study/desECR/figure_description.html",
    "title": "Reproducing Huang et al. 2019",
    "section": "",
    "text": "ed = Confirmed stroke patients\ninr = Non-emergency interventional neuroradiology patients\neir = Emergency interventional radiology patients\nir = Non-emergency interventional radiology patients"
  }
]